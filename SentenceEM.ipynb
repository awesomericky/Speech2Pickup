{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentenceEM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjubJvgqoi7O",
        "outputId": "f79a46f6-e8a5-411d-86ed-b02ce108c385"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DtnV8t-aTfD"
      },
      "source": [
        "%pip install wandb -q"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBA26NJTgUsh",
        "outputId": "076ae97d-ea31-4c11-96ab-e9ec4f8fe0d1"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ0XNqJJZ-ll"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.backend as kb\n",
        "from tensorflow.keras.models import save_model\n",
        "import random\n",
        "import pdb\n",
        "import wandb\n",
        "from os.path import join, isfile\n",
        "from os import listdir"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHYVABrggyHf",
        "outputId": "02de81a9-ee79-4d3b-a00d-b04d22089d1e"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di9QVKhraRmb",
        "outputId": "04da1ebe-a995-448d-e0f8-e78ed34b6d4a"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawesomericky\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EySe9_QPR-K",
        "outputId": "4a7217a2-4cb0-40cf-b0cd-86c9f25def94"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print('Found GPU at {}'.format(device_name))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biS128E0ZXvL"
      },
      "source": [
        "\"\"\"\n",
        "utils.py\n",
        "\"\"\"\n",
        "\n",
        "def read_script_files(script_dir_path):\n",
        "    script_files = [f for f in listdir(script_dir_path) if isfile(join(script_dir_path, f))]\n",
        "    script_files.sort()\n",
        "    return script_files"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76bPg01FZg_p"
      },
      "source": [
        "\"\"\"\n",
        "processed_data_loader.py\n",
        "\"\"\"\n",
        "\n",
        "def load_single_npz_data(relative_data_directory_path, file_name):\n",
        "    file_path = join(relative_data_directory_path, file_name)\n",
        "    data = np.load(file=file_path)\n",
        "    return data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDnb_Tk9F2JJ"
      },
      "source": [
        "\"\"\"\n",
        "TCN_and_decoder.py\n",
        "\"\"\"\n",
        "\n",
        "class ResidualBlock(tf.keras.Model):\n",
        "    def __init__(self, dilation_rate, num_filters, kernel_size, padding, \n",
        "                        dropout_rate, seed, training_state):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=1)\n",
        "        layers = tf.keras.layers\n",
        "        assert padding in ['causal', 'same']\n",
        "\n",
        "        self.training_state = training_state\n",
        "\n",
        "        # Block1\n",
        "        self.conv1 = layers.Conv1D(filters=num_filters, kernel_size=kernel_size, data_format='channels_last',\n",
        "                                    dilation_rate=dilation_rate, padding=padding, kernel_initializer=init)\n",
        "        self.batch1 = layers.BatchNormalization(axis=1, trainable=True)\n",
        "        self.ac1 = layers.LeakyReLU(alpha=0.2)\n",
        "        self.drop1 = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        # Block2\n",
        "        self.conv2 = layers.Conv1D(filters=num_filters, kernel_size=kernel_size, data_format='channels_last',\n",
        "                                    dilation_rate=dilation_rate, padding=padding, kernel_initializer=init)\n",
        "        self.batch2 = layers.BatchNormalization(axis=1, trainable=True)\n",
        "        self.ac2 = layers.LeakyReLU(alpha=0.2)\n",
        "        self.drop2 = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        self.downsample = layers.Conv1D(filters=num_filters, kernel_size=1,\n",
        "                                        padding='same', kernel_initializer=init)\n",
        "        self.ac3 = layers.LeakyReLU(alpha=0.2)\n",
        "    \n",
        "    def call(self, x):\n",
        "        # Block1\n",
        "        prev_x = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch1(x)\n",
        "        x = self.ac1(x)\n",
        "        x = self.drop1(x) if self.training_state else x\n",
        "\n",
        "        # Block2\n",
        "        x = self.conv2(x)\n",
        "        x = self.batch2(x)\n",
        "        x = self.ac2(x)\n",
        "        x = self.drop2(x) if self.training_state else x\n",
        "\n",
        "        # Match dimention\n",
        "        if prev_x.shape[-1] != x.shape[-1]:\n",
        "            prev_x = self.downsample(prev_x)\n",
        "        assert prev_x.shape == x.shape\n",
        "\n",
        "        # skip connection\n",
        "        return self.ac3(prev_x + x)\n",
        "\n",
        "# # Test\n",
        "# x = tf.convert_to_tensor(np.random.random((100, 80, 303)))   # batch, dim, seq_len\n",
        "# model = ResidualBlock(dilation_rate=1, num_filters=80, kernel_size=3, padding='causal', dropout_rate=0.2, seed=1, training_state=True)\n",
        "# y = model(x)\n",
        "# print(y.shape)\n",
        "# model.summary()\n",
        "\n",
        "# # Check model\n",
        "# x = tf.keras.Input(batch_shape=(10, 80, 303))\n",
        "# model = ResidualBlock(dilation_rate=1, num_filters=80, kernel_size=3, padding='causal', dropout_rate=0.2, seed=1, training_state=True)\n",
        "# model.build((10, 80, 303))\n",
        "# model = tf.keras.Model(inputs=x, outputs=model.call(x))\n",
        "# plot_model(model, to_file='residual_block.png')\n",
        "\n",
        "\n",
        "class TemporalBlock(tf.keras.Model):\n",
        "    def __init__(self, num_channels, kernel_size, dropout_rate, seed, training_state):\n",
        "        # num_channels is a list contains hidden channel numbers of Conv1D\n",
        "        # len(num_channels) is number of convolutional layers in one Temporal Block\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        assert isinstance(num_channels, list)\n",
        "\n",
        "        self.num_levels = len(num_channels)\n",
        "        self.resi_blocks = [0]*self.num_levels\n",
        "        for i in range(self.num_levels):\n",
        "            dilation_rate = 2**i\n",
        "            self.resi_blocks[i] = ResidualBlock(dilation_rate, num_channels[i], kernel_size, padding='causal',\n",
        "                            dropout_rate=dropout_rate, seed=seed, training_state=training_state)\n",
        "    \n",
        "    def call(self, x):\n",
        "        for i in range(self.num_levels):\n",
        "            x = self.resi_blocks[i](x)\n",
        "        return x\n",
        "\n",
        "# # Test\n",
        "# x = tf.convert_to_tensor(np.random.random((100, 80, 303)))   # batch, dim, seq_len\n",
        "# model = TemporalBlock(num_channels=[80, 80, 80], kernel_size=3, dropout_rate=0.2, seed=1, training_state=True)\n",
        "# y = model(x)\n",
        "# print(y.shape)\n",
        "# model.summary()\n",
        "\n",
        "# # Check model\n",
        "# x = tf.keras.Input(batch_shape=(10, 80, 303))\n",
        "# model = TemporalBlock(num_channels=[80, 80, 80, 80, 80, 80], kernel_size=3, dropout_rate=0.2, seed=1, training_state=True)\n",
        "# model.build((10, 80, 303))\n",
        "# model = tf.keras.Model(inputs=x, outputs=model.call(x))\n",
        "# plot_model(model, to_file='temporal_block.png')\n",
        "\n",
        "class TempConvnet(tf.keras.Model):\n",
        "    def __init__(self, num_stacks, num_channels, kernel_size, dropout_rate, return_type, seed, training_state):\n",
        "        # num_stacks number of Temporal Blocks in Temporal convolutional network\n",
        "        super(TempConvnet, self).__init__()\n",
        "        assert isinstance(num_stacks, int)\n",
        "        assert isinstance(num_channels, list)\n",
        "        assert return_type in ['whole', 'end']\n",
        "\n",
        "        self.num_stacks = num_stacks\n",
        "        self.temp_blocks = [0]*self.num_stacks\n",
        "        self.return_type = return_type\n",
        "        for i in range(num_stacks):\n",
        "            self.temp_blocks[i] = TemporalBlock(num_channels, kernel_size=kernel_size, dropout_rate=dropout_rate, seed=seed, training_state=training_state)\n",
        "    \n",
        "    def call(self, x):\n",
        "        for i in range(self.num_stacks):\n",
        "            x = self.temp_blocks[i](x)\n",
        "        \n",
        "        if self.return_type == 'whole':\n",
        "            return x\n",
        "        elif self.return_type == 'end':\n",
        "            return x[:, -1, :]\n",
        "\n",
        "# # Test\n",
        "# x = tf.convert_to_tensor(np.random.random((100, 303, 80)))   # batch, dim, seq_len\n",
        "# model = TempConvnet(num_stacks=3, num_channels=[80, 80, 80], kernel_size=3, dropout_rate=0.2, return_type='end', seed=1, training_state=True)\n",
        "# y = model(x)\n",
        "# print(y.shape)\n",
        "# model.summary()\n",
        "\n",
        "# # Check model\n",
        "# x = tf.keras.Input(batch_shape=(10, 80, 303))\n",
        "# model = TempConvnet(num_stacks=3, num_channels=[80, 80, 80, 80, 80, 80], kernel_size=3, dropout_rate=0.2, return_type='end', seed=1, training_state=True)\n",
        "# model.build((10, 80, 303))\n",
        "# model = tf.keras.Model(inputs=x, outputs=model.call(x))\n",
        "# plot_model(model, to_file='temporal_CNN.png')\n",
        "\n",
        "class TempConvnet_Decoder(tf.keras.Model):\n",
        "    def __init__(self, decoder_type, num_levels, num_channels, kernel_size, padding, upsample_size, dropout_rate, output_shape, seed, training_state):\n",
        "        super(TempConvnet_Decoder, self).__init__()\n",
        "        assert isinstance(num_channels, int)\n",
        "        assert isinstance(kernel_size, list)\n",
        "        assert isinstance(upsample_size, dict)\n",
        "        assert padding in ['causal', 'same']\n",
        "        assert decoder_type in ['linguistic', 'acoustic']\n",
        "        assert num_levels == upsample_size[2] + upsample_size[3] + upsample_size[0]\n",
        "\n",
        "        init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=1)\n",
        "        layers = tf.keras.layers\n",
        "\n",
        "        self.training_state = training_state\n",
        "\n",
        "        self.size2_upsample_num = upsample_size[2]\n",
        "        self.size3_upsample_num = upsample_size[3]\n",
        "        self.reshape_conv_num = 1\n",
        "        self.num_levels = num_levels\n",
        "        self.final_kernel_size = kernel_size[1]\n",
        "        self.output_len = output_shape[-1]  ## ex) (num_batch, num_time_steps, dictionary_length)\n",
        "\n",
        "        self.upsample_blocks = [0]*(num_levels-1)\n",
        "        self.conv_blocks = [0]*(num_levels-1)\n",
        "        self.batchnorm_blocks = [0]*(num_levels-1)\n",
        "        self.ac_blocks = [0]*(num_levels-1)\n",
        "        self.drop_blocks = [0]*(num_levels-1)\n",
        "\n",
        "        for i in range(num_levels-self.size3_upsample_num-self.reshape_conv_num):\n",
        "            self.upsample_blocks[i] = layers.UpSampling1D(size=2)\n",
        "            self.conv_blocks[i] = layers.Conv1D(filters=num_channels, kernel_size=kernel_size[0], data_format='channels_last',\n",
        "                                                    padding=padding, kernel_initializer=init)\n",
        "            self.batchnorm_blocks[i] = layers.BatchNormalization(axis=-1, trainable=True)\n",
        "            self.ac_blocks[i] = layers.ReLU() if decoder_type == 'linguistic' else layers.LeakyReLU(alpha=0.2)\n",
        "            self.drop_blocks[i] = layers.Dropout(rate=dropout_rate)\n",
        "        for i in range(num_levels-self.size3_upsample_num-self.reshape_conv_num, num_levels-self.reshape_conv_num):\n",
        "            self.upsample_blocks[i] = layers.UpSampling1D(size=3)\n",
        "            self.conv_blocks[i] = layers.Conv1D(filters=num_channels, kernel_size=kernel_size[0], data_format='channels_last',\n",
        "                                                    padding=padding, kernel_initializer=init)\n",
        "            self.batchnorm_blocks[i] = layers.BatchNormalization(axis=-1, trainable=True)\n",
        "            self.ac_blocks[i] = layers.Activation('tanh') if decoder_type == 'linguistic' else layers.LeakyReLU(alpha=0.2)\n",
        "            self.drop_blocks[i] = layers.Dropout(rate=dropout_rate)\n",
        "        self.final_conv_block = layers.Conv1D(filters=num_channels, kernel_size=self.final_kernel_size, data_format='channels_last',\n",
        "                                                    padding='valid', kernel_initializer=init)\n",
        "        self.final_ac_block = layers.Softmax(axis=-1) if decoder_type == 'linguistic' else layers.LeakyReLU(alpha=0.2)\n",
        "        self.final_reshape_block = layers.Permute((2, 1))\n",
        "    \n",
        "    def call(self, x):\n",
        "        for i in range(self.num_levels-1):\n",
        "            x = self.upsample_blocks[i](x)\n",
        "            x = self.conv_blocks[i](x)\n",
        "            x = self.batchnorm_blocks[i](x)\n",
        "            x = self.ac_blocks[i](x)\n",
        "            x = self.drop_blocks[i](x) if self.training_state else x\n",
        "        len_padding = (self.output_len - (x.shape[1] - self.final_kernel_size) - 1)//2\n",
        "        x = tf.keras.layers.ZeroPadding1D(padding=(len_padding, len_padding))(x)\n",
        "        x = self.final_conv_block(x)\n",
        "        x = self.final_ac_block(x)\n",
        "        y = self.final_reshape_block(x)\n",
        "        return y\n",
        "\n",
        "# # Test\n",
        "# x = tf.convert_to_tensor(np.random.random((100, 1, 80)))   # batch, seq_len, dim\n",
        "# model = TempConvnet_Decoder(decoder_type='linguistic', num_levels=8, num_channels=42, kernel_size=[2, 2], \n",
        "#                             padding='causal', upsample_size={2: 5, 3: 2, 0:1}, dropout_rate=0.2, output_shape=(100, 42, 303), seed=1, training_state=True)\n",
        "# y = model(x, True)\n",
        "# print(y.shape)\n",
        "# model.summary()\n",
        "\n",
        "# # Check model\n",
        "# # linguistic decoder\n",
        "# x = tf.keras.Input(batch_shape=(10, 1, 80)) # batch_size: 10\n",
        "# model = TempConvnet_Decoder(decoder_type='linguistic', num_levels=8, num_channels=42, kernel_size=[2, 2], \n",
        "#                             padding='causal', upsample_size={2: 5, 3: 2, 0:1}, dropout_rate=0.2, output_shape=(None, 42, 303), seed=1, training_state=True)\n",
        "# model.build((10, 1, 80))\n",
        "# model = tf.keras.Model(inputs=x, outputs=model.call(x))\n",
        "# plot_model(model, to_file='linguistic_decoder.png')\n",
        "\n",
        "# # acoustic decoder\n",
        "# x = tf.keras.Input(batch_shape=(10, 1, 80)) # batch_size: 10\n",
        "# model = TempConvnet_Decoder(decoder_type='acoustic', num_levels=8, num_channels=80, kernel_size=[2, 2], \n",
        "#                             padding='causal', upsample_size={2: 5, 3: 2, 0:1}, dropout_rate=0.2, output_shape=(None, 80, 303), seed=1, training_state=True)\n",
        "# model.build((10, 1, 80))\n",
        "# model = tf.keras.Model(inputs=x, outputs=model.call(x))\n",
        "# plot_model(model, to_file='acoustic_decoder.png')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2hmgak-js0D"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class History(Callback):\n",
        "\n",
        "    def on_train_begin(self,logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_tBigUbFqaT"
      },
      "source": [
        "\"\"\"\n",
        "sentenceEM.py\n",
        "\"\"\"\n",
        "\n",
        "class sentenceEM(tf.keras.Model):\n",
        "    def __init__(self, encoder_args, linguistic_decoder_args, acoustic_decoder_args, input_shapes, seed, training_state):\n",
        "        \"\"\"\n",
        "        1) encoder_args: \n",
        "        num_stacks, num_channels, kernel_size, dropout_rate, return_type, seed\n",
        "        2) linguistic_decoder_args:\n",
        "        decoder_type, num_levels, num_channels, kernel_size, padding, upsample_size, dropout_rate, output_shape, seed, training_state\n",
        "        3) acoustic_decoder_args:\n",
        "        decoder_type, num_levels, num_channels, kernel_size, padding, upsample_size, dropout_rate, output_shape, seed, training_state\n",
        "        \"\"\"\n",
        "        super(sentenceEM, self).__init__()\n",
        "        self.input_shapes = input_shapes\n",
        "        self.input_reshape_block = layers.Permute((2, 1))\n",
        "        self.encoder = TempConvnet(num_stacks=encoder_args['num_stacks'], num_channels=encoder_args['num_channels'],\n",
        "                                        kernel_size=encoder_args['kernel_size'], dropout_rate=encoder_args['dropout_rate'],\n",
        "                                        return_type=encoder_args['return_type'], seed=seed, training_state=training_state)\n",
        "        self.linguistic_decoder = TempConvnet_Decoder(decoder_type=linguistic_decoder_args['decoder_type'], num_levels=linguistic_decoder_args['num_levels'],\n",
        "                                                        num_channels=linguistic_decoder_args['num_channels'], kernel_size=linguistic_decoder_args['kernel_size'],\n",
        "                                                        padding=linguistic_decoder_args['padding'], upsample_size=linguistic_decoder_args['upsample_size'],\n",
        "                                                        dropout_rate=linguistic_decoder_args['dropout_rate'], output_shape=linguistic_decoder_args['output_shape'],\n",
        "                                                        seed=seed, training_state=training_state)\n",
        "        self.acoustic_decoder = TempConvnet_Decoder(decoder_type=acoustic_decoder_args['decoder_type'], num_levels=acoustic_decoder_args['num_levels'],\n",
        "                                                        num_channels=acoustic_decoder_args['num_channels'], kernel_size=acoustic_decoder_args['kernel_size'],\n",
        "                                                        padding=acoustic_decoder_args['padding'], upsample_size=acoustic_decoder_args['upsample_size'],\n",
        "                                                        dropout_rate=acoustic_decoder_args['dropout_rate'], output_shape=acoustic_decoder_args['output_shape'],\n",
        "                                                        seed=seed, training_state=training_state)\n",
        "    \n",
        "    def call(self, x):\n",
        "        embedded_outputs = self.input_reshape_block(x)\n",
        "        print(embedded_outputs.shape)\n",
        "        embedded_outputs = self.encoder(embedded_outputs)\n",
        "        embedded_outputs = kb.expand_dims(embedded_outputs, axis=1)\n",
        "        print(embedded_outputs.shape)\n",
        "        linguistic_outputs = self.linguistic_decoder(embedded_outputs)\n",
        "        acoustic_outputs = self.acoustic_decoder(embedded_outputs)\n",
        "\n",
        "        return linguistic_outputs, acoustic_outputs\n",
        "\n",
        "    def build_graph(self):\n",
        "        x = tf.keras.Input(batch_shape=self.input_shapes)\n",
        "        linguistic_outputs, acoustic_outputs = self.call(x)\n",
        "\n",
        "        self.model = tf.keras.Model(inputs=x, outputs=[linguistic_outputs, acoustic_outputs])\n",
        "    \n",
        "    # def model_visualize(self):\n",
        "    #     self.model.summary()\n",
        "    #     plot_model(self.model, to_file='/content/drive/MyDrive/Speech2Pickup/sentenceEM.png')\n",
        "    \n",
        "    def model_compile(self, lr, loss_weights):\n",
        "        self.model.compile(optimizer='adam', loss=[self.linguistic_loss_function, self.acoustic_loss_function], loss_weights=[loss_weights['linguistic'], loss_weights['acoustic']])\n",
        "\n",
        "    def model_train(self, X_train, Y_linguistic_train, Y_acoustic_train, batch_size):\n",
        "        hist = History()\n",
        "        self.model.fit(x=X_train, y=[Y_linguistic_train, Y_acoustic_train], batch_size=batch_size, epochs=1, verbose=1, callbacks=[hist])\n",
        "        return hist.losses\n",
        "    \n",
        "    def linguistic_loss_function(self, y_actual, y_predicted):\n",
        "        epsil = 1e-5\n",
        "        y_predicted = -kb.log(y_predicted + epsil)\n",
        "        loss = layers.multiply([y_actual, y_predicted])\n",
        "        loss = kb.sum(loss, axis=1)\n",
        "        loss = kb.mean(loss, keepdims=False)\n",
        "        return loss\n",
        "    \n",
        "    def acoustic_loss_function(self, y_actual, y_predicted):\n",
        "        loss = layers.subtract([y_actual, y_predicted])\n",
        "        loss = layers.multiply([loss, loss])\n",
        "        loss = kb.mean(loss, keepdims=False)\n",
        "        return loss\n",
        "\n",
        "def data_shuffle(relative_data_directory_path):\n",
        "    data_files = read_script_files(relative_data_directory_path)\n",
        "    random.shuffle(data_files)\n",
        "    return data_files"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRSZUTxv6OyO"
      },
      "source": [
        "# Model configure\n",
        "\n",
        "batch_size = 64; seed = 1; lr = 0.001; epochs = 300; loss_weights={'linguistic': 1, 'acoustic': 0.005}\n",
        "n_mels = 80\n",
        "time_steps = 303\n",
        "training_state = True\n",
        "input_shapes = (batch_size, n_mels, time_steps)\n",
        "encoder_args = {'num_stacks': 3, 'num_channels':[80, 80, 80, 80, 80, 80], 'kernel_size':3, 'dropout_rate': 0.2, 'return_type': 'end'}\n",
        "linguistic_decoder_args = {'decoder_type': 'linguistic', 'num_levels':8, 'num_channels': 42, 'kernel_size': [2, 2], 'padding': 'causal',\n",
        "                            'upsample_size': {2: 5, 3: 2, 0:1}, 'dropout_rate': 0.2, 'output_shape': (batch_size, 42, 303)}\n",
        "acoustic_decoder_args = {'decoder_type': 'acoustic', 'num_levels':8, 'num_channels': 80, 'kernel_size': [2, 2], 'padding': 'causal',\n",
        "                            'upsample_size': {2: 5, 3: 2, 0:1}, 'dropout_rate': 0.2, 'output_shape': (batch_size, 80, 303)}\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  sen_em_model = sentenceEM(encoder_args=encoder_args, linguistic_decoder_args=linguistic_decoder_args, acoustic_decoder_args=acoustic_decoder_args,\n",
        "                              input_shapes=input_shapes, seed=seed, training_state=training_state)\n",
        "  # sen_em_model.build(input_shapes)\n",
        "  sen_em_model.build_graph()\n",
        "  sen_em_model.model_compile(lr=lr, loss_weights=loss_weights)\n",
        "# sen_em_model.model_visualize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "N4vdOG-56Fk1",
        "outputId": "9d1b676f-505d-4b15-8194-5a6373220650"
      },
      "source": [
        "# Train\n",
        "relative_data_directory_path = '/content/drive/MyDrive/Speech2Pickup/data_v2.2'\n",
        "wandb.init(project='Speech2Pickup', name='sentenceEM_lr:0.001')\n",
        "model_save_freq = 5\n",
        "for i in range(epochs):\n",
        "    try:\n",
        "        print('='*20)\n",
        "        print('Epoch: {}'.format(i+1))\n",
        "\n",
        "        # Prepare training\n",
        "        one_shot_load_num = 50\n",
        "        start = 0\n",
        "        end = start + batch_size*one_shot_load_num\n",
        "        epoch_loss = []\n",
        "        extra_train_state = False\n",
        "        if i == 0:\n",
        "            data_files = data_shuffle(relative_data_directory_path)\n",
        "        else:\n",
        "            random.shuffle(data_files)\n",
        "          \n",
        "        if len(data_files) % (batch_size*one_shot_load_num) != 0:\n",
        "            extra_train_state = True\n",
        "            print('extra train needed')\n",
        "        \n",
        "        # Train\n",
        "        while end <= len(data_files):\n",
        "            print('Processing {}/{}'.format(end//(batch_size*one_shot_load_num), len(data_files)//(batch_size*one_shot_load_num))) if extra_train_state == False \\\n",
        "            else print('Processing {}/{}'.format(end//(batch_size*one_shot_load_num), (len(data_files)//(batch_size*one_shot_load_num))+1))\n",
        "            batch_data_files = data_files[start:end]\n",
        "            acoustic_train_batch = []\n",
        "            linguistic_train_batch = []\n",
        "            for ii in range(batch_size*one_shot_load_num):\n",
        "                data = load_single_npz_data(relative_data_directory_path=relative_data_directory_path, file_name=batch_data_files[ii])\n",
        "                acoustic_train_batch.append(data['arr_0'])\n",
        "                linguistic_train_batch.append(data['arr_1'])\n",
        "            acoustic_train_batch = np.array(acoustic_train_batch)\n",
        "            linguistic_train_batch = np.array(linguistic_train_batch)\n",
        "            with tf.device('/device:GPU:0'):\n",
        "              temp_loss = sen_em_model.model_train(X_train=acoustic_train_batch, Y_linguistic_train=linguistic_train_batch,\n",
        "                                                  Y_acoustic_train=acoustic_train_batch, batch_size=batch_size)\n",
        "              \n",
        "              for iii in range(len(temp_loss)):\n",
        "                wandb.log({'temp_loss': temp_loss[iii]})\n",
        "              epoch_loss.extend(temp_loss)\n",
        "            start = end\n",
        "            end = start + batch_size*one_shot_load_num\n",
        "            \n",
        "        if extra_train_state:\n",
        "            print('Processing {}/{}'.format((len(data_files)//(batch_size*one_shot_load_num))+1, (len(data_files)//(batch_size*one_shot_load_num))+1))\n",
        "            batch_data_files = data_files[start:]\n",
        "            acoustic_train_batch = []\n",
        "            linguistic_train_batch = []\n",
        "            for ii in range(len(batch_data_files)):\n",
        "                data = load_single_npz_data(relative_data_directory_path=relative_data_directory_path, file_name=batch_data_files[ii])\n",
        "                acoustic_train_batch.append(data['arr_0'])\n",
        "                linguistic_train_batch.append(data['arr_1'])\n",
        "            acoustic_train_batch = np.array(acoustic_train_batch)\n",
        "            linguistic_train_batch = np.array(linguistic_train_batch)\n",
        "            with tf.device('/device:GPU:0'):\n",
        "              temp_loss = sen_em_model.model_train(X_train=acoustic_train_batch, Y_linguistic_train=linguistic_train_batch,\n",
        "                                                  Y_acoustic_train=acoustic_train_batch, batch_size=batch_size)\n",
        "              for iii in range(len(temp_loss)):\n",
        "                wandb.log({'temp_loss': temp_loss[iii]})\n",
        "              epoch_loss.extend(temp_loss)\n",
        "\n",
        "        epoch_loss = np.mean(np.array(epoch_loss))\n",
        "        wandb.log({'epoch_loss': epoch_loss})\n",
        "\n",
        "        # Model save\n",
        "        if (i+1) % model_save_freq == 0:\n",
        "          print('Finished training {} epochs'.format(i+1))\n",
        "          sen_em_model.model.save_weights(filepath='/content/drive/MyDrive/Speech2Pickup/sentenceEM_model/sentenceEM_model', overwrite=True)\n",
        "          print('Model saving complete!')\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.11<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">sentenceEM_lr:0.001</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/awesomericky/Speech2Pickup\" target=\"_blank\">https://wandb.ai/awesomericky/Speech2Pickup</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/awesomericky/Speech2Pickup/runs/1sfmfj08\" target=\"_blank\">https://wandb.ai/awesomericky/Speech2Pickup/runs/1sfmfj08</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201121_172422-1sfmfj08</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "====================\n",
            "Epoch: 1\n",
            "extra train needed\n",
            "Processing 1/4\n",
            "Train on 3200 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibBz0FwpzLPw"
      },
      "source": [
        "# Erase model from memory\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "del sen_em_model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPa0THO1Umyn"
      },
      "source": [
        "# Test (Debugging)\n",
        "\n",
        "data = load_single_npz_data(relative_data_directory_path='/content/drive/MyDrive/Speech2Pickup/data_v2.2', file_name='senEM_preprocessed_100.npz')\n",
        "data1 = data['arr_0']\n",
        "data1 = np.float32(data1)\n",
        "data1 = kb.expand_dims(data1, axis=0)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  l_y, a_y = sen_em_model(data1)\n",
        "print(kb.eval(a_y)[0,:,180])\n",
        "print('='*20)\n",
        "print(kb.eval(l_y)[0,:,180])\n",
        "print(len(kb.eval(l_y)[0,:,180]))\n",
        "print('='*20)\n",
        "print(np.sum(kb.eval(l_y)[0,:,180]))\n",
        "print(np.sum(kb.eval(l_y)[0,:,300]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JciAh_P4Is6"
      },
      "source": [
        "# Test (Debugging)\n",
        "\n",
        "x = tf.convert_to_tensor(np.random.random((batch_size, 80, 303)), dtype=np.float32)   # batch, dim, seq_len\n",
        "l_y, a_y = sen_em_model(x)\n",
        "print('='*20)\n",
        "print(kb.eval(l_y)[0,:,0])\n",
        "print(len(kb.eval(l_y)[0,:,0]))\n",
        "print('='*20)\n",
        "print(np.sum(kb.eval(l_y)[0,:,0]))\n",
        "print(np.sum(kb.eval(l_y)[0,:,100]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHrguM7QsINY"
      },
      "source": [
        "# Load model (model compiling should be done first)\n",
        "sen_em_model.model.load_weights('/content/drive/MyDrive/Speech2Pickup/sentenceEM_model/sentenceEM_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "013hJm5c0SWq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}