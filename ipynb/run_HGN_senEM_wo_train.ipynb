{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "run_HGN_senEM_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw3YiVt838Tn"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFqZe5eq7wrP",
        "outputId": "debb808d-c0d5-4a0c-b8f6-28186cb0e8a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrwDjFKOu4Jg",
        "outputId": "3777fca8-e932-4147-ad9a-cfe01bd80f87"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e83VV0wby26c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bff6d66-c834-4749-bf72-b8f58c2ede85"
      },
      "source": [
        "% pip install wandb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5e/9df94df3bfee51b92b54a5e6fa277d6e1fcdf1f27b1872214b98f55ec0f7/wandb-0.10.12-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.3MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/d1/a7f8fe3df258549b303415157328bfcc63e9b11d06a7ad7a3327f3d32606/GitPython-3.1.11-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 35.8MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/d9/3d1f46b428fd7b646725896b58d2eddb84f79fd76912773e6193cf74263d/watchdog-1.0.2-py3-none-manylinux2014_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/5c/018bf9a5c24343a664deaea70e61f33f53bb1bd3caf193110f827bfd07e2/sentry_sdk-0.19.5-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (51.0.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6490 sha256=f2ea50f10766c80c7bc5b45ce9b0379b95bda0db7d6a89c623c472d7f370022b\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built subprocess32\n",
            "Installing collected packages: smmap, gitdb, GitPython, configparser, watchdog, sentry-sdk, subprocess32, docker-pycreds, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.11 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 sentry-sdk-0.19.5 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.10.12 watchdog-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8op54dlA38Tn"
      },
      "source": [
        "# Import dependencies\n",
        "\n",
        "from os import listdir, remove, makedirs\n",
        "from os.path import join, isdir, isfile\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import variable_scope as vs\n",
        "import tensorflow.keras.layers as layers\n",
        "import wandb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR5nqGf3SfJf",
        "outputId": "3ecc81d0-f26c-4628-9cc3-24737037437d"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawesomericky\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3CHnzG4yaXB"
      },
      "source": [
        "# Import python files\n",
        "\n",
        "from utils import HGN_organize_data, HGN_divide_train_test\n",
        "from code_HGN_senEM_train import train"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OGQHCK_38Tn"
      },
      "source": [
        "## Load Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqqjCosb38Tn",
        "outputId": "d19b2b42-ce78-47d4-8d69-ed52d48b30ea"
      },
      "source": [
        "relative_data_directory_path = '/content/drive/MyDrive/Speech2Pickup/dataset1/data_v1.2_single_channel/preprocessed4HGN_speech2pickup.npz'\n",
        "    \n",
        "data = np.load(relative_data_directory_path)\n",
        "\n",
        "img_idx = data['img_idx']\n",
        "sen_len = data['seq_len']\n",
        "speech_inputs = data['inputs']\n",
        "pos_outputs = data['outputs']\n",
        "sentence = data['sentence']\n",
        "\n",
        "print('End loading MetaData')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End loading MetaData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_dqmGhN38Tn"
      },
      "source": [
        "## Load data and divide training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlqqLiDe38To",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1e1b72-ee60-4359-b2f5-6d155b2ed686"
      },
      "source": [
        "num_data = img_idx.shape[0]\n",
        "\n",
        "img_path = '/content/drive/MyDrive/Speech2Pickup/dataset1/train_img'\n",
        "script_path = '/content/drive/MyDrive/Speech2Pickup/dataset1/train_heatmap'\n",
        "\n",
        "total_images, total_heatmaps = HGN_organize_data(img_path, script_path, model_type='speech2pickup');\n",
        "\n",
        "args = dict()\n",
        "args['img_idx'] = img_idx\n",
        "args['speech_inputs'] = speech_inputs\n",
        "args['pos_outputs'] = pos_outputs\n",
        "args['num_data'] = num_data\n",
        "\n",
        "idx_train, idx_test, num_train, num_test, \\\n",
        "train_img_idx, train_speech_inputs, train_pos_outputs, \\\n",
        "test_img_idx, test_speech_inputs, test_pos_outputs = \\\n",
        "HGN_divide_train_test(args, model_type='speech2pickup')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading image..\n",
            "Loading 1/478\n",
            "Loading 2/478\n",
            "Loading 3/478\n",
            "Loading 4/478\n",
            "Loading 5/478\n",
            "Loading 6/478\n",
            "Loading 7/478\n",
            "Loading 8/478\n",
            "Loading 9/478\n",
            "Loading 10/478\n",
            "Loading 11/478\n",
            "Loading 12/478\n",
            "Loading 13/478\n",
            "Loading 14/478\n",
            "Loading 15/478\n",
            "Loading 16/478\n",
            "Loading 17/478\n",
            "Loading 18/478\n",
            "Loading 19/478\n",
            "Loading 20/478\n",
            "Loading 21/478\n",
            "Loading 22/478\n",
            "Loading 23/478\n",
            "Loading 24/478\n",
            "Loading 25/478\n",
            "Loading 26/478\n",
            "Loading 27/478\n",
            "Loading 28/478\n",
            "Loading 29/478\n",
            "Loading 30/478\n",
            "Loading 31/478\n",
            "Loading 32/478\n",
            "Loading 33/478\n",
            "Loading 34/478\n",
            "Loading 35/478\n",
            "Loading 36/478\n",
            "Loading 37/478\n",
            "Loading 38/478\n",
            "Loading 39/478\n",
            "Loading 40/478\n",
            "Loading 41/478\n",
            "Loading 42/478\n",
            "Loading 43/478\n",
            "Loading 44/478\n",
            "Loading 45/478\n",
            "Loading 46/478\n",
            "Loading 47/478\n",
            "Loading 48/478\n",
            "Loading 49/478\n",
            "Loading 50/478\n",
            "Loading 51/478\n",
            "Loading 52/478\n",
            "Loading 53/478\n",
            "Loading 54/478\n",
            "Loading 55/478\n",
            "Loading 56/478\n",
            "Loading 57/478\n",
            "Loading 58/478\n",
            "Loading 59/478\n",
            "Loading 60/478\n",
            "Loading 61/478\n",
            "Loading 62/478\n",
            "Loading 63/478\n",
            "Loading 64/478\n",
            "Loading 65/478\n",
            "Loading 66/478\n",
            "Loading 67/478\n",
            "Loading 68/478\n",
            "Loading 69/478\n",
            "Loading 70/478\n",
            "Loading 71/478\n",
            "Loading 72/478\n",
            "Loading 73/478\n",
            "Loading 74/478\n",
            "Loading 75/478\n",
            "Loading 76/478\n",
            "Loading 77/478\n",
            "Loading 78/478\n",
            "Loading 79/478\n",
            "Loading 80/478\n",
            "Loading 81/478\n",
            "Loading 82/478\n",
            "Loading 83/478\n",
            "Loading 84/478\n",
            "Loading 85/478\n",
            "Loading 86/478\n",
            "Loading 87/478\n",
            "Loading 88/478\n",
            "Loading 89/478\n",
            "Loading 90/478\n",
            "Loading 91/478\n",
            "Loading 92/478\n",
            "Loading 93/478\n",
            "Loading 94/478\n",
            "Loading 95/478\n",
            "Loading 96/478\n",
            "Loading 97/478\n",
            "Loading 98/478\n",
            "Loading 99/478\n",
            "Loading 100/478\n",
            "Loading 101/478\n",
            "Loading 102/478\n",
            "Loading 103/478\n",
            "Loading 104/478\n",
            "Loading 105/478\n",
            "Loading 106/478\n",
            "Loading 107/478\n",
            "Loading 108/478\n",
            "Loading 109/478\n",
            "Loading 110/478\n",
            "Loading 111/478\n",
            "Loading 112/478\n",
            "Loading 113/478\n",
            "Loading 114/478\n",
            "Loading 115/478\n",
            "Loading 116/478\n",
            "Loading 117/478\n",
            "Loading 118/478\n",
            "Loading 119/478\n",
            "Loading 120/478\n",
            "Loading 121/478\n",
            "Loading 122/478\n",
            "Loading 123/478\n",
            "Loading 124/478\n",
            "Loading 125/478\n",
            "Loading 126/478\n",
            "Loading 127/478\n",
            "Loading 128/478\n",
            "Loading 129/478\n",
            "Loading 130/478\n",
            "Loading 131/478\n",
            "Loading 132/478\n",
            "Loading 133/478\n",
            "Loading 134/478\n",
            "Loading 135/478\n",
            "Loading 136/478\n",
            "Loading 137/478\n",
            "Loading 138/478\n",
            "Loading 139/478\n",
            "Loading 140/478\n",
            "Loading 141/478\n",
            "Loading 142/478\n",
            "Loading 143/478\n",
            "Loading 144/478\n",
            "Loading 145/478\n",
            "Loading 146/478\n",
            "Loading 147/478\n",
            "Loading 148/478\n",
            "Loading 149/478\n",
            "Loading 150/478\n",
            "Loading 151/478\n",
            "Loading 152/478\n",
            "Loading 153/478\n",
            "Loading 154/478\n",
            "Loading 155/478\n",
            "Loading 156/478\n",
            "Loading 157/478\n",
            "Loading 158/478\n",
            "Loading 159/478\n",
            "Loading 160/478\n",
            "Loading 161/478\n",
            "Loading 162/478\n",
            "Loading 163/478\n",
            "Loading 164/478\n",
            "Loading 165/478\n",
            "Loading 166/478\n",
            "Loading 167/478\n",
            "Loading 168/478\n",
            "Loading 169/478\n",
            "Loading 170/478\n",
            "Loading 171/478\n",
            "Loading 172/478\n",
            "Loading 173/478\n",
            "Loading 174/478\n",
            "Loading 175/478\n",
            "Loading 176/478\n",
            "Loading 177/478\n",
            "Loading 178/478\n",
            "Loading 179/478\n",
            "Loading 180/478\n",
            "Loading 181/478\n",
            "Loading 182/478\n",
            "Loading 183/478\n",
            "Loading 184/478\n",
            "Loading 185/478\n",
            "Loading 186/478\n",
            "Loading 187/478\n",
            "Loading 188/478\n",
            "Loading 189/478\n",
            "Loading 190/478\n",
            "Loading 191/478\n",
            "Loading 192/478\n",
            "Loading 193/478\n",
            "Loading 194/478\n",
            "Loading 195/478\n",
            "Loading 196/478\n",
            "Loading 197/478\n",
            "Loading 198/478\n",
            "Loading 199/478\n",
            "Loading 200/478\n",
            "Loading 201/478\n",
            "Loading 202/478\n",
            "Loading 203/478\n",
            "Loading 204/478\n",
            "Loading 205/478\n",
            "Loading 206/478\n",
            "Loading 207/478\n",
            "Loading 208/478\n",
            "Loading 209/478\n",
            "Loading 210/478\n",
            "Loading 211/478\n",
            "Loading 212/478\n",
            "Loading 213/478\n",
            "Loading 214/478\n",
            "Loading 215/478\n",
            "Loading 216/478\n",
            "Loading 217/478\n",
            "Loading 218/478\n",
            "Loading 219/478\n",
            "Loading 220/478\n",
            "Loading 221/478\n",
            "Loading 222/478\n",
            "Loading 223/478\n",
            "Loading 224/478\n",
            "Loading 225/478\n",
            "Loading 226/478\n",
            "Loading 227/478\n",
            "Loading 228/478\n",
            "Loading 229/478\n",
            "Loading 230/478\n",
            "Loading 231/478\n",
            "Loading 232/478\n",
            "Loading 233/478\n",
            "Loading 234/478\n",
            "Loading 235/478\n",
            "Loading 236/478\n",
            "Loading 237/478\n",
            "Loading 238/478\n",
            "Loading 239/478\n",
            "Loading 240/478\n",
            "Loading 241/478\n",
            "Loading 242/478\n",
            "Loading 243/478\n",
            "Loading 244/478\n",
            "Loading 245/478\n",
            "Loading 246/478\n",
            "Loading 247/478\n",
            "Loading 248/478\n",
            "Loading 249/478\n",
            "Loading 250/478\n",
            "Loading 251/478\n",
            "Loading 252/478\n",
            "Loading 253/478\n",
            "Loading 254/478\n",
            "Loading 255/478\n",
            "Loading 256/478\n",
            "Loading 257/478\n",
            "Loading 258/478\n",
            "Loading 259/478\n",
            "Loading 260/478\n",
            "Loading 261/478\n",
            "Loading 262/478\n",
            "Loading 263/478\n",
            "Loading 264/478\n",
            "Loading 265/478\n",
            "Loading 266/478\n",
            "Loading 267/478\n",
            "Loading 268/478\n",
            "Loading 269/478\n",
            "Loading 270/478\n",
            "Loading 271/478\n",
            "Loading 272/478\n",
            "Loading 273/478\n",
            "Loading 274/478\n",
            "Loading 275/478\n",
            "Loading 276/478\n",
            "Loading 277/478\n",
            "Loading 278/478\n",
            "Loading 279/478\n",
            "Loading 280/478\n",
            "Loading 281/478\n",
            "Loading 282/478\n",
            "Loading 283/478\n",
            "Loading 284/478\n",
            "Loading 285/478\n",
            "Loading 286/478\n",
            "Loading 287/478\n",
            "Loading 288/478\n",
            "Loading 289/478\n",
            "Loading 290/478\n",
            "Loading 291/478\n",
            "Loading 292/478\n",
            "Loading 293/478\n",
            "Loading 294/478\n",
            "Loading 295/478\n",
            "Loading 296/478\n",
            "Loading 297/478\n",
            "Loading 298/478\n",
            "Loading 299/478\n",
            "Loading 300/478\n",
            "Loading 301/478\n",
            "Loading 302/478\n",
            "Loading 303/478\n",
            "Loading 304/478\n",
            "Loading 305/478\n",
            "Loading 306/478\n",
            "Loading 307/478\n",
            "Loading 308/478\n",
            "Loading 309/478\n",
            "Loading 310/478\n",
            "Loading 311/478\n",
            "Loading 312/478\n",
            "Loading 313/478\n",
            "Loading 314/478\n",
            "Loading 315/478\n",
            "Loading 316/478\n",
            "Loading 317/478\n",
            "Loading 318/478\n",
            "Loading 319/478\n",
            "Loading 320/478\n",
            "Loading 321/478\n",
            "Loading 322/478\n",
            "Loading 323/478\n",
            "Loading 324/478\n",
            "Loading 325/478\n",
            "Loading 326/478\n",
            "Loading 327/478\n",
            "Loading 328/478\n",
            "Loading 329/478\n",
            "Loading 330/478\n",
            "Loading 331/478\n",
            "Loading 332/478\n",
            "Loading 333/478\n",
            "Loading 334/478\n",
            "Loading 335/478\n",
            "Loading 336/478\n",
            "Loading 337/478\n",
            "Loading 338/478\n",
            "Loading 339/478\n",
            "Loading 340/478\n",
            "Loading 341/478\n",
            "Loading 342/478\n",
            "Loading 343/478\n",
            "Loading 344/478\n",
            "Loading 345/478\n",
            "Loading 346/478\n",
            "Loading 347/478\n",
            "Loading 348/478\n",
            "Loading 349/478\n",
            "Loading 350/478\n",
            "Loading 351/478\n",
            "Loading 352/478\n",
            "Loading 353/478\n",
            "Loading 354/478\n",
            "Loading 355/478\n",
            "Loading 356/478\n",
            "Loading 357/478\n",
            "Loading 358/478\n",
            "Loading 359/478\n",
            "Loading 360/478\n",
            "Loading 361/478\n",
            "Loading 362/478\n",
            "Loading 363/478\n",
            "Loading 364/478\n",
            "Loading 365/478\n",
            "Loading 366/478\n",
            "Loading 367/478\n",
            "Loading 368/478\n",
            "Loading 369/478\n",
            "Loading 370/478\n",
            "Loading 371/478\n",
            "Loading 372/478\n",
            "Loading 373/478\n",
            "Loading 374/478\n",
            "Loading 375/478\n",
            "Loading 376/478\n",
            "Loading 377/478\n",
            "Loading 378/478\n",
            "Loading 379/478\n",
            "Loading 380/478\n",
            "Loading 381/478\n",
            "Loading 382/478\n",
            "Loading 383/478\n",
            "Loading 384/478\n",
            "Loading 385/478\n",
            "Loading 386/478\n",
            "Loading 387/478\n",
            "Loading 388/478\n",
            "Loading 389/478\n",
            "Loading 390/478\n",
            "Loading 391/478\n",
            "Loading 392/478\n",
            "Loading 393/478\n",
            "Loading 394/478\n",
            "Loading 395/478\n",
            "Loading 396/478\n",
            "Loading 397/478\n",
            "Loading 398/478\n",
            "Loading 399/478\n",
            "Loading 400/478\n",
            "Loading 401/478\n",
            "Loading 402/478\n",
            "Loading 403/478\n",
            "Loading 404/478\n",
            "Loading 405/478\n",
            "Loading 406/478\n",
            "Loading 407/478\n",
            "Loading 408/478\n",
            "Loading 409/478\n",
            "Loading 410/478\n",
            "Loading 411/478\n",
            "Loading 412/478\n",
            "Loading 413/478\n",
            "Loading 414/478\n",
            "Loading 415/478\n",
            "Loading 416/478\n",
            "Loading 417/478\n",
            "Loading 418/478\n",
            "Loading 419/478\n",
            "Loading 420/478\n",
            "Loading 421/478\n",
            "Loading 422/478\n",
            "Loading 423/478\n",
            "Loading 424/478\n",
            "Loading 425/478\n",
            "Loading 426/478\n",
            "Loading 427/478\n",
            "Loading 428/478\n",
            "Loading 429/478\n",
            "Loading 430/478\n",
            "Loading 431/478\n",
            "Loading 432/478\n",
            "Loading 433/478\n",
            "Loading 434/478\n",
            "Loading 435/478\n",
            "Loading 436/478\n",
            "Loading 437/478\n",
            "Loading 438/478\n",
            "Loading 439/478\n",
            "Loading 440/478\n",
            "Loading 441/478\n",
            "Loading 442/478\n",
            "Loading 443/478\n",
            "Loading 444/478\n",
            "Loading 445/478\n",
            "Loading 446/478\n",
            "Loading 447/478\n",
            "Loading 448/478\n",
            "Loading 449/478\n",
            "Loading 450/478\n",
            "Loading 451/478\n",
            "Loading 452/478\n",
            "Loading 453/478\n",
            "Loading 454/478\n",
            "Loading 455/478\n",
            "Loading 456/478\n",
            "Loading 457/478\n",
            "Loading 458/478\n",
            "Loading 459/478\n",
            "Loading 460/478\n",
            "Loading 461/478\n",
            "Loading 462/478\n",
            "Loading 463/478\n",
            "Loading 464/478\n",
            "Loading 465/478\n",
            "Loading 466/478\n",
            "Loading 467/478\n",
            "Loading 468/478\n",
            "Loading 469/478\n",
            "Loading 470/478\n",
            "Loading 471/478\n",
            "Loading 472/478\n",
            "Loading 473/478\n",
            "Loading 474/478\n",
            "Loading 475/478\n",
            "Loading 476/478\n",
            "Loading 477/478\n",
            "Loading 478/478\n",
            "Loading heatmap..\n",
            "Loading 1/2134\n",
            "Loading 2/2134\n",
            "Loading 3/2134\n",
            "Loading 4/2134\n",
            "Loading 5/2134\n",
            "Loading 6/2134\n",
            "Loading 7/2134\n",
            "Loading 8/2134\n",
            "Loading 9/2134\n",
            "Loading 10/2134\n",
            "Loading 11/2134\n",
            "Loading 12/2134\n",
            "Loading 13/2134\n",
            "Loading 14/2134\n",
            "Loading 15/2134\n",
            "Loading 16/2134\n",
            "Loading 17/2134\n",
            "Loading 18/2134\n",
            "Loading 19/2134\n",
            "Loading 20/2134\n",
            "Loading 21/2134\n",
            "Loading 22/2134\n",
            "Loading 23/2134\n",
            "Loading 24/2134\n",
            "Loading 25/2134\n",
            "Loading 26/2134\n",
            "Loading 27/2134\n",
            "Loading 28/2134\n",
            "Loading 29/2134\n",
            "Loading 30/2134\n",
            "Loading 31/2134\n",
            "Loading 32/2134\n",
            "Loading 33/2134\n",
            "Loading 34/2134\n",
            "Loading 35/2134\n",
            "Loading 36/2134\n",
            "Loading 37/2134\n",
            "Loading 38/2134\n",
            "Loading 39/2134\n",
            "Loading 40/2134\n",
            "Loading 41/2134\n",
            "Loading 42/2134\n",
            "Loading 43/2134\n",
            "Loading 44/2134\n",
            "Loading 45/2134\n",
            "Loading 46/2134\n",
            "Loading 47/2134\n",
            "Loading 48/2134\n",
            "Loading 49/2134\n",
            "Loading 50/2134\n",
            "Loading 51/2134\n",
            "Loading 52/2134\n",
            "Loading 53/2134\n",
            "Loading 54/2134\n",
            "Loading 55/2134\n",
            "Loading 56/2134\n",
            "Loading 57/2134\n",
            "Loading 58/2134\n",
            "Loading 59/2134\n",
            "Loading 60/2134\n",
            "Loading 61/2134\n",
            "Loading 62/2134\n",
            "Loading 63/2134\n",
            "Loading 64/2134\n",
            "Loading 65/2134\n",
            "Loading 66/2134\n",
            "Loading 67/2134\n",
            "Loading 68/2134\n",
            "Loading 69/2134\n",
            "Loading 70/2134\n",
            "Loading 71/2134\n",
            "Loading 72/2134\n",
            "Loading 73/2134\n",
            "Loading 74/2134\n",
            "Loading 75/2134\n",
            "Loading 76/2134\n",
            "Loading 77/2134\n",
            "Loading 78/2134\n",
            "Loading 79/2134\n",
            "Loading 80/2134\n",
            "Loading 81/2134\n",
            "Loading 82/2134\n",
            "Loading 83/2134\n",
            "Loading 84/2134\n",
            "Loading 85/2134\n",
            "Loading 86/2134\n",
            "Loading 87/2134\n",
            "Loading 88/2134\n",
            "Loading 89/2134\n",
            "Loading 90/2134\n",
            "Loading 91/2134\n",
            "Loading 92/2134\n",
            "Loading 93/2134\n",
            "Loading 94/2134\n",
            "Loading 95/2134\n",
            "Loading 96/2134\n",
            "Loading 97/2134\n",
            "Loading 98/2134\n",
            "Loading 99/2134\n",
            "Loading 100/2134\n",
            "Loading 101/2134\n",
            "Loading 102/2134\n",
            "Loading 103/2134\n",
            "Loading 104/2134\n",
            "Loading 105/2134\n",
            "Loading 106/2134\n",
            "Loading 107/2134\n",
            "Loading 108/2134\n",
            "Loading 109/2134\n",
            "Loading 110/2134\n",
            "Loading 111/2134\n",
            "Loading 112/2134\n",
            "Loading 113/2134\n",
            "Loading 114/2134\n",
            "Loading 115/2134\n",
            "Loading 116/2134\n",
            "Loading 117/2134\n",
            "Loading 118/2134\n",
            "Loading 119/2134\n",
            "Loading 120/2134\n",
            "Loading 121/2134\n",
            "Loading 122/2134\n",
            "Loading 123/2134\n",
            "Loading 124/2134\n",
            "Loading 125/2134\n",
            "Loading 126/2134\n",
            "Loading 127/2134\n",
            "Loading 128/2134\n",
            "Loading 129/2134\n",
            "Loading 130/2134\n",
            "Loading 131/2134\n",
            "Loading 132/2134\n",
            "Loading 133/2134\n",
            "Loading 134/2134\n",
            "Loading 135/2134\n",
            "Loading 136/2134\n",
            "Loading 137/2134\n",
            "Loading 138/2134\n",
            "Loading 139/2134\n",
            "Loading 140/2134\n",
            "Loading 141/2134\n",
            "Loading 142/2134\n",
            "Loading 143/2134\n",
            "Loading 144/2134\n",
            "Loading 145/2134\n",
            "Loading 146/2134\n",
            "Loading 147/2134\n",
            "Loading 148/2134\n",
            "Loading 149/2134\n",
            "Loading 150/2134\n",
            "Loading 151/2134\n",
            "Loading 152/2134\n",
            "Loading 153/2134\n",
            "Loading 154/2134\n",
            "Loading 155/2134\n",
            "Loading 156/2134\n",
            "Loading 157/2134\n",
            "Loading 158/2134\n",
            "Loading 159/2134\n",
            "Loading 160/2134\n",
            "Loading 161/2134\n",
            "Loading 162/2134\n",
            "Loading 163/2134\n",
            "Loading 164/2134\n",
            "Loading 165/2134\n",
            "Loading 166/2134\n",
            "Loading 167/2134\n",
            "Loading 168/2134\n",
            "Loading 169/2134\n",
            "Loading 170/2134\n",
            "Loading 171/2134\n",
            "Loading 172/2134\n",
            "Loading 173/2134\n",
            "Loading 174/2134\n",
            "Loading 175/2134\n",
            "Loading 176/2134\n",
            "Loading 177/2134\n",
            "Loading 178/2134\n",
            "Loading 179/2134\n",
            "Loading 180/2134\n",
            "Loading 181/2134\n",
            "Loading 182/2134\n",
            "Loading 183/2134\n",
            "Loading 184/2134\n",
            "Loading 185/2134\n",
            "Loading 186/2134\n",
            "Loading 187/2134\n",
            "Loading 188/2134\n",
            "Loading 189/2134\n",
            "Loading 190/2134\n",
            "Loading 191/2134\n",
            "Loading 192/2134\n",
            "Loading 193/2134\n",
            "Loading 194/2134\n",
            "Loading 195/2134\n",
            "Loading 196/2134\n",
            "Loading 197/2134\n",
            "Loading 198/2134\n",
            "Loading 199/2134\n",
            "Loading 200/2134\n",
            "Loading 201/2134\n",
            "Loading 202/2134\n",
            "Loading 203/2134\n",
            "Loading 204/2134\n",
            "Loading 205/2134\n",
            "Loading 206/2134\n",
            "Loading 207/2134\n",
            "Loading 208/2134\n",
            "Loading 209/2134\n",
            "Loading 210/2134\n",
            "Loading 211/2134\n",
            "Loading 212/2134\n",
            "Loading 213/2134\n",
            "Loading 214/2134\n",
            "Loading 215/2134\n",
            "Loading 216/2134\n",
            "Loading 217/2134\n",
            "Loading 218/2134\n",
            "Loading 219/2134\n",
            "Loading 220/2134\n",
            "Loading 221/2134\n",
            "Loading 222/2134\n",
            "Loading 223/2134\n",
            "Loading 224/2134\n",
            "Loading 225/2134\n",
            "Loading 226/2134\n",
            "Loading 227/2134\n",
            "Loading 228/2134\n",
            "Loading 229/2134\n",
            "Loading 230/2134\n",
            "Loading 231/2134\n",
            "Loading 232/2134\n",
            "Loading 233/2134\n",
            "Loading 234/2134\n",
            "Loading 235/2134\n",
            "Loading 236/2134\n",
            "Loading 237/2134\n",
            "Loading 238/2134\n",
            "Loading 239/2134\n",
            "Loading 240/2134\n",
            "Loading 241/2134\n",
            "Loading 242/2134\n",
            "Loading 243/2134\n",
            "Loading 244/2134\n",
            "Loading 245/2134\n",
            "Loading 246/2134\n",
            "Loading 247/2134\n",
            "Loading 248/2134\n",
            "Loading 249/2134\n",
            "Loading 250/2134\n",
            "Loading 251/2134\n",
            "Loading 252/2134\n",
            "Loading 253/2134\n",
            "Loading 254/2134\n",
            "Loading 255/2134\n",
            "Loading 256/2134\n",
            "Loading 257/2134\n",
            "Loading 258/2134\n",
            "Loading 259/2134\n",
            "Loading 260/2134\n",
            "Loading 261/2134\n",
            "Loading 262/2134\n",
            "Loading 263/2134\n",
            "Loading 264/2134\n",
            "Loading 265/2134\n",
            "Loading 266/2134\n",
            "Loading 267/2134\n",
            "Loading 268/2134\n",
            "Loading 269/2134\n",
            "Loading 270/2134\n",
            "Loading 271/2134\n",
            "Loading 272/2134\n",
            "Loading 273/2134\n",
            "Loading 274/2134\n",
            "Loading 275/2134\n",
            "Loading 276/2134\n",
            "Loading 277/2134\n",
            "Loading 278/2134\n",
            "Loading 279/2134\n",
            "Loading 280/2134\n",
            "Loading 281/2134\n",
            "Loading 282/2134\n",
            "Loading 283/2134\n",
            "Loading 284/2134\n",
            "Loading 285/2134\n",
            "Loading 286/2134\n",
            "Loading 287/2134\n",
            "Loading 288/2134\n",
            "Loading 289/2134\n",
            "Loading 290/2134\n",
            "Loading 291/2134\n",
            "Loading 292/2134\n",
            "Loading 293/2134\n",
            "Loading 294/2134\n",
            "Loading 295/2134\n",
            "Loading 296/2134\n",
            "Loading 297/2134\n",
            "Loading 298/2134\n",
            "Loading 299/2134\n",
            "Loading 300/2134\n",
            "Loading 301/2134\n",
            "Loading 302/2134\n",
            "Loading 303/2134\n",
            "Loading 304/2134\n",
            "Loading 305/2134\n",
            "Loading 306/2134\n",
            "Loading 307/2134\n",
            "Loading 308/2134\n",
            "Loading 309/2134\n",
            "Loading 310/2134\n",
            "Loading 311/2134\n",
            "Loading 312/2134\n",
            "Loading 313/2134\n",
            "Loading 314/2134\n",
            "Loading 315/2134\n",
            "Loading 316/2134\n",
            "Loading 317/2134\n",
            "Loading 318/2134\n",
            "Loading 319/2134\n",
            "Loading 320/2134\n",
            "Loading 321/2134\n",
            "Loading 322/2134\n",
            "Loading 323/2134\n",
            "Loading 324/2134\n",
            "Loading 325/2134\n",
            "Loading 326/2134\n",
            "Loading 327/2134\n",
            "Loading 328/2134\n",
            "Loading 329/2134\n",
            "Loading 330/2134\n",
            "Loading 331/2134\n",
            "Loading 332/2134\n",
            "Loading 333/2134\n",
            "Loading 334/2134\n",
            "Loading 335/2134\n",
            "Loading 336/2134\n",
            "Loading 337/2134\n",
            "Loading 338/2134\n",
            "Loading 339/2134\n",
            "Loading 340/2134\n",
            "Loading 341/2134\n",
            "Loading 342/2134\n",
            "Loading 343/2134\n",
            "Loading 344/2134\n",
            "Loading 345/2134\n",
            "Loading 346/2134\n",
            "Loading 347/2134\n",
            "Loading 348/2134\n",
            "Loading 349/2134\n",
            "Loading 350/2134\n",
            "Loading 351/2134\n",
            "Loading 352/2134\n",
            "Loading 353/2134\n",
            "Loading 354/2134\n",
            "Loading 355/2134\n",
            "Loading 356/2134\n",
            "Loading 357/2134\n",
            "Loading 358/2134\n",
            "Loading 359/2134\n",
            "Loading 360/2134\n",
            "Loading 361/2134\n",
            "Loading 362/2134\n",
            "Loading 363/2134\n",
            "Loading 364/2134\n",
            "Loading 365/2134\n",
            "Loading 366/2134\n",
            "Loading 367/2134\n",
            "Loading 368/2134\n",
            "Loading 369/2134\n",
            "Loading 370/2134\n",
            "Loading 371/2134\n",
            "Loading 372/2134\n",
            "Loading 373/2134\n",
            "Loading 374/2134\n",
            "Loading 375/2134\n",
            "Loading 376/2134\n",
            "Loading 377/2134\n",
            "Loading 378/2134\n",
            "Loading 379/2134\n",
            "Loading 380/2134\n",
            "Loading 381/2134\n",
            "Loading 382/2134\n",
            "Loading 383/2134\n",
            "Loading 384/2134\n",
            "Loading 385/2134\n",
            "Loading 386/2134\n",
            "Loading 387/2134\n",
            "Loading 388/2134\n",
            "Loading 389/2134\n",
            "Loading 390/2134\n",
            "Loading 391/2134\n",
            "Loading 392/2134\n",
            "Loading 393/2134\n",
            "Loading 394/2134\n",
            "Loading 395/2134\n",
            "Loading 396/2134\n",
            "Loading 397/2134\n",
            "Loading 398/2134\n",
            "Loading 399/2134\n",
            "Loading 400/2134\n",
            "Loading 401/2134\n",
            "Loading 402/2134\n",
            "Loading 403/2134\n",
            "Loading 404/2134\n",
            "Loading 405/2134\n",
            "Loading 406/2134\n",
            "Loading 407/2134\n",
            "Loading 408/2134\n",
            "Loading 409/2134\n",
            "Loading 410/2134\n",
            "Loading 411/2134\n",
            "Loading 412/2134\n",
            "Loading 413/2134\n",
            "Loading 414/2134\n",
            "Loading 415/2134\n",
            "Loading 416/2134\n",
            "Loading 417/2134\n",
            "Loading 418/2134\n",
            "Loading 419/2134\n",
            "Loading 420/2134\n",
            "Loading 421/2134\n",
            "Loading 422/2134\n",
            "Loading 423/2134\n",
            "Loading 424/2134\n",
            "Loading 425/2134\n",
            "Loading 426/2134\n",
            "Loading 427/2134\n",
            "Loading 428/2134\n",
            "Loading 429/2134\n",
            "Loading 430/2134\n",
            "Loading 431/2134\n",
            "Loading 432/2134\n",
            "Loading 433/2134\n",
            "Loading 434/2134\n",
            "Loading 435/2134\n",
            "Loading 436/2134\n",
            "Loading 437/2134\n",
            "Loading 438/2134\n",
            "Loading 439/2134\n",
            "Loading 440/2134\n",
            "Loading 441/2134\n",
            "Loading 442/2134\n",
            "Loading 443/2134\n",
            "Loading 444/2134\n",
            "Loading 445/2134\n",
            "Loading 446/2134\n",
            "Loading 447/2134\n",
            "Loading 448/2134\n",
            "Loading 449/2134\n",
            "Loading 450/2134\n",
            "Loading 451/2134\n",
            "Loading 452/2134\n",
            "Loading 453/2134\n",
            "Loading 454/2134\n",
            "Loading 455/2134\n",
            "Loading 456/2134\n",
            "Loading 457/2134\n",
            "Loading 458/2134\n",
            "Loading 459/2134\n",
            "Loading 460/2134\n",
            "Loading 461/2134\n",
            "Loading 462/2134\n",
            "Loading 463/2134\n",
            "Loading 464/2134\n",
            "Loading 465/2134\n",
            "Loading 466/2134\n",
            "Loading 467/2134\n",
            "Loading 468/2134\n",
            "Loading 469/2134\n",
            "Loading 470/2134\n",
            "Loading 471/2134\n",
            "Loading 472/2134\n",
            "Loading 473/2134\n",
            "Loading 474/2134\n",
            "Loading 475/2134\n",
            "Loading 476/2134\n",
            "Loading 477/2134\n",
            "Loading 478/2134\n",
            "Loading 479/2134\n",
            "Loading 480/2134\n",
            "Loading 481/2134\n",
            "Loading 482/2134\n",
            "Loading 483/2134\n",
            "Loading 484/2134\n",
            "Loading 485/2134\n",
            "Loading 486/2134\n",
            "Loading 487/2134\n",
            "Loading 488/2134\n",
            "Loading 489/2134\n",
            "Loading 490/2134\n",
            "Loading 491/2134\n",
            "Loading 492/2134\n",
            "Loading 493/2134\n",
            "Loading 494/2134\n",
            "Loading 495/2134\n",
            "Loading 496/2134\n",
            "Loading 497/2134\n",
            "Loading 498/2134\n",
            "Loading 499/2134\n",
            "Loading 500/2134\n",
            "Loading 501/2134\n",
            "Loading 502/2134\n",
            "Loading 503/2134\n",
            "Loading 504/2134\n",
            "Loading 505/2134\n",
            "Loading 506/2134\n",
            "Loading 507/2134\n",
            "Loading 508/2134\n",
            "Loading 509/2134\n",
            "Loading 510/2134\n",
            "Loading 511/2134\n",
            "Loading 512/2134\n",
            "Loading 513/2134\n",
            "Loading 514/2134\n",
            "Loading 515/2134\n",
            "Loading 516/2134\n",
            "Loading 517/2134\n",
            "Loading 518/2134\n",
            "Loading 519/2134\n",
            "Loading 520/2134\n",
            "Loading 521/2134\n",
            "Loading 522/2134\n",
            "Loading 523/2134\n",
            "Loading 524/2134\n",
            "Loading 525/2134\n",
            "Loading 526/2134\n",
            "Loading 527/2134\n",
            "Loading 528/2134\n",
            "Loading 529/2134\n",
            "Loading 530/2134\n",
            "Loading 531/2134\n",
            "Loading 532/2134\n",
            "Loading 533/2134\n",
            "Loading 534/2134\n",
            "Loading 535/2134\n",
            "Loading 536/2134\n",
            "Loading 537/2134\n",
            "Loading 538/2134\n",
            "Loading 539/2134\n",
            "Loading 540/2134\n",
            "Loading 541/2134\n",
            "Loading 542/2134\n",
            "Loading 543/2134\n",
            "Loading 544/2134\n",
            "Loading 545/2134\n",
            "Loading 546/2134\n",
            "Loading 547/2134\n",
            "Loading 548/2134\n",
            "Loading 549/2134\n",
            "Loading 550/2134\n",
            "Loading 551/2134\n",
            "Loading 552/2134\n",
            "Loading 553/2134\n",
            "Loading 554/2134\n",
            "Loading 555/2134\n",
            "Loading 556/2134\n",
            "Loading 557/2134\n",
            "Loading 558/2134\n",
            "Loading 559/2134\n",
            "Loading 560/2134\n",
            "Loading 561/2134\n",
            "Loading 562/2134\n",
            "Loading 563/2134\n",
            "Loading 564/2134\n",
            "Loading 565/2134\n",
            "Loading 566/2134\n",
            "Loading 567/2134\n",
            "Loading 568/2134\n",
            "Loading 569/2134\n",
            "Loading 570/2134\n",
            "Loading 571/2134\n",
            "Loading 572/2134\n",
            "Loading 573/2134\n",
            "Loading 574/2134\n",
            "Loading 575/2134\n",
            "Loading 576/2134\n",
            "Loading 577/2134\n",
            "Loading 578/2134\n",
            "Loading 579/2134\n",
            "Loading 580/2134\n",
            "Loading 581/2134\n",
            "Loading 582/2134\n",
            "Loading 583/2134\n",
            "Loading 584/2134\n",
            "Loading 585/2134\n",
            "Loading 586/2134\n",
            "Loading 587/2134\n",
            "Loading 588/2134\n",
            "Loading 589/2134\n",
            "Loading 590/2134\n",
            "Loading 591/2134\n",
            "Loading 592/2134\n",
            "Loading 593/2134\n",
            "Loading 594/2134\n",
            "Loading 595/2134\n",
            "Loading 596/2134\n",
            "Loading 597/2134\n",
            "Loading 598/2134\n",
            "Loading 599/2134\n",
            "Loading 600/2134\n",
            "Loading 601/2134\n",
            "Loading 602/2134\n",
            "Loading 603/2134\n",
            "Loading 604/2134\n",
            "Loading 605/2134\n",
            "Loading 606/2134\n",
            "Loading 607/2134\n",
            "Loading 608/2134\n",
            "Loading 609/2134\n",
            "Loading 610/2134\n",
            "Loading 611/2134\n",
            "Loading 612/2134\n",
            "Loading 613/2134\n",
            "Loading 614/2134\n",
            "Loading 615/2134\n",
            "Loading 616/2134\n",
            "Loading 617/2134\n",
            "Loading 618/2134\n",
            "Loading 619/2134\n",
            "Loading 620/2134\n",
            "Loading 621/2134\n",
            "Loading 622/2134\n",
            "Loading 623/2134\n",
            "Loading 624/2134\n",
            "Loading 625/2134\n",
            "Loading 626/2134\n",
            "Loading 627/2134\n",
            "Loading 628/2134\n",
            "Loading 629/2134\n",
            "Loading 630/2134\n",
            "Loading 631/2134\n",
            "Loading 632/2134\n",
            "Loading 633/2134\n",
            "Loading 634/2134\n",
            "Loading 635/2134\n",
            "Loading 636/2134\n",
            "Loading 637/2134\n",
            "Loading 638/2134\n",
            "Loading 639/2134\n",
            "Loading 640/2134\n",
            "Loading 641/2134\n",
            "Loading 642/2134\n",
            "Loading 643/2134\n",
            "Loading 644/2134\n",
            "Loading 645/2134\n",
            "Loading 646/2134\n",
            "Loading 647/2134\n",
            "Loading 648/2134\n",
            "Loading 649/2134\n",
            "Loading 650/2134\n",
            "Loading 651/2134\n",
            "Loading 652/2134\n",
            "Loading 653/2134\n",
            "Loading 654/2134\n",
            "Loading 655/2134\n",
            "Loading 656/2134\n",
            "Loading 657/2134\n",
            "Loading 658/2134\n",
            "Loading 659/2134\n",
            "Loading 660/2134\n",
            "Loading 661/2134\n",
            "Loading 662/2134\n",
            "Loading 663/2134\n",
            "Loading 664/2134\n",
            "Loading 665/2134\n",
            "Loading 666/2134\n",
            "Loading 667/2134\n",
            "Loading 668/2134\n",
            "Loading 669/2134\n",
            "Loading 670/2134\n",
            "Loading 671/2134\n",
            "Loading 672/2134\n",
            "Loading 673/2134\n",
            "Loading 674/2134\n",
            "Loading 675/2134\n",
            "Loading 676/2134\n",
            "Loading 677/2134\n",
            "Loading 678/2134\n",
            "Loading 679/2134\n",
            "Loading 680/2134\n",
            "Loading 681/2134\n",
            "Loading 682/2134\n",
            "Loading 683/2134\n",
            "Loading 684/2134\n",
            "Loading 685/2134\n",
            "Loading 686/2134\n",
            "Loading 687/2134\n",
            "Loading 688/2134\n",
            "Loading 689/2134\n",
            "Loading 690/2134\n",
            "Loading 691/2134\n",
            "Loading 692/2134\n",
            "Loading 693/2134\n",
            "Loading 694/2134\n",
            "Loading 695/2134\n",
            "Loading 696/2134\n",
            "Loading 697/2134\n",
            "Loading 698/2134\n",
            "Loading 699/2134\n",
            "Loading 700/2134\n",
            "Loading 701/2134\n",
            "Loading 702/2134\n",
            "Loading 703/2134\n",
            "Loading 704/2134\n",
            "Loading 705/2134\n",
            "Loading 706/2134\n",
            "Loading 707/2134\n",
            "Loading 708/2134\n",
            "Loading 709/2134\n",
            "Loading 710/2134\n",
            "Loading 711/2134\n",
            "Loading 712/2134\n",
            "Loading 713/2134\n",
            "Loading 714/2134\n",
            "Loading 715/2134\n",
            "Loading 716/2134\n",
            "Loading 717/2134\n",
            "Loading 718/2134\n",
            "Loading 719/2134\n",
            "Loading 720/2134\n",
            "Loading 721/2134\n",
            "Loading 722/2134\n",
            "Loading 723/2134\n",
            "Loading 724/2134\n",
            "Loading 725/2134\n",
            "Loading 726/2134\n",
            "Loading 727/2134\n",
            "Loading 728/2134\n",
            "Loading 729/2134\n",
            "Loading 730/2134\n",
            "Loading 731/2134\n",
            "Loading 732/2134\n",
            "Loading 733/2134\n",
            "Loading 734/2134\n",
            "Loading 735/2134\n",
            "Loading 736/2134\n",
            "Loading 737/2134\n",
            "Loading 738/2134\n",
            "Loading 739/2134\n",
            "Loading 740/2134\n",
            "Loading 741/2134\n",
            "Loading 742/2134\n",
            "Loading 743/2134\n",
            "Loading 744/2134\n",
            "Loading 745/2134\n",
            "Loading 746/2134\n",
            "Loading 747/2134\n",
            "Loading 748/2134\n",
            "Loading 749/2134\n",
            "Loading 750/2134\n",
            "Loading 751/2134\n",
            "Loading 752/2134\n",
            "Loading 753/2134\n",
            "Loading 754/2134\n",
            "Loading 755/2134\n",
            "Loading 756/2134\n",
            "Loading 757/2134\n",
            "Loading 758/2134\n",
            "Loading 759/2134\n",
            "Loading 760/2134\n",
            "Loading 761/2134\n",
            "Loading 762/2134\n",
            "Loading 763/2134\n",
            "Loading 764/2134\n",
            "Loading 765/2134\n",
            "Loading 766/2134\n",
            "Loading 767/2134\n",
            "Loading 768/2134\n",
            "Loading 769/2134\n",
            "Loading 770/2134\n",
            "Loading 771/2134\n",
            "Loading 772/2134\n",
            "Loading 773/2134\n",
            "Loading 774/2134\n",
            "Loading 775/2134\n",
            "Loading 776/2134\n",
            "Loading 777/2134\n",
            "Loading 778/2134\n",
            "Loading 779/2134\n",
            "Loading 780/2134\n",
            "Loading 781/2134\n",
            "Loading 782/2134\n",
            "Loading 783/2134\n",
            "Loading 784/2134\n",
            "Loading 785/2134\n",
            "Loading 786/2134\n",
            "Loading 787/2134\n",
            "Loading 788/2134\n",
            "Loading 789/2134\n",
            "Loading 790/2134\n",
            "Loading 791/2134\n",
            "Loading 792/2134\n",
            "Loading 793/2134\n",
            "Loading 794/2134\n",
            "Loading 795/2134\n",
            "Loading 796/2134\n",
            "Loading 797/2134\n",
            "Loading 798/2134\n",
            "Loading 799/2134\n",
            "Loading 800/2134\n",
            "Loading 801/2134\n",
            "Loading 802/2134\n",
            "Loading 803/2134\n",
            "Loading 804/2134\n",
            "Loading 805/2134\n",
            "Loading 806/2134\n",
            "Loading 807/2134\n",
            "Loading 808/2134\n",
            "Loading 809/2134\n",
            "Loading 810/2134\n",
            "Loading 811/2134\n",
            "Loading 812/2134\n",
            "Loading 813/2134\n",
            "Loading 814/2134\n",
            "Loading 815/2134\n",
            "Loading 816/2134\n",
            "Loading 817/2134\n",
            "Loading 818/2134\n",
            "Loading 819/2134\n",
            "Loading 820/2134\n",
            "Loading 821/2134\n",
            "Loading 822/2134\n",
            "Loading 823/2134\n",
            "Loading 824/2134\n",
            "Loading 825/2134\n",
            "Loading 826/2134\n",
            "Loading 827/2134\n",
            "Loading 828/2134\n",
            "Loading 829/2134\n",
            "Loading 830/2134\n",
            "Loading 831/2134\n",
            "Loading 832/2134\n",
            "Loading 833/2134\n",
            "Loading 834/2134\n",
            "Loading 835/2134\n",
            "Loading 836/2134\n",
            "Loading 837/2134\n",
            "Loading 838/2134\n",
            "Loading 839/2134\n",
            "Loading 840/2134\n",
            "Loading 841/2134\n",
            "Loading 842/2134\n",
            "Loading 843/2134\n",
            "Loading 844/2134\n",
            "Loading 845/2134\n",
            "Loading 846/2134\n",
            "Loading 847/2134\n",
            "Loading 848/2134\n",
            "Loading 849/2134\n",
            "Loading 850/2134\n",
            "Loading 851/2134\n",
            "Loading 852/2134\n",
            "Loading 853/2134\n",
            "Loading 854/2134\n",
            "Loading 855/2134\n",
            "Loading 856/2134\n",
            "Loading 857/2134\n",
            "Loading 858/2134\n",
            "Loading 859/2134\n",
            "Loading 860/2134\n",
            "Loading 861/2134\n",
            "Loading 862/2134\n",
            "Loading 863/2134\n",
            "Loading 864/2134\n",
            "Loading 865/2134\n",
            "Loading 866/2134\n",
            "Loading 867/2134\n",
            "Loading 868/2134\n",
            "Loading 869/2134\n",
            "Loading 870/2134\n",
            "Loading 871/2134\n",
            "Loading 872/2134\n",
            "Loading 873/2134\n",
            "Loading 874/2134\n",
            "Loading 875/2134\n",
            "Loading 876/2134\n",
            "Loading 877/2134\n",
            "Loading 878/2134\n",
            "Loading 879/2134\n",
            "Loading 880/2134\n",
            "Loading 881/2134\n",
            "Loading 882/2134\n",
            "Loading 883/2134\n",
            "Loading 884/2134\n",
            "Loading 885/2134\n",
            "Loading 886/2134\n",
            "Loading 887/2134\n",
            "Loading 888/2134\n",
            "Loading 889/2134\n",
            "Loading 890/2134\n",
            "Loading 891/2134\n",
            "Loading 892/2134\n",
            "Loading 893/2134\n",
            "Loading 894/2134\n",
            "Loading 895/2134\n",
            "Loading 896/2134\n",
            "Loading 897/2134\n",
            "Loading 898/2134\n",
            "Loading 899/2134\n",
            "Loading 900/2134\n",
            "Loading 901/2134\n",
            "Loading 902/2134\n",
            "Loading 903/2134\n",
            "Loading 904/2134\n",
            "Loading 905/2134\n",
            "Loading 906/2134\n",
            "Loading 907/2134\n",
            "Loading 908/2134\n",
            "Loading 909/2134\n",
            "Loading 910/2134\n",
            "Loading 911/2134\n",
            "Loading 912/2134\n",
            "Loading 913/2134\n",
            "Loading 914/2134\n",
            "Loading 915/2134\n",
            "Loading 916/2134\n",
            "Loading 917/2134\n",
            "Loading 918/2134\n",
            "Loading 919/2134\n",
            "Loading 920/2134\n",
            "Loading 921/2134\n",
            "Loading 922/2134\n",
            "Loading 923/2134\n",
            "Loading 924/2134\n",
            "Loading 925/2134\n",
            "Loading 926/2134\n",
            "Loading 927/2134\n",
            "Loading 928/2134\n",
            "Loading 929/2134\n",
            "Loading 930/2134\n",
            "Loading 931/2134\n",
            "Loading 932/2134\n",
            "Loading 933/2134\n",
            "Loading 934/2134\n",
            "Loading 935/2134\n",
            "Loading 936/2134\n",
            "Loading 937/2134\n",
            "Loading 938/2134\n",
            "Loading 939/2134\n",
            "Loading 940/2134\n",
            "Loading 941/2134\n",
            "Loading 942/2134\n",
            "Loading 943/2134\n",
            "Loading 944/2134\n",
            "Loading 945/2134\n",
            "Loading 946/2134\n",
            "Loading 947/2134\n",
            "Loading 948/2134\n",
            "Loading 949/2134\n",
            "Loading 950/2134\n",
            "Loading 951/2134\n",
            "Loading 952/2134\n",
            "Loading 953/2134\n",
            "Loading 954/2134\n",
            "Loading 955/2134\n",
            "Loading 956/2134\n",
            "Loading 957/2134\n",
            "Loading 958/2134\n",
            "Loading 959/2134\n",
            "Loading 960/2134\n",
            "Loading 961/2134\n",
            "Loading 962/2134\n",
            "Loading 963/2134\n",
            "Loading 964/2134\n",
            "Loading 965/2134\n",
            "Loading 966/2134\n",
            "Loading 967/2134\n",
            "Loading 968/2134\n",
            "Loading 969/2134\n",
            "Loading 970/2134\n",
            "Loading 971/2134\n",
            "Loading 972/2134\n",
            "Loading 973/2134\n",
            "Loading 974/2134\n",
            "Loading 975/2134\n",
            "Loading 976/2134\n",
            "Loading 977/2134\n",
            "Loading 978/2134\n",
            "Loading 979/2134\n",
            "Loading 980/2134\n",
            "Loading 981/2134\n",
            "Loading 982/2134\n",
            "Loading 983/2134\n",
            "Loading 984/2134\n",
            "Loading 985/2134\n",
            "Loading 986/2134\n",
            "Loading 987/2134\n",
            "Loading 988/2134\n",
            "Loading 989/2134\n",
            "Loading 990/2134\n",
            "Loading 991/2134\n",
            "Loading 992/2134\n",
            "Loading 993/2134\n",
            "Loading 994/2134\n",
            "Loading 995/2134\n",
            "Loading 996/2134\n",
            "Loading 997/2134\n",
            "Loading 998/2134\n",
            "Loading 999/2134\n",
            "Loading 1000/2134\n",
            "Loading 1001/2134\n",
            "Loading 1002/2134\n",
            "Loading 1003/2134\n",
            "Loading 1004/2134\n",
            "Loading 1005/2134\n",
            "Loading 1006/2134\n",
            "Loading 1007/2134\n",
            "Loading 1008/2134\n",
            "Loading 1009/2134\n",
            "Loading 1010/2134\n",
            "Loading 1011/2134\n",
            "Loading 1012/2134\n",
            "Loading 1013/2134\n",
            "Loading 1014/2134\n",
            "Loading 1015/2134\n",
            "Loading 1016/2134\n",
            "Loading 1017/2134\n",
            "Loading 1018/2134\n",
            "Loading 1019/2134\n",
            "Loading 1020/2134\n",
            "Loading 1021/2134\n",
            "Loading 1022/2134\n",
            "Loading 1023/2134\n",
            "Loading 1024/2134\n",
            "Loading 1025/2134\n",
            "Loading 1026/2134\n",
            "Loading 1027/2134\n",
            "Loading 1028/2134\n",
            "Loading 1029/2134\n",
            "Loading 1030/2134\n",
            "Loading 1031/2134\n",
            "Loading 1032/2134\n",
            "Loading 1033/2134\n",
            "Loading 1034/2134\n",
            "Loading 1035/2134\n",
            "Loading 1036/2134\n",
            "Loading 1037/2134\n",
            "Loading 1038/2134\n",
            "Loading 1039/2134\n",
            "Loading 1040/2134\n",
            "Loading 1041/2134\n",
            "Loading 1042/2134\n",
            "Loading 1043/2134\n",
            "Loading 1044/2134\n",
            "Loading 1045/2134\n",
            "Loading 1046/2134\n",
            "Loading 1047/2134\n",
            "Loading 1048/2134\n",
            "Loading 1049/2134\n",
            "Loading 1050/2134\n",
            "Loading 1051/2134\n",
            "Loading 1052/2134\n",
            "Loading 1053/2134\n",
            "Loading 1054/2134\n",
            "Loading 1055/2134\n",
            "Loading 1056/2134\n",
            "Loading 1057/2134\n",
            "Loading 1058/2134\n",
            "Loading 1059/2134\n",
            "Loading 1060/2134\n",
            "Loading 1061/2134\n",
            "Loading 1062/2134\n",
            "Loading 1063/2134\n",
            "Loading 1064/2134\n",
            "Loading 1065/2134\n",
            "Loading 1066/2134\n",
            "Loading 1067/2134\n",
            "Loading 1068/2134\n",
            "Loading 1069/2134\n",
            "Loading 1070/2134\n",
            "Loading 1071/2134\n",
            "Loading 1072/2134\n",
            "Loading 1073/2134\n",
            "Loading 1074/2134\n",
            "Loading 1075/2134\n",
            "Loading 1076/2134\n",
            "Loading 1077/2134\n",
            "Loading 1078/2134\n",
            "Loading 1079/2134\n",
            "Loading 1080/2134\n",
            "Loading 1081/2134\n",
            "Loading 1082/2134\n",
            "Loading 1083/2134\n",
            "Loading 1084/2134\n",
            "Loading 1085/2134\n",
            "Loading 1086/2134\n",
            "Loading 1087/2134\n",
            "Loading 1088/2134\n",
            "Loading 1089/2134\n",
            "Loading 1090/2134\n",
            "Loading 1091/2134\n",
            "Loading 1092/2134\n",
            "Loading 1093/2134\n",
            "Loading 1094/2134\n",
            "Loading 1095/2134\n",
            "Loading 1096/2134\n",
            "Loading 1097/2134\n",
            "Loading 1098/2134\n",
            "Loading 1099/2134\n",
            "Loading 1100/2134\n",
            "Loading 1101/2134\n",
            "Loading 1102/2134\n",
            "Loading 1103/2134\n",
            "Loading 1104/2134\n",
            "Loading 1105/2134\n",
            "Loading 1106/2134\n",
            "Loading 1107/2134\n",
            "Loading 1108/2134\n",
            "Loading 1109/2134\n",
            "Loading 1110/2134\n",
            "Loading 1111/2134\n",
            "Loading 1112/2134\n",
            "Loading 1113/2134\n",
            "Loading 1114/2134\n",
            "Loading 1115/2134\n",
            "Loading 1116/2134\n",
            "Loading 1117/2134\n",
            "Loading 1118/2134\n",
            "Loading 1119/2134\n",
            "Loading 1120/2134\n",
            "Loading 1121/2134\n",
            "Loading 1122/2134\n",
            "Loading 1123/2134\n",
            "Loading 1124/2134\n",
            "Loading 1125/2134\n",
            "Loading 1126/2134\n",
            "Loading 1127/2134\n",
            "Loading 1128/2134\n",
            "Loading 1129/2134\n",
            "Loading 1130/2134\n",
            "Loading 1131/2134\n",
            "Loading 1132/2134\n",
            "Loading 1133/2134\n",
            "Loading 1134/2134\n",
            "Loading 1135/2134\n",
            "Loading 1136/2134\n",
            "Loading 1137/2134\n",
            "Loading 1138/2134\n",
            "Loading 1139/2134\n",
            "Loading 1140/2134\n",
            "Loading 1141/2134\n",
            "Loading 1142/2134\n",
            "Loading 1143/2134\n",
            "Loading 1144/2134\n",
            "Loading 1145/2134\n",
            "Loading 1146/2134\n",
            "Loading 1147/2134\n",
            "Loading 1148/2134\n",
            "Loading 1149/2134\n",
            "Loading 1150/2134\n",
            "Loading 1151/2134\n",
            "Loading 1152/2134\n",
            "Loading 1153/2134\n",
            "Loading 1154/2134\n",
            "Loading 1155/2134\n",
            "Loading 1156/2134\n",
            "Loading 1157/2134\n",
            "Loading 1158/2134\n",
            "Loading 1159/2134\n",
            "Loading 1160/2134\n",
            "Loading 1161/2134\n",
            "Loading 1162/2134\n",
            "Loading 1163/2134\n",
            "Loading 1164/2134\n",
            "Loading 1165/2134\n",
            "Loading 1166/2134\n",
            "Loading 1167/2134\n",
            "Loading 1168/2134\n",
            "Loading 1169/2134\n",
            "Loading 1170/2134\n",
            "Loading 1171/2134\n",
            "Loading 1172/2134\n",
            "Loading 1173/2134\n",
            "Loading 1174/2134\n",
            "Loading 1175/2134\n",
            "Loading 1176/2134\n",
            "Loading 1177/2134\n",
            "Loading 1178/2134\n",
            "Loading 1179/2134\n",
            "Loading 1180/2134\n",
            "Loading 1181/2134\n",
            "Loading 1182/2134\n",
            "Loading 1183/2134\n",
            "Loading 1184/2134\n",
            "Loading 1185/2134\n",
            "Loading 1186/2134\n",
            "Loading 1187/2134\n",
            "Loading 1188/2134\n",
            "Loading 1189/2134\n",
            "Loading 1190/2134\n",
            "Loading 1191/2134\n",
            "Loading 1192/2134\n",
            "Loading 1193/2134\n",
            "Loading 1194/2134\n",
            "Loading 1195/2134\n",
            "Loading 1196/2134\n",
            "Loading 1197/2134\n",
            "Loading 1198/2134\n",
            "Loading 1199/2134\n",
            "Loading 1200/2134\n",
            "Loading 1201/2134\n",
            "Loading 1202/2134\n",
            "Loading 1203/2134\n",
            "Loading 1204/2134\n",
            "Loading 1205/2134\n",
            "Loading 1206/2134\n",
            "Loading 1207/2134\n",
            "Loading 1208/2134\n",
            "Loading 1209/2134\n",
            "Loading 1210/2134\n",
            "Loading 1211/2134\n",
            "Loading 1212/2134\n",
            "Loading 1213/2134\n",
            "Loading 1214/2134\n",
            "Loading 1215/2134\n",
            "Loading 1216/2134\n",
            "Loading 1217/2134\n",
            "Loading 1218/2134\n",
            "Loading 1219/2134\n",
            "Loading 1220/2134\n",
            "Loading 1221/2134\n",
            "Loading 1222/2134\n",
            "Loading 1223/2134\n",
            "Loading 1224/2134\n",
            "Loading 1225/2134\n",
            "Loading 1226/2134\n",
            "Loading 1227/2134\n",
            "Loading 1228/2134\n",
            "Loading 1229/2134\n",
            "Loading 1230/2134\n",
            "Loading 1231/2134\n",
            "Loading 1232/2134\n",
            "Loading 1233/2134\n",
            "Loading 1234/2134\n",
            "Loading 1235/2134\n",
            "Loading 1236/2134\n",
            "Loading 1237/2134\n",
            "Loading 1238/2134\n",
            "Loading 1239/2134\n",
            "Loading 1240/2134\n",
            "Loading 1241/2134\n",
            "Loading 1242/2134\n",
            "Loading 1243/2134\n",
            "Loading 1244/2134\n",
            "Loading 1245/2134\n",
            "Loading 1246/2134\n",
            "Loading 1247/2134\n",
            "Loading 1248/2134\n",
            "Loading 1249/2134\n",
            "Loading 1250/2134\n",
            "Loading 1251/2134\n",
            "Loading 1252/2134\n",
            "Loading 1253/2134\n",
            "Loading 1254/2134\n",
            "Loading 1255/2134\n",
            "Loading 1256/2134\n",
            "Loading 1257/2134\n",
            "Loading 1258/2134\n",
            "Loading 1259/2134\n",
            "Loading 1260/2134\n",
            "Loading 1261/2134\n",
            "Loading 1262/2134\n",
            "Loading 1263/2134\n",
            "Loading 1264/2134\n",
            "Loading 1265/2134\n",
            "Loading 1266/2134\n",
            "Loading 1267/2134\n",
            "Loading 1268/2134\n",
            "Loading 1269/2134\n",
            "Loading 1270/2134\n",
            "Loading 1271/2134\n",
            "Loading 1272/2134\n",
            "Loading 1273/2134\n",
            "Loading 1274/2134\n",
            "Loading 1275/2134\n",
            "Loading 1276/2134\n",
            "Loading 1277/2134\n",
            "Loading 1278/2134\n",
            "Loading 1279/2134\n",
            "Loading 1280/2134\n",
            "Loading 1281/2134\n",
            "Loading 1282/2134\n",
            "Loading 1283/2134\n",
            "Loading 1284/2134\n",
            "Loading 1285/2134\n",
            "Loading 1286/2134\n",
            "Loading 1287/2134\n",
            "Loading 1288/2134\n",
            "Loading 1289/2134\n",
            "Loading 1290/2134\n",
            "Loading 1291/2134\n",
            "Loading 1292/2134\n",
            "Loading 1293/2134\n",
            "Loading 1294/2134\n",
            "Loading 1295/2134\n",
            "Loading 1296/2134\n",
            "Loading 1297/2134\n",
            "Loading 1298/2134\n",
            "Loading 1299/2134\n",
            "Loading 1300/2134\n",
            "Loading 1301/2134\n",
            "Loading 1302/2134\n",
            "Loading 1303/2134\n",
            "Loading 1304/2134\n",
            "Loading 1305/2134\n",
            "Loading 1306/2134\n",
            "Loading 1307/2134\n",
            "Loading 1308/2134\n",
            "Loading 1309/2134\n",
            "Loading 1310/2134\n",
            "Loading 1311/2134\n",
            "Loading 1312/2134\n",
            "Loading 1313/2134\n",
            "Loading 1314/2134\n",
            "Loading 1315/2134\n",
            "Loading 1316/2134\n",
            "Loading 1317/2134\n",
            "Loading 1318/2134\n",
            "Loading 1319/2134\n",
            "Loading 1320/2134\n",
            "Loading 1321/2134\n",
            "Loading 1322/2134\n",
            "Loading 1323/2134\n",
            "Loading 1324/2134\n",
            "Loading 1325/2134\n",
            "Loading 1326/2134\n",
            "Loading 1327/2134\n",
            "Loading 1328/2134\n",
            "Loading 1329/2134\n",
            "Loading 1330/2134\n",
            "Loading 1331/2134\n",
            "Loading 1332/2134\n",
            "Loading 1333/2134\n",
            "Loading 1334/2134\n",
            "Loading 1335/2134\n",
            "Loading 1336/2134\n",
            "Loading 1337/2134\n",
            "Loading 1338/2134\n",
            "Loading 1339/2134\n",
            "Loading 1340/2134\n",
            "Loading 1341/2134\n",
            "Loading 1342/2134\n",
            "Loading 1343/2134\n",
            "Loading 1344/2134\n",
            "Loading 1345/2134\n",
            "Loading 1346/2134\n",
            "Loading 1347/2134\n",
            "Loading 1348/2134\n",
            "Loading 1349/2134\n",
            "Loading 1350/2134\n",
            "Loading 1351/2134\n",
            "Loading 1352/2134\n",
            "Loading 1353/2134\n",
            "Loading 1354/2134\n",
            "Loading 1355/2134\n",
            "Loading 1356/2134\n",
            "Loading 1357/2134\n",
            "Loading 1358/2134\n",
            "Loading 1359/2134\n",
            "Loading 1360/2134\n",
            "Loading 1361/2134\n",
            "Loading 1362/2134\n",
            "Loading 1363/2134\n",
            "Loading 1364/2134\n",
            "Loading 1365/2134\n",
            "Loading 1366/2134\n",
            "Loading 1367/2134\n",
            "Loading 1368/2134\n",
            "Loading 1369/2134\n",
            "Loading 1370/2134\n",
            "Loading 1371/2134\n",
            "Loading 1372/2134\n",
            "Loading 1373/2134\n",
            "Loading 1374/2134\n",
            "Loading 1375/2134\n",
            "Loading 1376/2134\n",
            "Loading 1377/2134\n",
            "Loading 1378/2134\n",
            "Loading 1379/2134\n",
            "Loading 1380/2134\n",
            "Loading 1381/2134\n",
            "Loading 1382/2134\n",
            "Loading 1383/2134\n",
            "Loading 1384/2134\n",
            "Loading 1385/2134\n",
            "Loading 1386/2134\n",
            "Loading 1387/2134\n",
            "Loading 1388/2134\n",
            "Loading 1389/2134\n",
            "Loading 1390/2134\n",
            "Loading 1391/2134\n",
            "Loading 1392/2134\n",
            "Loading 1393/2134\n",
            "Loading 1394/2134\n",
            "Loading 1395/2134\n",
            "Loading 1396/2134\n",
            "Loading 1397/2134\n",
            "Loading 1398/2134\n",
            "Loading 1399/2134\n",
            "Loading 1400/2134\n",
            "Loading 1401/2134\n",
            "Loading 1402/2134\n",
            "Loading 1403/2134\n",
            "Loading 1404/2134\n",
            "Loading 1405/2134\n",
            "Loading 1406/2134\n",
            "Loading 1407/2134\n",
            "Loading 1408/2134\n",
            "Loading 1409/2134\n",
            "Loading 1410/2134\n",
            "Loading 1411/2134\n",
            "Loading 1412/2134\n",
            "Loading 1413/2134\n",
            "Loading 1414/2134\n",
            "Loading 1415/2134\n",
            "Loading 1416/2134\n",
            "Loading 1417/2134\n",
            "Loading 1418/2134\n",
            "Loading 1419/2134\n",
            "Loading 1420/2134\n",
            "Loading 1421/2134\n",
            "Loading 1422/2134\n",
            "Loading 1423/2134\n",
            "Loading 1424/2134\n",
            "Loading 1425/2134\n",
            "Loading 1426/2134\n",
            "Loading 1427/2134\n",
            "Loading 1428/2134\n",
            "Loading 1429/2134\n",
            "Loading 1430/2134\n",
            "Loading 1431/2134\n",
            "Loading 1432/2134\n",
            "Loading 1433/2134\n",
            "Loading 1434/2134\n",
            "Loading 1435/2134\n",
            "Loading 1436/2134\n",
            "Loading 1437/2134\n",
            "Loading 1438/2134\n",
            "Loading 1439/2134\n",
            "Loading 1440/2134\n",
            "Loading 1441/2134\n",
            "Loading 1442/2134\n",
            "Loading 1443/2134\n",
            "Loading 1444/2134\n",
            "Loading 1445/2134\n",
            "Loading 1446/2134\n",
            "Loading 1447/2134\n",
            "Loading 1448/2134\n",
            "Loading 1449/2134\n",
            "Loading 1450/2134\n",
            "Loading 1451/2134\n",
            "Loading 1452/2134\n",
            "Loading 1453/2134\n",
            "Loading 1454/2134\n",
            "Loading 1455/2134\n",
            "Loading 1456/2134\n",
            "Loading 1457/2134\n",
            "Loading 1458/2134\n",
            "Loading 1459/2134\n",
            "Loading 1460/2134\n",
            "Loading 1461/2134\n",
            "Loading 1462/2134\n",
            "Loading 1463/2134\n",
            "Loading 1464/2134\n",
            "Loading 1465/2134\n",
            "Loading 1466/2134\n",
            "Loading 1467/2134\n",
            "Loading 1468/2134\n",
            "Loading 1469/2134\n",
            "Loading 1470/2134\n",
            "Loading 1471/2134\n",
            "Loading 1472/2134\n",
            "Loading 1473/2134\n",
            "Loading 1474/2134\n",
            "Loading 1475/2134\n",
            "Loading 1476/2134\n",
            "Loading 1477/2134\n",
            "Loading 1478/2134\n",
            "Loading 1479/2134\n",
            "Loading 1480/2134\n",
            "Loading 1481/2134\n",
            "Loading 1482/2134\n",
            "Loading 1483/2134\n",
            "Loading 1484/2134\n",
            "Loading 1485/2134\n",
            "Loading 1486/2134\n",
            "Loading 1487/2134\n",
            "Loading 1488/2134\n",
            "Loading 1489/2134\n",
            "Loading 1490/2134\n",
            "Loading 1491/2134\n",
            "Loading 1492/2134\n",
            "Loading 1493/2134\n",
            "Loading 1494/2134\n",
            "Loading 1495/2134\n",
            "Loading 1496/2134\n",
            "Loading 1497/2134\n",
            "Loading 1498/2134\n",
            "Loading 1499/2134\n",
            "Loading 1500/2134\n",
            "Loading 1501/2134\n",
            "Loading 1502/2134\n",
            "Loading 1503/2134\n",
            "Loading 1504/2134\n",
            "Loading 1505/2134\n",
            "Loading 1506/2134\n",
            "Loading 1507/2134\n",
            "Loading 1508/2134\n",
            "Loading 1509/2134\n",
            "Loading 1510/2134\n",
            "Loading 1511/2134\n",
            "Loading 1512/2134\n",
            "Loading 1513/2134\n",
            "Loading 1514/2134\n",
            "Loading 1515/2134\n",
            "Loading 1516/2134\n",
            "Loading 1517/2134\n",
            "Loading 1518/2134\n",
            "Loading 1519/2134\n",
            "Loading 1520/2134\n",
            "Loading 1521/2134\n",
            "Loading 1522/2134\n",
            "Loading 1523/2134\n",
            "Loading 1524/2134\n",
            "Loading 1525/2134\n",
            "Loading 1526/2134\n",
            "Loading 1527/2134\n",
            "Loading 1528/2134\n",
            "Loading 1529/2134\n",
            "Loading 1530/2134\n",
            "Loading 1531/2134\n",
            "Loading 1532/2134\n",
            "Loading 1533/2134\n",
            "Loading 1534/2134\n",
            "Loading 1535/2134\n",
            "Loading 1536/2134\n",
            "Loading 1537/2134\n",
            "Loading 1538/2134\n",
            "Loading 1539/2134\n",
            "Loading 1540/2134\n",
            "Loading 1541/2134\n",
            "Loading 1542/2134\n",
            "Loading 1543/2134\n",
            "Loading 1544/2134\n",
            "Loading 1545/2134\n",
            "Loading 1546/2134\n",
            "Loading 1547/2134\n",
            "Loading 1548/2134\n",
            "Loading 1549/2134\n",
            "Loading 1550/2134\n",
            "Loading 1551/2134\n",
            "Loading 1552/2134\n",
            "Loading 1553/2134\n",
            "Loading 1554/2134\n",
            "Loading 1555/2134\n",
            "Loading 1556/2134\n",
            "Loading 1557/2134\n",
            "Loading 1558/2134\n",
            "Loading 1559/2134\n",
            "Loading 1560/2134\n",
            "Loading 1561/2134\n",
            "Loading 1562/2134\n",
            "Loading 1563/2134\n",
            "Loading 1564/2134\n",
            "Loading 1565/2134\n",
            "Loading 1566/2134\n",
            "Loading 1567/2134\n",
            "Loading 1568/2134\n",
            "Loading 1569/2134\n",
            "Loading 1570/2134\n",
            "Loading 1571/2134\n",
            "Loading 1572/2134\n",
            "Loading 1573/2134\n",
            "Loading 1574/2134\n",
            "Loading 1575/2134\n",
            "Loading 1576/2134\n",
            "Loading 1577/2134\n",
            "Loading 1578/2134\n",
            "Loading 1579/2134\n",
            "Loading 1580/2134\n",
            "Loading 1581/2134\n",
            "Loading 1582/2134\n",
            "Loading 1583/2134\n",
            "Loading 1584/2134\n",
            "Loading 1585/2134\n",
            "Loading 1586/2134\n",
            "Loading 1587/2134\n",
            "Loading 1588/2134\n",
            "Loading 1589/2134\n",
            "Loading 1590/2134\n",
            "Loading 1591/2134\n",
            "Loading 1592/2134\n",
            "Loading 1593/2134\n",
            "Loading 1594/2134\n",
            "Loading 1595/2134\n",
            "Loading 1596/2134\n",
            "Loading 1597/2134\n",
            "Loading 1598/2134\n",
            "Loading 1599/2134\n",
            "Loading 1600/2134\n",
            "Loading 1601/2134\n",
            "Loading 1602/2134\n",
            "Loading 1603/2134\n",
            "Loading 1604/2134\n",
            "Loading 1605/2134\n",
            "Loading 1606/2134\n",
            "Loading 1607/2134\n",
            "Loading 1608/2134\n",
            "Loading 1609/2134\n",
            "Loading 1610/2134\n",
            "Loading 1611/2134\n",
            "Loading 1612/2134\n",
            "Loading 1613/2134\n",
            "Loading 1614/2134\n",
            "Loading 1615/2134\n",
            "Loading 1616/2134\n",
            "Loading 1617/2134\n",
            "Loading 1618/2134\n",
            "Loading 1619/2134\n",
            "Loading 1620/2134\n",
            "Loading 1621/2134\n",
            "Loading 1622/2134\n",
            "Loading 1623/2134\n",
            "Loading 1624/2134\n",
            "Loading 1625/2134\n",
            "Loading 1626/2134\n",
            "Loading 1627/2134\n",
            "Loading 1628/2134\n",
            "Loading 1629/2134\n",
            "Loading 1630/2134\n",
            "Loading 1631/2134\n",
            "Loading 1632/2134\n",
            "Loading 1633/2134\n",
            "Loading 1634/2134\n",
            "Loading 1635/2134\n",
            "Loading 1636/2134\n",
            "Loading 1637/2134\n",
            "Loading 1638/2134\n",
            "Loading 1639/2134\n",
            "Loading 1640/2134\n",
            "Loading 1641/2134\n",
            "Loading 1642/2134\n",
            "Loading 1643/2134\n",
            "Loading 1644/2134\n",
            "Loading 1645/2134\n",
            "Loading 1646/2134\n",
            "Loading 1647/2134\n",
            "Loading 1648/2134\n",
            "Loading 1649/2134\n",
            "Loading 1650/2134\n",
            "Loading 1651/2134\n",
            "Loading 1652/2134\n",
            "Loading 1653/2134\n",
            "Loading 1654/2134\n",
            "Loading 1655/2134\n",
            "Loading 1656/2134\n",
            "Loading 1657/2134\n",
            "Loading 1658/2134\n",
            "Loading 1659/2134\n",
            "Loading 1660/2134\n",
            "Loading 1661/2134\n",
            "Loading 1662/2134\n",
            "Loading 1663/2134\n",
            "Loading 1664/2134\n",
            "Loading 1665/2134\n",
            "Loading 1666/2134\n",
            "Loading 1667/2134\n",
            "Loading 1668/2134\n",
            "Loading 1669/2134\n",
            "Loading 1670/2134\n",
            "Loading 1671/2134\n",
            "Loading 1672/2134\n",
            "Loading 1673/2134\n",
            "Loading 1674/2134\n",
            "Loading 1675/2134\n",
            "Loading 1676/2134\n",
            "Loading 1677/2134\n",
            "Loading 1678/2134\n",
            "Loading 1679/2134\n",
            "Loading 1680/2134\n",
            "Loading 1681/2134\n",
            "Loading 1682/2134\n",
            "Loading 1683/2134\n",
            "Loading 1684/2134\n",
            "Loading 1685/2134\n",
            "Loading 1686/2134\n",
            "Loading 1687/2134\n",
            "Loading 1688/2134\n",
            "Loading 1689/2134\n",
            "Loading 1690/2134\n",
            "Loading 1691/2134\n",
            "Loading 1692/2134\n",
            "Loading 1693/2134\n",
            "Loading 1694/2134\n",
            "Loading 1695/2134\n",
            "Loading 1696/2134\n",
            "Loading 1697/2134\n",
            "Loading 1698/2134\n",
            "Loading 1699/2134\n",
            "Loading 1700/2134\n",
            "Loading 1701/2134\n",
            "Loading 1702/2134\n",
            "Loading 1703/2134\n",
            "Loading 1704/2134\n",
            "Loading 1705/2134\n",
            "Loading 1706/2134\n",
            "Loading 1707/2134\n",
            "Loading 1708/2134\n",
            "Loading 1709/2134\n",
            "Loading 1710/2134\n",
            "Loading 1711/2134\n",
            "Loading 1712/2134\n",
            "Loading 1713/2134\n",
            "Loading 1714/2134\n",
            "Loading 1715/2134\n",
            "Loading 1716/2134\n",
            "Loading 1717/2134\n",
            "Loading 1718/2134\n",
            "Loading 1719/2134\n",
            "Loading 1720/2134\n",
            "Loading 1721/2134\n",
            "Loading 1722/2134\n",
            "Loading 1723/2134\n",
            "Loading 1724/2134\n",
            "Loading 1725/2134\n",
            "Loading 1726/2134\n",
            "Loading 1727/2134\n",
            "Loading 1728/2134\n",
            "Loading 1729/2134\n",
            "Loading 1730/2134\n",
            "Loading 1731/2134\n",
            "Loading 1732/2134\n",
            "Loading 1733/2134\n",
            "Loading 1734/2134\n",
            "Loading 1735/2134\n",
            "Loading 1736/2134\n",
            "Loading 1737/2134\n",
            "Loading 1738/2134\n",
            "Loading 1739/2134\n",
            "Loading 1740/2134\n",
            "Loading 1741/2134\n",
            "Loading 1742/2134\n",
            "Loading 1743/2134\n",
            "Loading 1744/2134\n",
            "Loading 1745/2134\n",
            "Loading 1746/2134\n",
            "Loading 1747/2134\n",
            "Loading 1748/2134\n",
            "Loading 1749/2134\n",
            "Loading 1750/2134\n",
            "Loading 1751/2134\n",
            "Loading 1752/2134\n",
            "Loading 1753/2134\n",
            "Loading 1754/2134\n",
            "Loading 1755/2134\n",
            "Loading 1756/2134\n",
            "Loading 1757/2134\n",
            "Loading 1758/2134\n",
            "Loading 1759/2134\n",
            "Loading 1760/2134\n",
            "Loading 1761/2134\n",
            "Loading 1762/2134\n",
            "Loading 1763/2134\n",
            "Loading 1764/2134\n",
            "Loading 1765/2134\n",
            "Loading 1766/2134\n",
            "Loading 1767/2134\n",
            "Loading 1768/2134\n",
            "Loading 1769/2134\n",
            "Loading 1770/2134\n",
            "Loading 1771/2134\n",
            "Loading 1772/2134\n",
            "Loading 1773/2134\n",
            "Loading 1774/2134\n",
            "Loading 1775/2134\n",
            "Loading 1776/2134\n",
            "Loading 1777/2134\n",
            "Loading 1778/2134\n",
            "Loading 1779/2134\n",
            "Loading 1780/2134\n",
            "Loading 1781/2134\n",
            "Loading 1782/2134\n",
            "Loading 1783/2134\n",
            "Loading 1784/2134\n",
            "Loading 1785/2134\n",
            "Loading 1786/2134\n",
            "Loading 1787/2134\n",
            "Loading 1788/2134\n",
            "Loading 1789/2134\n",
            "Loading 1790/2134\n",
            "Loading 1791/2134\n",
            "Loading 1792/2134\n",
            "Loading 1793/2134\n",
            "Loading 1794/2134\n",
            "Loading 1795/2134\n",
            "Loading 1796/2134\n",
            "Loading 1797/2134\n",
            "Loading 1798/2134\n",
            "Loading 1799/2134\n",
            "Loading 1800/2134\n",
            "Loading 1801/2134\n",
            "Loading 1802/2134\n",
            "Loading 1803/2134\n",
            "Loading 1804/2134\n",
            "Loading 1805/2134\n",
            "Loading 1806/2134\n",
            "Loading 1807/2134\n",
            "Loading 1808/2134\n",
            "Loading 1809/2134\n",
            "Loading 1810/2134\n",
            "Loading 1811/2134\n",
            "Loading 1812/2134\n",
            "Loading 1813/2134\n",
            "Loading 1814/2134\n",
            "Loading 1815/2134\n",
            "Loading 1816/2134\n",
            "Loading 1817/2134\n",
            "Loading 1818/2134\n",
            "Loading 1819/2134\n",
            "Loading 1820/2134\n",
            "Loading 1821/2134\n",
            "Loading 1822/2134\n",
            "Loading 1823/2134\n",
            "Loading 1824/2134\n",
            "Loading 1825/2134\n",
            "Loading 1826/2134\n",
            "Loading 1827/2134\n",
            "Loading 1828/2134\n",
            "Loading 1829/2134\n",
            "Loading 1830/2134\n",
            "Loading 1831/2134\n",
            "Loading 1832/2134\n",
            "Loading 1833/2134\n",
            "Loading 1834/2134\n",
            "Loading 1835/2134\n",
            "Loading 1836/2134\n",
            "Loading 1837/2134\n",
            "Loading 1838/2134\n",
            "Loading 1839/2134\n",
            "Loading 1840/2134\n",
            "Loading 1841/2134\n",
            "Loading 1842/2134\n",
            "Loading 1843/2134\n",
            "Loading 1844/2134\n",
            "Loading 1845/2134\n",
            "Loading 1846/2134\n",
            "Loading 1847/2134\n",
            "Loading 1848/2134\n",
            "Loading 1849/2134\n",
            "Loading 1850/2134\n",
            "Loading 1851/2134\n",
            "Loading 1852/2134\n",
            "Loading 1853/2134\n",
            "Loading 1854/2134\n",
            "Loading 1855/2134\n",
            "Loading 1856/2134\n",
            "Loading 1857/2134\n",
            "Loading 1858/2134\n",
            "Loading 1859/2134\n",
            "Loading 1860/2134\n",
            "Loading 1861/2134\n",
            "Loading 1862/2134\n",
            "Loading 1863/2134\n",
            "Loading 1864/2134\n",
            "Loading 1865/2134\n",
            "Loading 1866/2134\n",
            "Loading 1867/2134\n",
            "Loading 1868/2134\n",
            "Loading 1869/2134\n",
            "Loading 1870/2134\n",
            "Loading 1871/2134\n",
            "Loading 1872/2134\n",
            "Loading 1873/2134\n",
            "Loading 1874/2134\n",
            "Loading 1875/2134\n",
            "Loading 1876/2134\n",
            "Loading 1877/2134\n",
            "Loading 1878/2134\n",
            "Loading 1879/2134\n",
            "Loading 1880/2134\n",
            "Loading 1881/2134\n",
            "Loading 1882/2134\n",
            "Loading 1883/2134\n",
            "Loading 1884/2134\n",
            "Loading 1885/2134\n",
            "Loading 1886/2134\n",
            "Loading 1887/2134\n",
            "Loading 1888/2134\n",
            "Loading 1889/2134\n",
            "Loading 1890/2134\n",
            "Loading 1891/2134\n",
            "Loading 1892/2134\n",
            "Loading 1893/2134\n",
            "Loading 1894/2134\n",
            "Loading 1895/2134\n",
            "Loading 1896/2134\n",
            "Loading 1897/2134\n",
            "Loading 1898/2134\n",
            "Loading 1899/2134\n",
            "Loading 1900/2134\n",
            "Loading 1901/2134\n",
            "Loading 1902/2134\n",
            "Loading 1903/2134\n",
            "Loading 1904/2134\n",
            "Loading 1905/2134\n",
            "Loading 1906/2134\n",
            "Loading 1907/2134\n",
            "Loading 1908/2134\n",
            "Loading 1909/2134\n",
            "Loading 1910/2134\n",
            "Loading 1911/2134\n",
            "Loading 1912/2134\n",
            "Loading 1913/2134\n",
            "Loading 1914/2134\n",
            "Loading 1915/2134\n",
            "Loading 1916/2134\n",
            "Loading 1917/2134\n",
            "Loading 1918/2134\n",
            "Loading 1919/2134\n",
            "Loading 1920/2134\n",
            "Loading 1921/2134\n",
            "Loading 1922/2134\n",
            "Loading 1923/2134\n",
            "Loading 1924/2134\n",
            "Loading 1925/2134\n",
            "Loading 1926/2134\n",
            "Loading 1927/2134\n",
            "Loading 1928/2134\n",
            "Loading 1929/2134\n",
            "Loading 1930/2134\n",
            "Loading 1931/2134\n",
            "Loading 1932/2134\n",
            "Loading 1933/2134\n",
            "Loading 1934/2134\n",
            "Loading 1935/2134\n",
            "Loading 1936/2134\n",
            "Loading 1937/2134\n",
            "Loading 1938/2134\n",
            "Loading 1939/2134\n",
            "Loading 1940/2134\n",
            "Loading 1941/2134\n",
            "Loading 1942/2134\n",
            "Loading 1943/2134\n",
            "Loading 1944/2134\n",
            "Loading 1945/2134\n",
            "Loading 1946/2134\n",
            "Loading 1947/2134\n",
            "Loading 1948/2134\n",
            "Loading 1949/2134\n",
            "Loading 1950/2134\n",
            "Loading 1951/2134\n",
            "Loading 1952/2134\n",
            "Loading 1953/2134\n",
            "Loading 1954/2134\n",
            "Loading 1955/2134\n",
            "Loading 1956/2134\n",
            "Loading 1957/2134\n",
            "Loading 1958/2134\n",
            "Loading 1959/2134\n",
            "Loading 1960/2134\n",
            "Loading 1961/2134\n",
            "Loading 1962/2134\n",
            "Loading 1963/2134\n",
            "Loading 1964/2134\n",
            "Loading 1965/2134\n",
            "Loading 1966/2134\n",
            "Loading 1967/2134\n",
            "Loading 1968/2134\n",
            "Loading 1969/2134\n",
            "Loading 1970/2134\n",
            "Loading 1971/2134\n",
            "Loading 1972/2134\n",
            "Loading 1973/2134\n",
            "Loading 1974/2134\n",
            "Loading 1975/2134\n",
            "Loading 1976/2134\n",
            "Loading 1977/2134\n",
            "Loading 1978/2134\n",
            "Loading 1979/2134\n",
            "Loading 1980/2134\n",
            "Loading 1981/2134\n",
            "Loading 1982/2134\n",
            "Loading 1983/2134\n",
            "Loading 1984/2134\n",
            "Loading 1985/2134\n",
            "Loading 1986/2134\n",
            "Loading 1987/2134\n",
            "Loading 1988/2134\n",
            "Loading 1989/2134\n",
            "Loading 1990/2134\n",
            "Loading 1991/2134\n",
            "Loading 1992/2134\n",
            "Loading 1993/2134\n",
            "Loading 1994/2134\n",
            "Loading 1995/2134\n",
            "Loading 1996/2134\n",
            "Loading 1997/2134\n",
            "Loading 1998/2134\n",
            "Loading 1999/2134\n",
            "Loading 2000/2134\n",
            "Loading 2001/2134\n",
            "Loading 2002/2134\n",
            "Loading 2003/2134\n",
            "Loading 2004/2134\n",
            "Loading 2005/2134\n",
            "Loading 2006/2134\n",
            "Loading 2007/2134\n",
            "Loading 2008/2134\n",
            "Loading 2009/2134\n",
            "Loading 2010/2134\n",
            "Loading 2011/2134\n",
            "Loading 2012/2134\n",
            "Loading 2013/2134\n",
            "Loading 2014/2134\n",
            "Loading 2015/2134\n",
            "Loading 2016/2134\n",
            "Loading 2017/2134\n",
            "Loading 2018/2134\n",
            "Loading 2019/2134\n",
            "Loading 2020/2134\n",
            "Loading 2021/2134\n",
            "Loading 2022/2134\n",
            "Loading 2023/2134\n",
            "Loading 2024/2134\n",
            "Loading 2025/2134\n",
            "Loading 2026/2134\n",
            "Loading 2027/2134\n",
            "Loading 2028/2134\n",
            "Loading 2029/2134\n",
            "Loading 2030/2134\n",
            "Loading 2031/2134\n",
            "Loading 2032/2134\n",
            "Loading 2033/2134\n",
            "Loading 2034/2134\n",
            "Loading 2035/2134\n",
            "Loading 2036/2134\n",
            "Loading 2037/2134\n",
            "Loading 2038/2134\n",
            "Loading 2039/2134\n",
            "Loading 2040/2134\n",
            "Loading 2041/2134\n",
            "Loading 2042/2134\n",
            "Loading 2043/2134\n",
            "Loading 2044/2134\n",
            "Loading 2045/2134\n",
            "Loading 2046/2134\n",
            "Loading 2047/2134\n",
            "Loading 2048/2134\n",
            "Loading 2049/2134\n",
            "Loading 2050/2134\n",
            "Loading 2051/2134\n",
            "Loading 2052/2134\n",
            "Loading 2053/2134\n",
            "Loading 2054/2134\n",
            "Loading 2055/2134\n",
            "Loading 2056/2134\n",
            "Loading 2057/2134\n",
            "Loading 2058/2134\n",
            "Loading 2059/2134\n",
            "Loading 2060/2134\n",
            "Loading 2061/2134\n",
            "Loading 2062/2134\n",
            "Loading 2063/2134\n",
            "Loading 2064/2134\n",
            "Loading 2065/2134\n",
            "Loading 2066/2134\n",
            "Loading 2067/2134\n",
            "Loading 2068/2134\n",
            "Loading 2069/2134\n",
            "Loading 2070/2134\n",
            "Loading 2071/2134\n",
            "Loading 2072/2134\n",
            "Loading 2073/2134\n",
            "Loading 2074/2134\n",
            "Loading 2075/2134\n",
            "Loading 2076/2134\n",
            "Loading 2077/2134\n",
            "Loading 2078/2134\n",
            "Loading 2079/2134\n",
            "Loading 2080/2134\n",
            "Loading 2081/2134\n",
            "Loading 2082/2134\n",
            "Loading 2083/2134\n",
            "Loading 2084/2134\n",
            "Loading 2085/2134\n",
            "Loading 2086/2134\n",
            "Loading 2087/2134\n",
            "Loading 2088/2134\n",
            "Loading 2089/2134\n",
            "Loading 2090/2134\n",
            "Loading 2091/2134\n",
            "Loading 2092/2134\n",
            "Loading 2093/2134\n",
            "Loading 2094/2134\n",
            "Loading 2095/2134\n",
            "Loading 2096/2134\n",
            "Loading 2097/2134\n",
            "Loading 2098/2134\n",
            "Loading 2099/2134\n",
            "Loading 2100/2134\n",
            "Loading 2101/2134\n",
            "Loading 2102/2134\n",
            "Loading 2103/2134\n",
            "Loading 2104/2134\n",
            "Loading 2105/2134\n",
            "Loading 2106/2134\n",
            "Loading 2107/2134\n",
            "Loading 2108/2134\n",
            "Loading 2109/2134\n",
            "Loading 2110/2134\n",
            "Loading 2111/2134\n",
            "Loading 2112/2134\n",
            "Loading 2113/2134\n",
            "Loading 2114/2134\n",
            "Loading 2115/2134\n",
            "Loading 2116/2134\n",
            "Loading 2117/2134\n",
            "Loading 2118/2134\n",
            "Loading 2119/2134\n",
            "Loading 2120/2134\n",
            "Loading 2121/2134\n",
            "Loading 2122/2134\n",
            "Loading 2123/2134\n",
            "Loading 2124/2134\n",
            "Loading 2125/2134\n",
            "Loading 2126/2134\n",
            "Loading 2127/2134\n",
            "Loading 2128/2134\n",
            "Loading 2129/2134\n",
            "Loading 2130/2134\n",
            "Loading 2131/2134\n",
            "Loading 2132/2134\n",
            "Loading 2133/2134\n",
            "Loading 2134/2134\n",
            "Test image index : [154, 47, 59, 31, 419, 446, 217, 63, 152, 236, 289, 229, 434, 67, 45, 316, 352, 135, 285, 77, 318, 41, 142]\n",
            "Ended divided training and test dataset. Training : 38574, Test : 2124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMTIgLyC38To"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amc_PFY438To",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05d0a536-32e2-4696-8245-75c4a10201e5"
      },
      "source": [
        "batch_size = 8\n",
        "max_epoch = 300\n",
        "n_mels = 40\n",
        "time_steps = 303\n",
        "num_hg_Depth = 4\n",
        "dim_hg_feat = 256\n",
        "learning_rate = 0.00001\n",
        "dropout_rate = 0.0\n",
        "train_model_number = 1\n",
        "input_channel_type = 'single'\n",
        "run_date = '2021-01-06'\n",
        "encoder_args = {'num_stacks': 3, 'num_channels':[n_mels for i in range(6)], 'kernel_size':3,\n",
        "                'dropout_rate': dropout_rate, 'activation': 'leaky-relu', 'return_type': 'end'}\n",
        "input_shapes = (None, n_mels, time_steps)\n",
        "seed = 1\n",
        "training_state = True\n",
        "restore_flag = 1\n",
        "restore_epoch = 0\n",
        "# raise ValueError('Check restore_flag and restore_epoch again!! If confident # this code')\n",
        "\"\"\"\n",
        "1) restore_flag == 0\n",
        "- Restore encoder model weights only\n",
        "- restore_epoch == 0\n",
        "\n",
        "2) restore_flag == 1\n",
        "- Restore total model weights\n",
        "- restore_epoch != 0\n",
        "\"\"\"\n",
        "if restore_flag == 0 and restore_epoch != 0:\n",
        "    raise ValueError('Check restore_flag and restore_epoch')\n",
        "elif restore_flag == 1 and restore_epoch == 0:\n",
        "    raise ValueError('Check restore_flag and restore_epoch')\n",
        "\n",
        "wandb_config = {'batch_size': batch_size,\n",
        "                'max_epoch': max_epoch,\n",
        "                'n_mels': n_mels,\n",
        "                'num_hg_Depth': num_hg_Depth,\n",
        "                'dim_hg_feat': dim_hg_feat,\n",
        "                'learning_rate': learning_rate,\n",
        "                'dropout_rate': dropout_rate,\n",
        "                'train_model_number': train_model_number,\n",
        "                'input_channel_type': input_channel_type,\n",
        "                'seed': seed,\n",
        "                'training_state': training_state,\n",
        "                'num_train': num_train,\n",
        "                'num_test': num_test,\n",
        "                'date': run_date}\n",
        "\n",
        "if restore_flag == 2:\n",
        "    id = wandb.util.generate_id()\n",
        "    print('Wandb id: {}'.format(id))\n",
        "    with open('/content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/wandbID.txt', 'w') as f:\n",
        "        f.write(str(id))\n",
        "elif restore_flag == 1:\n",
        "    id = '9k5xv50p'\n",
        "else:\n",
        "    raise ValueError('Unsupported restore_flag')\n",
        "\n",
        "wandb_run = wandb.init(project='Speech2Pickup', name='HGN_senEM', id=id, resume=\"allow\", config=wandb_config)\n",
        "\n",
        "train(img_resize = 256, # must be fixed\n",
        "      heatmap_resize = 64, # must be fixed\n",
        "      num_hg_Depth = num_hg_Depth, # good be fixed\n",
        "      dim_hg_feat = dim_hg_feat, # good be fixed\n",
        "      n_mels = n_mels,\n",
        "      time_steps = time_steps,\n",
        "      encoder_model_path = '/content/drive/MyDrive/Speech2Pickup/sentenceEM_model/SOTA/encoder_model/sentenceEM_encoder_model',\n",
        "      encoder_args = encoder_args,\n",
        "      input_shapes = input_shapes,\n",
        "      seed = seed,\n",
        "      training_state = training_state,\n",
        "      batch_size = batch_size, \n",
        "      max_epoch = max_epoch,\n",
        "      num_train = num_train,\n",
        "      save_stride = 1,\n",
        "      learning_rate = learning_rate, # good be fixed\n",
        "      dropout_rate = dropout_rate,\n",
        "      restore_flag = restore_flag,\n",
        "      restore_path = '/content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt',\n",
        "      restore_epoch = restore_epoch,\n",
        "      total_images = total_images,\n",
        "      total_heatmaps = total_heatmaps,\n",
        "      train_speech_inputs = train_speech_inputs,\n",
        "      train_img_idx = train_img_idx,\n",
        "      train_pos_outputs = train_pos_outputs,\n",
        "      model_save_path = '/content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wandb id: 9k5xv50p\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">HGN_senEM</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/awesomericky/Speech2Pickup\" target=\"_blank\">https://wandb.ai/awesomericky/Speech2Pickup</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/awesomericky/Speech2Pickup/runs/9k5xv50p\" target=\"_blank\">https://wandb.ai/awesomericky/Speech2Pickup/runs/9k5xv50p</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210106_044911-9k5xv50p</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/code_HGN_senEM_train.py:16: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/TCN_and_decoder.py:10: The name tf.keras.initializers.RandomNormal is deprecated. Please use tf.compat.v1.keras.initializers.RandomNormal instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method TempConvnet.call of <TCN_and_decoder.TempConvnet object at 0x7f561e2845f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TempConvnet.call of <TCN_and_decoder.TempConvnet object at 0x7f561e2845f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561e284828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561e284828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e284a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e284a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e2481d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e2481d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c149da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c149da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c156978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c156978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c162550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c162550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c16d128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c16d128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c175c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c175c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c175e48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c175e48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c181ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c181ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c10e978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c10e978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c118748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c118748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c127518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c127518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1342e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1342e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c1420b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c1420b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1422e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1422e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d3080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d3080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d9e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d9e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0e7be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0e7be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0f69b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0f69b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c102780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c102780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:119: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:119: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:Entity <bound method TempConvnet.call of <TCN_and_decoder.TempConvnet object at 0x7f561e2845f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TempConvnet.call of <TCN_and_decoder.TempConvnet object at 0x7f561e2845f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561e284828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561e284828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e284a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e284a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e2481d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e2481d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c149da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c149da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c156978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c156978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c162550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c162550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c16d128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c16d128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c175c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c175c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c175e48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c175e48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c181ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c181ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c10e978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c10e978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c118748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c118748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c127518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c127518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1342e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1342e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c1420b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c1420b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1422e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1422e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d3080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d3080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d9e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d9e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0e7be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0e7be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0f69b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0f69b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c102780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c102780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TempConvnet.call of <TCN_and_decoder.TempConvnet object at 0x7f561e2845f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TempConvnet.call of <TCN_and_decoder.TempConvnet object at 0x7f561e2845f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561e284828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561e284828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e284a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e284a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e2481d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561e2481d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c149da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c149da0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c156978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c156978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c162550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c162550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c16d128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c16d128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c175c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c175c50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c175e48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c175e48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c181ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c181ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c10e978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c10e978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c118748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c118748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c127518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c127518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1342e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1342e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c1420b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <TCN_and_decoder.TemporalBlock object at 0x7f561c1420b8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1422e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c1422e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d3080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d3080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d9e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0d9e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0e7be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0e7be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0f69b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c0f69b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c102780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ResidualBlock.call of <TCN_and_decoder.ResidualBlock object at 0x7f561c102780>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:82: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:83: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:85: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:91: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:49: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:52: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:52: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/hourglass_with_senEM.py:58: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/code_HGN_senEM_train.py:34: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/code_HGN_senEM_train.py:36: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/code_HGN_senEM_train.py:38: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/code_HGN_senEM_train.py:40: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "Now ready to start the session.\n",
            "WARNING:tensorflow:From /content/code_HGN_senEM_train.py:46: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "batch loss : 4860.897 -> about 58543.640 second left to finish this epoch\n",
            "batch loss : 49.066986 -> about 1253.021 second left to finish this epoch\n",
            "batch loss : 21.278732 -> about 952.109 second left to finish this epoch\n",
            "batch loss : 13.189396 -> about 841.050 second left to finish this epoch\n",
            "batch loss : 7.243085 -> about 778.114 second left to finish this epoch\n",
            "batch loss : 5.2181835 -> about 734.534 second left to finish this epoch\n",
            "batch loss : 4.4617186 -> about 700.864 second left to finish this epoch\n",
            "batch loss : 2.9384637 -> about 673.152 second left to finish this epoch\n",
            "batch loss : 2.4088366 -> about 648.324 second left to finish this epoch\n",
            "batch loss : 1.7610958 -> about 625.572 second left to finish this epoch\n",
            "batch loss : 2.74433 -> about 604.387 second left to finish this epoch\n",
            "batch loss : 1.1640662 -> about 584.440 second left to finish this epoch\n",
            "batch loss : 1.3362609 -> about 565.397 second left to finish this epoch\n",
            "batch loss : 1.0207682 -> about 547.085 second left to finish this epoch\n",
            "batch loss : 0.87388086 -> about 529.157 second left to finish this epoch\n",
            "batch loss : 1.7369232 -> about 511.651 second left to finish this epoch\n",
            "batch loss : 0.8498225 -> about 494.457 second left to finish this epoch\n",
            "batch loss : 0.690567 -> about 477.764 second left to finish this epoch\n",
            "batch loss : 0.5663616 -> about 461.151 second left to finish this epoch\n",
            "batch loss : 0.55454576 -> about 444.679 second left to finish this epoch\n",
            "batch loss : 0.5660799 -> about 428.431 second left to finish this epoch\n",
            "batch loss : 0.4659935 -> about 412.360 second left to finish this epoch\n",
            "batch loss : 0.56601584 -> about 396.446 second left to finish this epoch\n",
            "batch loss : 0.40726018 -> about 380.667 second left to finish this epoch\n",
            "batch loss : 0.35653466 -> about 364.924 second left to finish this epoch\n",
            "batch loss : 0.30884063 -> about 349.313 second left to finish this epoch\n",
            "batch loss : 0.5861312 -> about 333.800 second left to finish this epoch\n",
            "batch loss : 0.3375563 -> about 318.319 second left to finish this epoch\n",
            "batch loss : 0.2515896 -> about 302.909 second left to finish this epoch\n",
            "batch loss : 0.31727517 -> about 287.583 second left to finish this epoch\n",
            "batch loss : 0.45245445 -> about 272.286 second left to finish this epoch\n",
            "batch loss : 0.20380776 -> about 257.032 second left to finish this epoch\n",
            "batch loss : 0.22687362 -> about 241.830 second left to finish this epoch\n",
            "batch loss : 0.23267293 -> about 226.733 second left to finish this epoch\n",
            "batch loss : 0.21177945 -> about 211.741 second left to finish this epoch\n",
            "batch loss : 0.22528198 -> about 196.736 second left to finish this epoch\n",
            "batch loss : 0.32917693 -> about 181.768 second left to finish this epoch\n",
            "batch loss : 0.19789663 -> about 166.808 second left to finish this epoch\n",
            "batch loss : 0.16532397 -> about 151.835 second left to finish this epoch\n",
            "batch loss : 0.17060225 -> about 136.895 second left to finish this epoch\n",
            "batch loss : 0.43405843 -> about 121.943 second left to finish this epoch\n",
            "batch loss : 0.13153589 -> about 107.014 second left to finish this epoch\n",
            "batch loss : 0.14317463 -> about 92.110 second left to finish this epoch\n",
            "batch loss : 0.13249831 -> about 77.228 second left to finish this epoch\n",
            "batch loss : 0.1327891 -> about 62.367 second left to finish this epoch\n",
            "batch loss : 0.16693339 -> about 47.524 second left to finish this epoch\n",
            "batch loss : 0.12937771 -> about 32.702 second left to finish this epoch\n",
            "batch loss : 0.111917004 -> about 17.894 second left to finish this epoch\n",
            "batch loss : 0.15784438 -> about 3.104 second left to finish this epoch\n",
            "current epoch : 1 , current train loss : 12.066684745360313\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.11943507 -> about 738.555 second left to finish this epoch\n",
            "batch loss : 0.11092168 -> about 682.060 second left to finish this epoch\n",
            "batch loss : 0.37118605 -> about 667.041 second left to finish this epoch\n",
            "batch loss : 0.09451555 -> about 652.247 second left to finish this epoch\n",
            "batch loss : 0.09925658 -> about 637.987 second left to finish this epoch\n",
            "batch loss : 0.08326997 -> about 623.271 second left to finish this epoch\n",
            "batch loss : 0.15160623 -> about 608.712 second left to finish this epoch\n",
            "batch loss : 0.07181408 -> about 594.280 second left to finish this epoch\n",
            "batch loss : 0.0766892 -> about 580.041 second left to finish this epoch\n",
            "batch loss : 0.119597584 -> about 565.652 second left to finish this epoch\n",
            "batch loss : 0.073627725 -> about 551.153 second left to finish this epoch\n",
            "batch loss : 0.07274702 -> about 536.690 second left to finish this epoch\n",
            "batch loss : 0.10232656 -> about 522.644 second left to finish this epoch\n",
            "batch loss : 0.07961926 -> about 508.307 second left to finish this epoch\n",
            "batch loss : 0.06399883 -> about 493.695 second left to finish this epoch\n",
            "batch loss : 0.070494756 -> about 479.148 second left to finish this epoch\n",
            "batch loss : 0.057646714 -> about 464.672 second left to finish this epoch\n",
            "batch loss : 0.06445245 -> about 450.083 second left to finish this epoch\n",
            "batch loss : 0.068910494 -> about 435.617 second left to finish this epoch\n",
            "batch loss : 0.058908794 -> about 421.163 second left to finish this epoch\n",
            "batch loss : 0.19886969 -> about 406.686 second left to finish this epoch\n",
            "batch loss : 0.18359941 -> about 392.188 second left to finish this epoch\n",
            "batch loss : 0.065629594 -> about 377.669 second left to finish this epoch\n",
            "batch loss : 0.048217915 -> about 363.245 second left to finish this epoch\n",
            "batch loss : 0.04268625 -> about 348.773 second left to finish this epoch\n",
            "batch loss : 0.04741863 -> about 334.333 second left to finish this epoch\n",
            "batch loss : 0.048978075 -> about 319.863 second left to finish this epoch\n",
            "batch loss : 0.0485235 -> about 305.454 second left to finish this epoch\n",
            "batch loss : 0.051699117 -> about 291.015 second left to finish this epoch\n",
            "batch loss : 0.102361515 -> about 276.623 second left to finish this epoch\n",
            "batch loss : 0.03586918 -> about 262.225 second left to finish this epoch\n",
            "batch loss : 0.04278545 -> about 247.784 second left to finish this epoch\n",
            "batch loss : 0.067037314 -> about 233.380 second left to finish this epoch\n",
            "batch loss : 0.037385456 -> about 219.004 second left to finish this epoch\n",
            "batch loss : 0.03941951 -> about 204.625 second left to finish this epoch\n",
            "batch loss : 0.03697048 -> about 190.208 second left to finish this epoch\n",
            "batch loss : 0.034152307 -> about 175.817 second left to finish this epoch\n",
            "batch loss : 0.040935885 -> about 161.406 second left to finish this epoch\n",
            "batch loss : 0.03485435 -> about 146.994 second left to finish this epoch\n",
            "batch loss : 0.033091225 -> about 132.593 second left to finish this epoch\n",
            "batch loss : 0.036851708 -> about 118.181 second left to finish this epoch\n",
            "batch loss : 0.03951889 -> about 103.783 second left to finish this epoch\n",
            "batch loss : 0.034665402 -> about 89.379 second left to finish this epoch\n",
            "batch loss : 0.034048203 -> about 74.985 second left to finish this epoch\n",
            "batch loss : 0.03235095 -> about 60.587 second left to finish this epoch\n",
            "batch loss : 0.029456 -> about 46.192 second left to finish this epoch\n",
            "batch loss : 0.032219276 -> about 31.801 second left to finish this epoch\n",
            "batch loss : 0.032977834 -> about 17.410 second left to finish this epoch\n",
            "batch loss : 0.043208085 -> about 3.022 second left to finish this epoch\n",
            "current epoch : 2 , current train loss : 0.06703828747452442\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.028242972 -> about 710.474 second left to finish this epoch\n",
            "batch loss : 0.030999733 -> about 677.358 second left to finish this epoch\n",
            "batch loss : 0.026950061 -> about 662.795 second left to finish this epoch\n",
            "batch loss : 0.02875392 -> about 649.152 second left to finish this epoch\n",
            "batch loss : 0.031090342 -> about 635.021 second left to finish this epoch\n",
            "batch loss : 0.027533945 -> about 620.467 second left to finish this epoch\n",
            "batch loss : 0.032821372 -> about 606.325 second left to finish this epoch\n",
            "batch loss : 0.043849334 -> about 592.377 second left to finish this epoch\n",
            "batch loss : 0.11071566 -> about 577.532 second left to finish this epoch\n",
            "batch loss : 0.026943317 -> about 563.145 second left to finish this epoch\n",
            "batch loss : 0.027120676 -> about 548.535 second left to finish this epoch\n",
            "batch loss : 0.026585061 -> about 534.165 second left to finish this epoch\n",
            "batch loss : 0.026072623 -> about 519.765 second left to finish this epoch\n",
            "batch loss : 0.03643865 -> about 505.385 second left to finish this epoch\n",
            "batch loss : 0.024030203 -> about 490.942 second left to finish this epoch\n",
            "batch loss : 0.02856215 -> about 476.591 second left to finish this epoch\n",
            "batch loss : 0.027644217 -> about 462.260 second left to finish this epoch\n",
            "batch loss : 0.06441688 -> about 447.839 second left to finish this epoch\n",
            "batch loss : 0.038510084 -> about 433.409 second left to finish this epoch\n",
            "batch loss : 0.02342821 -> about 419.063 second left to finish this epoch\n",
            "batch loss : 0.021782426 -> about 404.615 second left to finish this epoch\n",
            "batch loss : 0.026007675 -> about 390.289 second left to finish this epoch\n",
            "batch loss : 0.025308691 -> about 375.904 second left to finish this epoch\n",
            "batch loss : 0.03541819 -> about 361.522 second left to finish this epoch\n",
            "batch loss : 0.02358424 -> about 347.221 second left to finish this epoch\n",
            "batch loss : 0.023321759 -> about 332.841 second left to finish this epoch\n",
            "batch loss : 0.021613987 -> about 318.491 second left to finish this epoch\n",
            "batch loss : 0.02351926 -> about 304.155 second left to finish this epoch\n",
            "batch loss : 0.023697548 -> about 289.857 second left to finish this epoch\n",
            "batch loss : 0.02361628 -> about 275.557 second left to finish this epoch\n",
            "batch loss : 0.024194721 -> about 261.194 second left to finish this epoch\n",
            "batch loss : 0.030391566 -> about 246.837 second left to finish this epoch\n",
            "batch loss : 0.02240523 -> about 232.486 second left to finish this epoch\n",
            "batch loss : 0.02225555 -> about 218.130 second left to finish this epoch\n",
            "batch loss : 0.023105074 -> about 203.772 second left to finish this epoch\n",
            "batch loss : 0.021252848 -> about 189.421 second left to finish this epoch\n",
            "batch loss : 0.021880712 -> about 175.057 second left to finish this epoch\n",
            "batch loss : 0.020056494 -> about 160.727 second left to finish this epoch\n",
            "batch loss : 0.022620294 -> about 146.395 second left to finish this epoch\n",
            "batch loss : 0.02321294 -> about 132.052 second left to finish this epoch\n",
            "batch loss : 0.021694627 -> about 117.710 second left to finish this epoch\n",
            "batch loss : 0.021612622 -> about 103.373 second left to finish this epoch\n",
            "batch loss : 0.025305621 -> about 89.030 second left to finish this epoch\n",
            "batch loss : 0.02444702 -> about 74.688 second left to finish this epoch\n",
            "batch loss : 0.024592362 -> about 60.358 second left to finish this epoch\n",
            "batch loss : 0.022512987 -> about 46.020 second left to finish this epoch\n",
            "batch loss : 0.020671915 -> about 31.681 second left to finish this epoch\n",
            "batch loss : 0.01965255 -> about 17.345 second left to finish this epoch\n",
            "batch loss : 0.021967584 -> about 3.010 second left to finish this epoch\n",
            "current epoch : 3 , current train loss : 0.02572380860016025\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.018739857 -> about 698.168 second left to finish this epoch\n",
            "batch loss : 0.021722246 -> about 676.001 second left to finish this epoch\n",
            "batch loss : 0.019575976 -> about 664.853 second left to finish this epoch\n",
            "batch loss : 0.0197563 -> about 648.404 second left to finish this epoch\n",
            "batch loss : 0.019100126 -> about 632.999 second left to finish this epoch\n",
            "batch loss : 0.020209126 -> about 618.636 second left to finish this epoch\n",
            "batch loss : 0.025535379 -> about 604.487 second left to finish this epoch\n",
            "batch loss : 0.018514544 -> about 590.080 second left to finish this epoch\n",
            "batch loss : 0.018888067 -> about 575.415 second left to finish this epoch\n",
            "batch loss : 0.020854644 -> about 561.026 second left to finish this epoch\n",
            "batch loss : 0.022507975 -> about 547.340 second left to finish this epoch\n",
            "batch loss : 0.01858776 -> about 533.843 second left to finish this epoch\n",
            "batch loss : 0.01918647 -> about 519.839 second left to finish this epoch\n",
            "batch loss : 0.01921863 -> about 505.551 second left to finish this epoch\n",
            "batch loss : 0.021071829 -> about 491.103 second left to finish this epoch\n",
            "batch loss : 0.01710818 -> about 476.924 second left to finish this epoch\n",
            "batch loss : 0.01900487 -> about 462.453 second left to finish this epoch\n",
            "batch loss : 0.018403985 -> about 448.155 second left to finish this epoch\n",
            "batch loss : 0.020224504 -> about 433.810 second left to finish this epoch\n",
            "batch loss : 0.019424427 -> about 419.386 second left to finish this epoch\n",
            "batch loss : 0.01970423 -> about 404.965 second left to finish this epoch\n",
            "batch loss : 0.020387528 -> about 390.477 second left to finish this epoch\n",
            "batch loss : 0.02091211 -> about 376.032 second left to finish this epoch\n",
            "batch loss : 0.017362617 -> about 361.749 second left to finish this epoch\n",
            "batch loss : 0.02086116 -> about 347.438 second left to finish this epoch\n",
            "batch loss : 0.01826629 -> about 333.010 second left to finish this epoch\n",
            "batch loss : 0.01709465 -> about 318.593 second left to finish this epoch\n",
            "batch loss : 0.01705315 -> about 304.182 second left to finish this epoch\n",
            "batch loss : 0.020805039 -> about 289.811 second left to finish this epoch\n",
            "batch loss : 0.01901881 -> about 275.429 second left to finish this epoch\n",
            "batch loss : 0.017852176 -> about 261.054 second left to finish this epoch\n",
            "batch loss : 0.017575346 -> about 246.698 second left to finish this epoch\n",
            "batch loss : 0.018828172 -> about 232.361 second left to finish this epoch\n",
            "batch loss : 0.015754836 -> about 218.052 second left to finish this epoch\n",
            "batch loss : 0.019527234 -> about 203.709 second left to finish this epoch\n",
            "batch loss : 0.017253904 -> about 189.398 second left to finish this epoch\n",
            "batch loss : 0.017424624 -> about 175.049 second left to finish this epoch\n",
            "batch loss : 0.019364709 -> about 160.684 second left to finish this epoch\n",
            "batch loss : 0.018204737 -> about 146.339 second left to finish this epoch\n",
            "batch loss : 0.018189656 -> about 131.992 second left to finish this epoch\n",
            "batch loss : 0.015453546 -> about 117.669 second left to finish this epoch\n",
            "batch loss : 0.018129634 -> about 103.324 second left to finish this epoch\n",
            "batch loss : 0.01972735 -> about 88.986 second left to finish this epoch\n",
            "batch loss : 0.017317615 -> about 74.646 second left to finish this epoch\n",
            "batch loss : 0.01882112 -> about 60.313 second left to finish this epoch\n",
            "batch loss : 0.018557703 -> about 45.991 second left to finish this epoch\n",
            "batch loss : 0.01636708 -> about 31.663 second left to finish this epoch\n",
            "batch loss : 0.016320491 -> about 17.335 second left to finish this epoch\n",
            "batch loss : 0.018700259 -> about 3.008 second left to finish this epoch\n",
            "current epoch : 4 , current train loss : 0.01901757506720507\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.017996904 -> about 701.677 second left to finish this epoch\n",
            "batch loss : 0.016205627 -> about 673.128 second left to finish this epoch\n",
            "batch loss : 0.017087743 -> about 658.434 second left to finish this epoch\n",
            "batch loss : 0.01656715 -> about 643.959 second left to finish this epoch\n",
            "batch loss : 0.018283758 -> about 629.377 second left to finish this epoch\n",
            "batch loss : 0.01754899 -> about 614.832 second left to finish this epoch\n",
            "batch loss : 0.018021774 -> about 600.266 second left to finish this epoch\n",
            "batch loss : 0.017761908 -> about 586.098 second left to finish this epoch\n",
            "batch loss : 0.017324809 -> about 571.866 second left to finish this epoch\n",
            "batch loss : 0.016263485 -> about 558.138 second left to finish this epoch\n",
            "batch loss : 0.018526426 -> about 543.963 second left to finish this epoch\n",
            "batch loss : 0.017025746 -> about 529.875 second left to finish this epoch\n",
            "batch loss : 0.018291846 -> about 515.666 second left to finish this epoch\n",
            "batch loss : 0.015651455 -> about 501.623 second left to finish this epoch\n",
            "batch loss : 0.016202217 -> about 487.500 second left to finish this epoch\n",
            "batch loss : 0.017225862 -> about 473.275 second left to finish this epoch\n",
            "batch loss : 0.014707094 -> about 458.976 second left to finish this epoch\n",
            "batch loss : 0.016818926 -> about 444.769 second left to finish this epoch\n",
            "batch loss : 0.017518524 -> about 430.661 second left to finish this epoch\n",
            "batch loss : 0.01835295 -> about 416.575 second left to finish this epoch\n",
            "batch loss : 0.014203515 -> about 402.377 second left to finish this epoch\n",
            "batch loss : 0.018364709 -> about 388.130 second left to finish this epoch\n",
            "batch loss : 0.016769113 -> about 373.880 second left to finish this epoch\n",
            "batch loss : 0.014813248 -> about 359.635 second left to finish this epoch\n",
            "batch loss : 0.016788067 -> about 345.342 second left to finish this epoch\n",
            "batch loss : 0.01633236 -> about 331.042 second left to finish this epoch\n",
            "batch loss : 0.013005218 -> about 316.795 second left to finish this epoch\n",
            "batch loss : 0.0155576095 -> about 302.526 second left to finish this epoch\n",
            "batch loss : 0.0150504345 -> about 288.289 second left to finish this epoch\n",
            "batch loss : 0.012729514 -> about 274.026 second left to finish this epoch\n",
            "batch loss : 0.016566336 -> about 259.770 second left to finish this epoch\n",
            "batch loss : 0.018402576 -> about 245.509 second left to finish this epoch\n",
            "batch loss : 0.015504252 -> about 231.238 second left to finish this epoch\n",
            "batch loss : 0.01655134 -> about 216.972 second left to finish this epoch\n",
            "batch loss : 0.01592243 -> about 202.711 second left to finish this epoch\n",
            "batch loss : 0.017337928 -> about 188.450 second left to finish this epoch\n",
            "batch loss : 0.013830258 -> about 174.176 second left to finish this epoch\n",
            "batch loss : 0.014253756 -> about 159.919 second left to finish this epoch\n",
            "batch loss : 0.013595458 -> about 145.651 second left to finish this epoch\n",
            "batch loss : 0.013651372 -> about 131.383 second left to finish this epoch\n",
            "batch loss : 0.016174048 -> about 117.136 second left to finish this epoch\n",
            "batch loss : 0.013947398 -> about 102.881 second left to finish this epoch\n",
            "batch loss : 0.0136758275 -> about 88.617 second left to finish this epoch\n",
            "batch loss : 0.019511703 -> about 74.349 second left to finish this epoch\n",
            "batch loss : 0.01581719 -> about 60.078 second left to finish this epoch\n",
            "batch loss : 0.016285807 -> about 45.806 second left to finish this epoch\n",
            "batch loss : 0.013679661 -> about 31.536 second left to finish this epoch\n",
            "batch loss : 0.017425012 -> about 17.265 second left to finish this epoch\n",
            "batch loss : 0.015797578 -> about 2.996 second left to finish this epoch\n",
            "current epoch : 5 , current train loss : 0.01628560700554851\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.012175622 -> about 697.547 second left to finish this epoch\n",
            "batch loss : 0.014471052 -> about 672.016 second left to finish this epoch\n",
            "batch loss : 0.018272452 -> about 658.137 second left to finish this epoch\n",
            "batch loss : 0.011576158 -> about 643.872 second left to finish this epoch\n",
            "batch loss : 0.014690921 -> about 628.627 second left to finish this epoch\n",
            "batch loss : 0.011510005 -> about 614.369 second left to finish this epoch\n",
            "batch loss : 0.01584107 -> about 600.273 second left to finish this epoch\n",
            "batch loss : 0.013898678 -> about 586.241 second left to finish this epoch\n",
            "batch loss : 0.014420586 -> about 572.023 second left to finish this epoch\n",
            "batch loss : 0.014510386 -> about 557.748 second left to finish this epoch\n",
            "batch loss : 0.010166609 -> about 543.528 second left to finish this epoch\n",
            "batch loss : 0.0145481415 -> about 529.375 second left to finish this epoch\n",
            "batch loss : 0.010368429 -> about 515.169 second left to finish this epoch\n",
            "batch loss : 0.011684244 -> about 500.830 second left to finish this epoch\n",
            "batch loss : 0.013182832 -> about 486.942 second left to finish this epoch\n",
            "batch loss : 0.013225688 -> about 472.713 second left to finish this epoch\n",
            "batch loss : 0.00917998 -> about 458.404 second left to finish this epoch\n",
            "batch loss : 0.010961462 -> about 444.170 second left to finish this epoch\n",
            "batch loss : 0.015341571 -> about 429.935 second left to finish this epoch\n",
            "batch loss : 0.009587323 -> about 415.686 second left to finish this epoch\n",
            "batch loss : 0.013676487 -> about 401.440 second left to finish this epoch\n",
            "batch loss : 0.008681439 -> about 387.166 second left to finish this epoch\n",
            "batch loss : 0.010281483 -> about 372.941 second left to finish this epoch\n",
            "batch loss : 0.01611093 -> about 358.715 second left to finish this epoch\n",
            "batch loss : 0.010652429 -> about 344.466 second left to finish this epoch\n",
            "batch loss : 0.01205701 -> about 330.204 second left to finish this epoch\n",
            "batch loss : 0.013490995 -> about 315.934 second left to finish this epoch\n",
            "batch loss : 0.013936231 -> about 301.717 second left to finish this epoch\n",
            "batch loss : 0.009040208 -> about 287.497 second left to finish this epoch\n",
            "batch loss : 0.013959675 -> about 273.315 second left to finish this epoch\n",
            "batch loss : 0.009349224 -> about 259.062 second left to finish this epoch\n",
            "batch loss : 0.011510823 -> about 244.817 second left to finish this epoch\n",
            "batch loss : 0.014608668 -> about 230.590 second left to finish this epoch\n",
            "batch loss : 0.013590893 -> about 216.363 second left to finish this epoch\n",
            "batch loss : 0.006112166 -> about 202.143 second left to finish this epoch\n",
            "batch loss : 0.014101576 -> about 187.927 second left to finish this epoch\n",
            "batch loss : 0.012540491 -> about 173.742 second left to finish this epoch\n",
            "batch loss : 0.01326363 -> about 159.511 second left to finish this epoch\n",
            "batch loss : 0.009666456 -> about 145.277 second left to finish this epoch\n",
            "batch loss : 0.0075982856 -> about 131.046 second left to finish this epoch\n",
            "batch loss : 0.012597089 -> about 116.817 second left to finish this epoch\n",
            "batch loss : 0.010502544 -> about 102.588 second left to finish this epoch\n",
            "batch loss : 0.014013729 -> about 88.354 second left to finish this epoch\n",
            "batch loss : 0.010461576 -> about 74.119 second left to finish this epoch\n",
            "batch loss : 0.012941718 -> about 59.891 second left to finish this epoch\n",
            "batch loss : 0.006196662 -> about 45.663 second left to finish this epoch\n",
            "batch loss : 0.009780003 -> about 31.437 second left to finish this epoch\n",
            "batch loss : 0.010957396 -> about 17.211 second left to finish this epoch\n",
            "batch loss : 0.015542524 -> about 2.987 second left to finish this epoch\n",
            "current epoch : 6 , current train loss : 0.012387424483420078\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0090199625 -> about 681.767 second left to finish this epoch\n",
            "batch loss : 0.012682732 -> about 672.582 second left to finish this epoch\n",
            "batch loss : 0.011760099 -> about 658.374 second left to finish this epoch\n",
            "batch loss : 0.012136479 -> about 644.292 second left to finish this epoch\n",
            "batch loss : 0.008436118 -> about 629.642 second left to finish this epoch\n",
            "batch loss : 0.009379362 -> about 615.173 second left to finish this epoch\n",
            "batch loss : 0.0054647927 -> about 600.795 second left to finish this epoch\n",
            "batch loss : 0.011873817 -> about 586.343 second left to finish this epoch\n",
            "batch loss : 0.013959409 -> about 572.140 second left to finish this epoch\n",
            "batch loss : 0.007967707 -> about 558.655 second left to finish this epoch\n",
            "batch loss : 0.008102525 -> about 544.349 second left to finish this epoch\n",
            "batch loss : 0.0114521235 -> about 529.966 second left to finish this epoch\n",
            "batch loss : 0.01410382 -> about 515.591 second left to finish this epoch\n",
            "batch loss : 0.008484057 -> about 501.250 second left to finish this epoch\n",
            "batch loss : 0.009241732 -> about 487.083 second left to finish this epoch\n",
            "batch loss : 0.015946776 -> about 472.867 second left to finish this epoch\n",
            "batch loss : 0.008163291 -> about 458.617 second left to finish this epoch\n",
            "batch loss : 0.010890475 -> about 444.429 second left to finish this epoch\n",
            "batch loss : 0.009822288 -> about 430.260 second left to finish this epoch\n",
            "batch loss : 0.008216302 -> about 415.989 second left to finish this epoch\n",
            "batch loss : 0.011527335 -> about 401.786 second left to finish this epoch\n",
            "batch loss : 0.010240454 -> about 387.508 second left to finish this epoch\n",
            "batch loss : 0.00999265 -> about 373.266 second left to finish this epoch\n",
            "batch loss : 0.009113146 -> about 359.002 second left to finish this epoch\n",
            "batch loss : 0.013564984 -> about 344.782 second left to finish this epoch\n",
            "batch loss : 0.0077252616 -> about 330.548 second left to finish this epoch\n",
            "batch loss : 0.0046650446 -> about 316.283 second left to finish this epoch\n",
            "batch loss : 0.009171803 -> about 302.038 second left to finish this epoch\n",
            "batch loss : 0.009401602 -> about 287.758 second left to finish this epoch\n",
            "batch loss : 0.012418806 -> about 273.470 second left to finish this epoch\n",
            "batch loss : 0.013911417 -> about 259.206 second left to finish this epoch\n",
            "batch loss : 0.011282897 -> about 245.033 second left to finish this epoch\n",
            "batch loss : 0.008077728 -> about 230.767 second left to finish this epoch\n",
            "batch loss : 0.00951263 -> about 216.498 second left to finish this epoch\n",
            "batch loss : 0.0147996135 -> about 202.249 second left to finish this epoch\n",
            "batch loss : 0.01465011 -> about 187.982 second left to finish this epoch\n",
            "batch loss : 0.008208524 -> about 173.733 second left to finish this epoch\n",
            "batch loss : 0.0075093084 -> about 159.486 second left to finish this epoch\n",
            "batch loss : 0.0071143387 -> about 145.240 second left to finish this epoch\n",
            "batch loss : 0.0062631364 -> about 131.010 second left to finish this epoch\n",
            "batch loss : 0.014277749 -> about 116.772 second left to finish this epoch\n",
            "batch loss : 0.009193555 -> about 102.541 second left to finish this epoch\n",
            "batch loss : 0.015541658 -> about 88.313 second left to finish this epoch\n",
            "batch loss : 0.006987317 -> about 74.086 second left to finish this epoch\n",
            "batch loss : 0.0075048553 -> about 59.859 second left to finish this epoch\n",
            "batch loss : 0.0070727994 -> about 45.640 second left to finish this epoch\n",
            "batch loss : 0.007079577 -> about 31.419 second left to finish this epoch\n",
            "batch loss : 0.0095097115 -> about 17.201 second left to finish this epoch\n",
            "batch loss : 0.011527142 -> about 2.985 second left to finish this epoch\n",
            "current epoch : 7 , current train loss : 0.009863049901706614\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0077572498 -> about 688.888 second left to finish this epoch\n",
            "batch loss : 0.012850301 -> about 671.483 second left to finish this epoch\n",
            "batch loss : 0.008966987 -> about 655.729 second left to finish this epoch\n",
            "batch loss : 0.009457028 -> about 641.720 second left to finish this epoch\n",
            "batch loss : 0.009449994 -> about 627.367 second left to finish this epoch\n",
            "batch loss : 0.005576077 -> about 614.126 second left to finish this epoch\n",
            "batch loss : 0.005860374 -> about 599.694 second left to finish this epoch\n",
            "batch loss : 0.007946215 -> about 585.204 second left to finish this epoch\n",
            "batch loss : 0.010538116 -> about 571.066 second left to finish this epoch\n",
            "batch loss : 0.009958319 -> about 556.675 second left to finish this epoch\n",
            "batch loss : 0.0077310847 -> about 542.365 second left to finish this epoch\n",
            "batch loss : 0.005854322 -> about 528.053 second left to finish this epoch\n",
            "batch loss : 0.0042153653 -> about 513.763 second left to finish this epoch\n",
            "batch loss : 0.0071845097 -> about 499.416 second left to finish this epoch\n",
            "batch loss : 0.008526464 -> about 485.096 second left to finish this epoch\n",
            "batch loss : 0.009747168 -> about 470.878 second left to finish this epoch\n",
            "batch loss : 0.010444023 -> about 456.692 second left to finish this epoch\n",
            "batch loss : 0.0067811063 -> about 442.528 second left to finish this epoch\n",
            "batch loss : 0.009695068 -> about 428.376 second left to finish this epoch\n",
            "batch loss : 0.016970664 -> about 414.152 second left to finish this epoch\n",
            "batch loss : 0.005278892 -> about 399.914 second left to finish this epoch\n",
            "batch loss : 0.0056455247 -> about 385.676 second left to finish this epoch\n",
            "batch loss : 0.006691047 -> about 371.499 second left to finish this epoch\n",
            "batch loss : 0.011015501 -> about 357.291 second left to finish this epoch\n",
            "batch loss : 0.009329456 -> about 343.086 second left to finish this epoch\n",
            "batch loss : 0.0078111636 -> about 328.891 second left to finish this epoch\n",
            "batch loss : 0.008290739 -> about 314.724 second left to finish this epoch\n",
            "batch loss : 0.007772553 -> about 300.630 second left to finish this epoch\n",
            "batch loss : 0.0040691714 -> about 286.463 second left to finish this epoch\n",
            "batch loss : 0.007999654 -> about 272.252 second left to finish this epoch\n",
            "batch loss : 0.007068963 -> about 258.074 second left to finish this epoch\n",
            "batch loss : 0.0059571024 -> about 243.883 second left to finish this epoch\n",
            "batch loss : 0.00619042 -> about 229.714 second left to finish this epoch\n",
            "batch loss : 0.009471881 -> about 215.533 second left to finish this epoch\n",
            "batch loss : 0.0061579975 -> about 201.363 second left to finish this epoch\n",
            "batch loss : 0.012423684 -> about 187.175 second left to finish this epoch\n",
            "batch loss : 0.0066532744 -> about 172.995 second left to finish this epoch\n",
            "batch loss : 0.0073250495 -> about 158.824 second left to finish this epoch\n",
            "batch loss : 0.0077791526 -> about 144.653 second left to finish this epoch\n",
            "batch loss : 0.008990041 -> about 130.486 second left to finish this epoch\n",
            "batch loss : 0.009121336 -> about 116.320 second left to finish this epoch\n",
            "batch loss : 0.011821756 -> about 102.152 second left to finish this epoch\n",
            "batch loss : 0.0077603543 -> about 87.977 second left to finish this epoch\n",
            "batch loss : 0.0044387216 -> about 73.809 second left to finish this epoch\n",
            "batch loss : 0.006091675 -> about 59.640 second left to finish this epoch\n",
            "batch loss : 0.004758832 -> about 45.472 second left to finish this epoch\n",
            "batch loss : 0.013927846 -> about 31.305 second left to finish this epoch\n",
            "batch loss : 0.007316099 -> about 17.140 second left to finish this epoch\n",
            "batch loss : 0.009602199 -> about 2.975 second left to finish this epoch\n",
            "current epoch : 8 , current train loss : 0.008375589153294723\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0068487125 -> about 695.362 second left to finish this epoch\n",
            "batch loss : 0.0038091564 -> about 671.522 second left to finish this epoch\n",
            "batch loss : 0.0037746313 -> about 655.872 second left to finish this epoch\n",
            "batch loss : 0.004856956 -> about 640.990 second left to finish this epoch\n",
            "batch loss : 0.011897048 -> about 626.300 second left to finish this epoch\n",
            "batch loss : 0.007285671 -> about 611.661 second left to finish this epoch\n",
            "batch loss : 0.007945463 -> about 597.317 second left to finish this epoch\n",
            "batch loss : 0.0057719937 -> about 583.146 second left to finish this epoch\n",
            "batch loss : 0.0063074958 -> about 568.891 second left to finish this epoch\n",
            "batch loss : 0.004582233 -> about 554.679 second left to finish this epoch\n",
            "batch loss : 0.010184264 -> about 540.490 second left to finish this epoch\n",
            "batch loss : 0.00403944 -> about 526.369 second left to finish this epoch\n",
            "batch loss : 0.008540167 -> about 512.217 second left to finish this epoch\n",
            "batch loss : 0.008073691 -> about 498.238 second left to finish this epoch\n",
            "batch loss : 0.0067201154 -> about 484.090 second left to finish this epoch\n",
            "batch loss : 0.009115234 -> about 469.879 second left to finish this epoch\n",
            "batch loss : 0.0053387266 -> about 455.646 second left to finish this epoch\n",
            "batch loss : 0.0060973866 -> about 441.514 second left to finish this epoch\n",
            "batch loss : 0.010252784 -> about 427.348 second left to finish this epoch\n",
            "batch loss : 0.0035014024 -> about 413.241 second left to finish this epoch\n",
            "batch loss : 0.00508223 -> about 399.102 second left to finish this epoch\n",
            "batch loss : 0.009063607 -> about 384.934 second left to finish this epoch\n",
            "batch loss : 0.0049524335 -> about 370.935 second left to finish this epoch\n",
            "batch loss : 0.0078005227 -> about 356.801 second left to finish this epoch\n",
            "batch loss : 0.0065429397 -> about 342.643 second left to finish this epoch\n",
            "batch loss : 0.0030702702 -> about 328.495 second left to finish this epoch\n",
            "batch loss : 0.0050822613 -> about 314.312 second left to finish this epoch\n",
            "batch loss : 0.007024862 -> about 300.141 second left to finish this epoch\n",
            "batch loss : 0.008169627 -> about 286.001 second left to finish this epoch\n",
            "batch loss : 0.010880973 -> about 271.821 second left to finish this epoch\n",
            "batch loss : 0.004092474 -> about 257.645 second left to finish this epoch\n",
            "batch loss : 0.008346166 -> about 243.518 second left to finish this epoch\n",
            "batch loss : 0.011896111 -> about 229.360 second left to finish this epoch\n",
            "batch loss : 0.009193893 -> about 215.220 second left to finish this epoch\n",
            "batch loss : 0.0042719794 -> about 201.080 second left to finish this epoch\n",
            "batch loss : 0.006962659 -> about 186.948 second left to finish this epoch\n",
            "batch loss : 0.0051743286 -> about 172.807 second left to finish this epoch\n",
            "batch loss : 0.0072979694 -> about 158.660 second left to finish this epoch\n",
            "batch loss : 0.004641345 -> about 144.503 second left to finish this epoch\n",
            "batch loss : 0.0030557767 -> about 130.353 second left to finish this epoch\n",
            "batch loss : 0.010643732 -> about 116.194 second left to finish this epoch\n",
            "batch loss : 0.0063994573 -> about 102.037 second left to finish this epoch\n",
            "batch loss : 0.0058767097 -> about 87.889 second left to finish this epoch\n",
            "batch loss : 0.005337462 -> about 73.735 second left to finish this epoch\n",
            "batch loss : 0.0097061405 -> about 59.595 second left to finish this epoch\n",
            "batch loss : 0.0033469778 -> about 45.440 second left to finish this epoch\n",
            "batch loss : 0.0022163573 -> about 31.284 second left to finish this epoch\n",
            "batch loss : 0.009046634 -> about 17.128 second left to finish this epoch\n",
            "batch loss : 0.006856826 -> about 2.973 second left to finish this epoch\n",
            "current epoch : 9 , current train loss : 0.007356130061509833\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.008029221 -> about 709.509 second left to finish this epoch\n",
            "batch loss : 0.006198503 -> about 669.845 second left to finish this epoch\n",
            "batch loss : 0.010609538 -> about 654.443 second left to finish this epoch\n",
            "batch loss : 0.008703281 -> about 639.977 second left to finish this epoch\n",
            "batch loss : 0.008403349 -> about 625.984 second left to finish this epoch\n",
            "batch loss : 0.0063497787 -> about 611.860 second left to finish this epoch\n",
            "batch loss : 0.005991405 -> about 597.692 second left to finish this epoch\n",
            "batch loss : 0.008141702 -> about 583.582 second left to finish this epoch\n",
            "batch loss : 0.008721703 -> about 569.435 second left to finish this epoch\n",
            "batch loss : 0.002219778 -> about 555.148 second left to finish this epoch\n",
            "batch loss : 0.005157172 -> about 540.894 second left to finish this epoch\n",
            "batch loss : 0.0047139195 -> about 526.764 second left to finish this epoch\n",
            "batch loss : 0.002568297 -> about 512.574 second left to finish this epoch\n",
            "batch loss : 0.0034822805 -> about 498.478 second left to finish this epoch\n",
            "batch loss : 0.008207883 -> about 484.246 second left to finish this epoch\n",
            "batch loss : 0.0076329596 -> about 470.009 second left to finish this epoch\n",
            "batch loss : 0.010000155 -> about 455.850 second left to finish this epoch\n",
            "batch loss : 0.008046851 -> about 441.721 second left to finish this epoch\n",
            "batch loss : 0.005394185 -> about 427.785 second left to finish this epoch\n",
            "batch loss : 0.0018073682 -> about 413.594 second left to finish this epoch\n",
            "batch loss : 0.011325299 -> about 399.441 second left to finish this epoch\n",
            "batch loss : 0.003970181 -> about 385.314 second left to finish this epoch\n",
            "batch loss : 0.008376813 -> about 371.197 second left to finish this epoch\n",
            "batch loss : 0.012600801 -> about 357.044 second left to finish this epoch\n",
            "batch loss : 0.0069191786 -> about 342.827 second left to finish this epoch\n",
            "batch loss : 0.007877076 -> about 328.597 second left to finish this epoch\n",
            "batch loss : 0.007728914 -> about 314.435 second left to finish this epoch\n",
            "batch loss : 0.0093653705 -> about 300.271 second left to finish this epoch\n",
            "batch loss : 0.0014845273 -> about 286.258 second left to finish this epoch\n",
            "batch loss : 0.0033616521 -> about 272.153 second left to finish this epoch\n",
            "batch loss : 0.0029134576 -> about 258.056 second left to finish this epoch\n",
            "batch loss : 0.010789175 -> about 243.921 second left to finish this epoch\n",
            "batch loss : 0.011333947 -> about 229.739 second left to finish this epoch\n",
            "batch loss : 0.0066963206 -> about 215.572 second left to finish this epoch\n",
            "batch loss : 0.007937359 -> about 201.378 second left to finish this epoch\n",
            "batch loss : 0.0064906934 -> about 187.202 second left to finish this epoch\n",
            "batch loss : 0.0047707953 -> about 173.031 second left to finish this epoch\n",
            "batch loss : 0.0049699545 -> about 158.853 second left to finish this epoch\n",
            "batch loss : 0.00892762 -> about 144.681 second left to finish this epoch\n",
            "batch loss : 0.0042842054 -> about 130.514 second left to finish this epoch\n",
            "batch loss : 0.0035603908 -> about 116.365 second left to finish this epoch\n",
            "batch loss : 0.0025627338 -> about 102.186 second left to finish this epoch\n",
            "batch loss : 0.007224813 -> about 88.006 second left to finish this epoch\n",
            "batch loss : 0.0016485597 -> about 73.829 second left to finish this epoch\n",
            "batch loss : 0.0023830286 -> about 59.655 second left to finish this epoch\n",
            "batch loss : 0.00594919 -> about 45.482 second left to finish this epoch\n",
            "batch loss : 0.006574543 -> about 31.313 second left to finish this epoch\n",
            "batch loss : 0.005976593 -> about 17.143 second left to finish this epoch\n",
            "batch loss : 0.0031635175 -> about 2.975 second left to finish this epoch\n",
            "current epoch : 10 , current train loss : 0.0065676771493763265\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0018688124 -> about 682.740 second left to finish this epoch\n",
            "batch loss : 0.0065675927 -> about 669.044 second left to finish this epoch\n",
            "batch loss : 0.0044762027 -> about 655.309 second left to finish this epoch\n",
            "batch loss : 0.004649623 -> about 640.989 second left to finish this epoch\n",
            "batch loss : 0.009348877 -> about 626.270 second left to finish this epoch\n",
            "batch loss : 0.011053275 -> about 612.033 second left to finish this epoch\n",
            "batch loss : 0.0037710227 -> about 597.785 second left to finish this epoch\n",
            "batch loss : 0.0029918493 -> about 583.423 second left to finish this epoch\n",
            "batch loss : 0.0042052628 -> about 569.238 second left to finish this epoch\n",
            "batch loss : 0.007544047 -> about 555.052 second left to finish this epoch\n",
            "batch loss : 0.011853807 -> about 540.963 second left to finish this epoch\n",
            "batch loss : 0.007609684 -> about 526.763 second left to finish this epoch\n",
            "batch loss : 0.0062502967 -> about 512.652 second left to finish this epoch\n",
            "batch loss : 0.00923574 -> about 498.715 second left to finish this epoch\n",
            "batch loss : 0.003911156 -> about 484.813 second left to finish this epoch\n",
            "batch loss : 0.0056833713 -> about 470.572 second left to finish this epoch\n",
            "batch loss : 0.007973619 -> about 456.396 second left to finish this epoch\n",
            "batch loss : 0.0093037635 -> about 442.156 second left to finish this epoch\n",
            "batch loss : 0.005575807 -> about 427.958 second left to finish this epoch\n",
            "batch loss : 0.010502292 -> about 413.757 second left to finish this epoch\n",
            "batch loss : 0.0062542083 -> about 399.634 second left to finish this epoch\n",
            "batch loss : 0.0062949928 -> about 385.558 second left to finish this epoch\n",
            "batch loss : 0.004094483 -> about 371.450 second left to finish this epoch\n",
            "batch loss : 0.006127715 -> about 357.271 second left to finish this epoch\n",
            "batch loss : 0.006234961 -> about 343.141 second left to finish this epoch\n",
            "batch loss : 0.00965863 -> about 328.980 second left to finish this epoch\n",
            "batch loss : 0.007342797 -> about 314.789 second left to finish this epoch\n",
            "batch loss : 0.0042747096 -> about 300.581 second left to finish this epoch\n",
            "batch loss : 0.010258099 -> about 286.426 second left to finish this epoch\n",
            "batch loss : 0.0059204213 -> about 272.242 second left to finish this epoch\n",
            "batch loss : 0.0069886167 -> about 258.052 second left to finish this epoch\n",
            "batch loss : 0.0037017767 -> about 243.890 second left to finish this epoch\n",
            "batch loss : 0.0034679936 -> about 229.703 second left to finish this epoch\n",
            "batch loss : 0.006405049 -> about 215.522 second left to finish this epoch\n",
            "batch loss : 0.0029775235 -> about 201.334 second left to finish this epoch\n",
            "batch loss : 0.0042360052 -> about 187.230 second left to finish this epoch\n",
            "batch loss : 0.004603755 -> about 173.124 second left to finish this epoch\n",
            "batch loss : 0.0035759313 -> about 158.986 second left to finish this epoch\n",
            "batch loss : 0.003384691 -> about 144.820 second left to finish this epoch\n",
            "batch loss : 0.0073289713 -> about 130.628 second left to finish this epoch\n",
            "batch loss : 0.0041810814 -> about 116.439 second left to finish this epoch\n",
            "batch loss : 0.005253825 -> about 102.253 second left to finish this epoch\n",
            "batch loss : 0.0033814376 -> about 88.070 second left to finish this epoch\n",
            "batch loss : 0.0058638686 -> about 73.888 second left to finish this epoch\n",
            "batch loss : 0.006958044 -> about 59.707 second left to finish this epoch\n",
            "batch loss : 0.0060779746 -> about 45.534 second left to finish this epoch\n",
            "batch loss : 0.006144503 -> about 31.351 second left to finish this epoch\n",
            "batch loss : 0.0043709376 -> about 17.165 second left to finish this epoch\n",
            "batch loss : 0.011538984 -> about 2.979 second left to finish this epoch\n",
            "current epoch : 11 , current train loss : 0.0058330159919510105\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0027301738 -> about 686.711 second left to finish this epoch\n",
            "batch loss : 0.007708979 -> about 672.060 second left to finish this epoch\n",
            "batch loss : 0.003629377 -> about 656.963 second left to finish this epoch\n",
            "batch loss : 0.0050697736 -> about 641.931 second left to finish this epoch\n",
            "batch loss : 0.014277389 -> about 627.568 second left to finish this epoch\n",
            "batch loss : 0.009015105 -> about 614.100 second left to finish this epoch\n",
            "batch loss : 0.004449087 -> about 600.197 second left to finish this epoch\n",
            "batch loss : 0.003176157 -> about 585.553 second left to finish this epoch\n",
            "batch loss : 0.0020097513 -> about 571.095 second left to finish this epoch\n",
            "batch loss : 0.0038373168 -> about 557.327 second left to finish this epoch\n",
            "batch loss : 0.0030281842 -> about 543.021 second left to finish this epoch\n",
            "batch loss : 0.0029463745 -> about 528.947 second left to finish this epoch\n",
            "batch loss : 0.005847295 -> about 515.112 second left to finish this epoch\n",
            "batch loss : 0.011734778 -> about 501.106 second left to finish this epoch\n",
            "batch loss : 0.004895557 -> about 486.976 second left to finish this epoch\n",
            "batch loss : 0.0051485496 -> about 472.813 second left to finish this epoch\n",
            "batch loss : 0.0019170674 -> about 458.660 second left to finish this epoch\n",
            "batch loss : 0.008104902 -> about 444.446 second left to finish this epoch\n",
            "batch loss : 0.004598693 -> about 430.143 second left to finish this epoch\n",
            "batch loss : 0.00624627 -> about 415.925 second left to finish this epoch\n",
            "batch loss : 0.009383304 -> about 401.611 second left to finish this epoch\n",
            "batch loss : 0.0077245547 -> about 387.240 second left to finish this epoch\n",
            "batch loss : 0.0040780823 -> about 372.956 second left to finish this epoch\n",
            "batch loss : 0.00575992 -> about 358.647 second left to finish this epoch\n",
            "batch loss : 0.0008277276 -> about 344.371 second left to finish this epoch\n",
            "batch loss : 0.0015541662 -> about 330.086 second left to finish this epoch\n",
            "batch loss : 0.0063749924 -> about 315.831 second left to finish this epoch\n",
            "batch loss : 0.0048238416 -> about 301.562 second left to finish this epoch\n",
            "batch loss : 0.0077711027 -> about 287.314 second left to finish this epoch\n",
            "batch loss : 0.0016076597 -> about 273.054 second left to finish this epoch\n",
            "batch loss : 0.0041175205 -> about 258.833 second left to finish this epoch\n",
            "batch loss : 0.0056552766 -> about 244.681 second left to finish this epoch\n",
            "batch loss : 0.0016586498 -> about 230.501 second left to finish this epoch\n",
            "batch loss : 0.0053602196 -> about 216.335 second left to finish this epoch\n",
            "batch loss : 0.0018331921 -> about 202.147 second left to finish this epoch\n",
            "batch loss : 0.0044175284 -> about 187.908 second left to finish this epoch\n",
            "batch loss : 0.0037695616 -> about 173.658 second left to finish this epoch\n",
            "batch loss : 0.004255265 -> about 159.409 second left to finish this epoch\n",
            "batch loss : 0.0025976226 -> about 145.175 second left to finish this epoch\n",
            "batch loss : 0.0027624972 -> about 130.949 second left to finish this epoch\n",
            "batch loss : 0.0023870538 -> about 116.723 second left to finish this epoch\n",
            "batch loss : 0.0047722934 -> about 102.498 second left to finish this epoch\n",
            "batch loss : 0.006683166 -> about 88.276 second left to finish this epoch\n",
            "batch loss : 0.0059416164 -> about 74.051 second left to finish this epoch\n",
            "batch loss : 0.0048497478 -> about 59.833 second left to finish this epoch\n",
            "batch loss : 0.005130061 -> about 45.619 second left to finish this epoch\n",
            "batch loss : 0.007712822 -> about 31.404 second left to finish this epoch\n",
            "batch loss : 0.002607541 -> about 17.193 second left to finish this epoch\n",
            "batch loss : 0.0011736297 -> about 2.984 second left to finish this epoch\n",
            "current epoch : 12 , current train loss : 0.00506081973034508\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0031196545 -> about 684.542 second left to finish this epoch\n",
            "batch loss : 0.0038360143 -> about 669.534 second left to finish this epoch\n",
            "batch loss : 0.0073965658 -> about 655.281 second left to finish this epoch\n",
            "batch loss : 0.0039737336 -> about 640.922 second left to finish this epoch\n",
            "batch loss : 0.00933049 -> about 627.109 second left to finish this epoch\n",
            "batch loss : 0.0037176942 -> about 612.500 second left to finish this epoch\n",
            "batch loss : 0.009734081 -> about 598.015 second left to finish this epoch\n",
            "batch loss : 0.0042594867 -> about 583.869 second left to finish this epoch\n",
            "batch loss : 0.0029489351 -> about 569.537 second left to finish this epoch\n",
            "batch loss : 0.006452495 -> about 555.578 second left to finish this epoch\n",
            "batch loss : 0.004048692 -> about 541.443 second left to finish this epoch\n",
            "batch loss : 0.0031622443 -> about 527.088 second left to finish this epoch\n",
            "batch loss : 0.0038792167 -> about 512.794 second left to finish this epoch\n",
            "batch loss : 0.0013803232 -> about 498.544 second left to finish this epoch\n",
            "batch loss : 0.00744501 -> about 484.380 second left to finish this epoch\n",
            "batch loss : 0.0027946788 -> about 470.056 second left to finish this epoch\n",
            "batch loss : 0.0021474285 -> about 455.824 second left to finish this epoch\n",
            "batch loss : 0.0038949584 -> about 441.571 second left to finish this epoch\n",
            "batch loss : 0.0019021156 -> about 427.356 second left to finish this epoch\n",
            "batch loss : 0.0019936766 -> about 413.173 second left to finish this epoch\n",
            "batch loss : 0.006154511 -> about 398.991 second left to finish this epoch\n",
            "batch loss : 0.0024310034 -> about 384.826 second left to finish this epoch\n",
            "batch loss : 0.00627923 -> about 370.620 second left to finish this epoch\n",
            "batch loss : 0.0035256436 -> about 356.463 second left to finish this epoch\n",
            "batch loss : 0.002852518 -> about 342.286 second left to finish this epoch\n",
            "batch loss : 0.0052132276 -> about 328.152 second left to finish this epoch\n",
            "batch loss : 0.004255044 -> about 314.083 second left to finish this epoch\n",
            "batch loss : 0.0052112574 -> about 299.959 second left to finish this epoch\n",
            "batch loss : 0.001578087 -> about 285.841 second left to finish this epoch\n",
            "batch loss : 0.0026900624 -> about 271.669 second left to finish this epoch\n",
            "batch loss : 0.0015655576 -> about 257.513 second left to finish this epoch\n",
            "batch loss : 0.002654229 -> about 243.366 second left to finish this epoch\n",
            "batch loss : 0.008484521 -> about 229.204 second left to finish this epoch\n",
            "batch loss : 0.0026260167 -> about 215.053 second left to finish this epoch\n",
            "batch loss : 0.0052223997 -> about 200.906 second left to finish this epoch\n",
            "batch loss : 0.006082746 -> about 186.784 second left to finish this epoch\n",
            "batch loss : 0.0010437808 -> about 172.642 second left to finish this epoch\n",
            "batch loss : 0.008592349 -> about 158.506 second left to finish this epoch\n",
            "batch loss : 0.005616549 -> about 144.358 second left to finish this epoch\n",
            "batch loss : 0.0010664333 -> about 130.204 second left to finish this epoch\n",
            "batch loss : 0.0023652988 -> about 116.058 second left to finish this epoch\n",
            "batch loss : 0.0016123845 -> about 101.919 second left to finish this epoch\n",
            "batch loss : 0.0044487426 -> about 87.783 second left to finish this epoch\n",
            "batch loss : 0.0044185906 -> about 73.644 second left to finish this epoch\n",
            "batch loss : 0.0012057632 -> about 59.510 second left to finish this epoch\n",
            "batch loss : 0.0032138668 -> about 45.373 second left to finish this epoch\n",
            "batch loss : 0.0045980606 -> about 31.238 second left to finish this epoch\n",
            "batch loss : 0.0025631767 -> about 17.103 second left to finish this epoch\n",
            "batch loss : 0.004900802 -> about 2.969 second left to finish this epoch\n",
            "current epoch : 13 , current train loss : 0.004379451540838941\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.003978299 -> about 691.121 second left to finish this epoch\n",
            "batch loss : 0.00206673 -> about 667.597 second left to finish this epoch\n",
            "batch loss : 0.005602219 -> about 652.733 second left to finish this epoch\n",
            "batch loss : 0.0013010345 -> about 639.055 second left to finish this epoch\n",
            "batch loss : 0.003056078 -> about 624.884 second left to finish this epoch\n",
            "batch loss : 0.006657134 -> about 611.076 second left to finish this epoch\n",
            "batch loss : 0.005222442 -> about 596.958 second left to finish this epoch\n",
            "batch loss : 0.007310437 -> about 582.722 second left to finish this epoch\n",
            "batch loss : 0.0011393572 -> about 568.469 second left to finish this epoch\n",
            "batch loss : 0.006125847 -> about 554.163 second left to finish this epoch\n",
            "batch loss : 0.0049468484 -> about 539.950 second left to finish this epoch\n",
            "batch loss : 0.002624671 -> about 525.832 second left to finish this epoch\n",
            "batch loss : 0.0044222395 -> about 511.649 second left to finish this epoch\n",
            "batch loss : 0.002876065 -> about 497.486 second left to finish this epoch\n",
            "batch loss : 0.0032828627 -> about 483.403 second left to finish this epoch\n",
            "batch loss : 0.0043144715 -> about 469.268 second left to finish this epoch\n",
            "batch loss : 0.0026303064 -> about 455.125 second left to finish this epoch\n",
            "batch loss : 0.0016311546 -> about 440.919 second left to finish this epoch\n",
            "batch loss : 0.004211476 -> about 426.760 second left to finish this epoch\n",
            "batch loss : 0.0058986805 -> about 412.601 second left to finish this epoch\n",
            "batch loss : 0.0035048747 -> about 398.429 second left to finish this epoch\n",
            "batch loss : 0.0049059126 -> about 384.351 second left to finish this epoch\n",
            "batch loss : 0.0017567977 -> about 370.422 second left to finish this epoch\n",
            "batch loss : 0.0022481857 -> about 356.344 second left to finish this epoch\n",
            "batch loss : 0.0014111301 -> about 342.253 second left to finish this epoch\n",
            "batch loss : 0.0032919326 -> about 328.154 second left to finish this epoch\n",
            "batch loss : 0.0063094506 -> about 314.032 second left to finish this epoch\n",
            "batch loss : 0.0036587561 -> about 299.901 second left to finish this epoch\n",
            "batch loss : 0.004939665 -> about 285.771 second left to finish this epoch\n",
            "batch loss : 0.0021050877 -> about 271.650 second left to finish this epoch\n",
            "batch loss : 0.0068046856 -> about 257.545 second left to finish this epoch\n",
            "batch loss : 0.0034863101 -> about 243.424 second left to finish this epoch\n",
            "batch loss : 0.0043854266 -> about 229.274 second left to finish this epoch\n",
            "batch loss : 0.002102564 -> about 215.147 second left to finish this epoch\n",
            "batch loss : 0.0062265657 -> about 201.025 second left to finish this epoch\n",
            "batch loss : 0.0011949189 -> about 186.880 second left to finish this epoch\n",
            "batch loss : 0.0037925455 -> about 172.746 second left to finish this epoch\n",
            "batch loss : 0.002788898 -> about 158.610 second left to finish this epoch\n",
            "batch loss : 0.0042341845 -> about 144.476 second left to finish this epoch\n",
            "batch loss : 0.00086995703 -> about 130.323 second left to finish this epoch\n",
            "batch loss : 0.00421444 -> about 116.157 second left to finish this epoch\n",
            "batch loss : 0.0028731094 -> about 101.997 second left to finish this epoch\n",
            "batch loss : 0.0049073445 -> about 87.836 second left to finish this epoch\n",
            "batch loss : 0.0019427004 -> about 73.681 second left to finish this epoch\n",
            "batch loss : 0.0032238723 -> about 59.545 second left to finish this epoch\n",
            "batch loss : 0.007031276 -> about 45.399 second left to finish this epoch\n",
            "batch loss : 0.0043508317 -> about 31.253 second left to finish this epoch\n",
            "batch loss : 0.0030139615 -> about 17.110 second left to finish this epoch\n",
            "batch loss : 0.0012555112 -> about 2.969 second left to finish this epoch\n",
            "current epoch : 14 , current train loss : 0.003819591329617801\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0060617123 -> about 685.399 second left to finish this epoch\n",
            "batch loss : 0.0007301494 -> about 664.902 second left to finish this epoch\n",
            "batch loss : 0.0015688326 -> about 650.858 second left to finish this epoch\n",
            "batch loss : 0.0020518445 -> about 637.525 second left to finish this epoch\n",
            "batch loss : 0.007332285 -> about 623.280 second left to finish this epoch\n",
            "batch loss : 0.0022352363 -> about 609.149 second left to finish this epoch\n",
            "batch loss : 0.005147378 -> about 594.727 second left to finish this epoch\n",
            "batch loss : 0.0014482471 -> about 580.738 second left to finish this epoch\n",
            "batch loss : 0.0013017736 -> about 566.630 second left to finish this epoch\n",
            "batch loss : 0.003075562 -> about 552.516 second left to finish this epoch\n",
            "batch loss : 0.0011786737 -> about 538.374 second left to finish this epoch\n",
            "batch loss : 0.0034180817 -> about 524.318 second left to finish this epoch\n",
            "batch loss : 0.0060201604 -> about 510.165 second left to finish this epoch\n",
            "batch loss : 0.00068450614 -> about 496.058 second left to finish this epoch\n",
            "batch loss : 0.0034111827 -> about 481.985 second left to finish this epoch\n",
            "batch loss : 0.004899123 -> about 467.876 second left to finish this epoch\n",
            "batch loss : 0.0034802216 -> about 453.853 second left to finish this epoch\n",
            "batch loss : 0.006555665 -> about 439.860 second left to finish this epoch\n",
            "batch loss : 0.004143025 -> about 426.187 second left to finish this epoch\n",
            "batch loss : 0.0009986673 -> about 412.212 second left to finish this epoch\n",
            "batch loss : 0.0041330755 -> about 398.186 second left to finish this epoch\n",
            "batch loss : 0.002064642 -> about 384.151 second left to finish this epoch\n",
            "batch loss : 0.0015197664 -> about 369.993 second left to finish this epoch\n",
            "batch loss : 0.0013724926 -> about 355.848 second left to finish this epoch\n",
            "batch loss : 0.0025234395 -> about 341.733 second left to finish this epoch\n",
            "batch loss : 0.0023463187 -> about 327.639 second left to finish this epoch\n",
            "batch loss : 0.0035461858 -> about 313.501 second left to finish this epoch\n",
            "batch loss : 0.0074584354 -> about 299.356 second left to finish this epoch\n",
            "batch loss : 0.0010199011 -> about 285.225 second left to finish this epoch\n",
            "batch loss : 0.0038410313 -> about 271.080 second left to finish this epoch\n",
            "batch loss : 0.0008431941 -> about 256.976 second left to finish this epoch\n",
            "batch loss : 0.001020952 -> about 242.858 second left to finish this epoch\n",
            "batch loss : 0.001437017 -> about 228.729 second left to finish this epoch\n",
            "batch loss : 0.0042843097 -> about 214.611 second left to finish this epoch\n",
            "batch loss : 0.0038173145 -> about 200.489 second left to finish this epoch\n",
            "batch loss : 0.00091768417 -> about 186.371 second left to finish this epoch\n",
            "batch loss : 0.004085228 -> about 172.262 second left to finish this epoch\n",
            "batch loss : 0.0050666574 -> about 158.169 second left to finish this epoch\n",
            "batch loss : 0.0010326986 -> about 144.056 second left to finish this epoch\n",
            "batch loss : 0.010093516 -> about 129.958 second left to finish this epoch\n",
            "batch loss : 0.0005401942 -> about 115.869 second left to finish this epoch\n",
            "batch loss : 0.004014756 -> about 101.755 second left to finish this epoch\n",
            "batch loss : 0.0011195837 -> about 87.642 second left to finish this epoch\n",
            "batch loss : 0.0036836644 -> about 73.540 second left to finish this epoch\n",
            "batch loss : 0.0037677125 -> about 59.425 second left to finish this epoch\n",
            "batch loss : 0.0066170385 -> about 45.312 second left to finish this epoch\n",
            "batch loss : 0.0056331977 -> about 31.196 second left to finish this epoch\n",
            "batch loss : 0.0011023704 -> about 17.081 second left to finish this epoch\n",
            "batch loss : 0.005377334 -> about 2.965 second left to finish this epoch\n",
            "current epoch : 15 , current train loss : 0.0033421442045462567\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0017011687 -> about 683.439 second left to finish this epoch\n",
            "batch loss : 0.002350396 -> about 669.125 second left to finish this epoch\n",
            "batch loss : 0.001950092 -> about 654.407 second left to finish this epoch\n",
            "batch loss : 0.002130566 -> about 639.825 second left to finish this epoch\n",
            "batch loss : 0.0011684246 -> about 627.371 second left to finish this epoch\n",
            "batch loss : 0.0034528498 -> about 613.729 second left to finish this epoch\n",
            "batch loss : 0.0029261673 -> about 600.241 second left to finish this epoch\n",
            "batch loss : 0.003956293 -> about 585.857 second left to finish this epoch\n",
            "batch loss : 0.00469443 -> about 571.290 second left to finish this epoch\n",
            "batch loss : 0.001881493 -> about 556.720 second left to finish this epoch\n",
            "batch loss : 0.0040842644 -> about 542.342 second left to finish this epoch\n",
            "batch loss : 0.0021717495 -> about 528.266 second left to finish this epoch\n",
            "batch loss : 0.0100703 -> about 513.892 second left to finish this epoch\n",
            "batch loss : 0.0016114821 -> about 499.890 second left to finish this epoch\n",
            "batch loss : 0.0017604642 -> about 485.626 second left to finish this epoch\n",
            "batch loss : 0.0019286271 -> about 471.390 second left to finish this epoch\n",
            "batch loss : 0.003708341 -> about 457.111 second left to finish this epoch\n",
            "batch loss : 0.004365921 -> about 442.885 second left to finish this epoch\n",
            "batch loss : 0.0025700526 -> about 428.669 second left to finish this epoch\n",
            "batch loss : 0.0017746957 -> about 414.369 second left to finish this epoch\n",
            "batch loss : 0.004978738 -> about 400.219 second left to finish this epoch\n",
            "batch loss : 0.0023983156 -> about 385.952 second left to finish this epoch\n",
            "batch loss : 0.009348186 -> about 371.728 second left to finish this epoch\n",
            "batch loss : 0.0011507294 -> about 357.562 second left to finish this epoch\n",
            "batch loss : 0.0040238127 -> about 343.381 second left to finish this epoch\n",
            "batch loss : 0.0013642847 -> about 329.148 second left to finish this epoch\n",
            "batch loss : 0.0015812048 -> about 314.950 second left to finish this epoch\n",
            "batch loss : 0.0021569147 -> about 300.816 second left to finish this epoch\n",
            "batch loss : 0.0023268599 -> about 286.689 second left to finish this epoch\n",
            "batch loss : 0.0015844353 -> about 272.485 second left to finish this epoch\n",
            "batch loss : 0.0019870964 -> about 258.287 second left to finish this epoch\n",
            "batch loss : 0.008133252 -> about 244.078 second left to finish this epoch\n",
            "batch loss : 0.003835681 -> about 229.880 second left to finish this epoch\n",
            "batch loss : 0.0065224064 -> about 215.706 second left to finish this epoch\n",
            "batch loss : 0.00480213 -> about 201.512 second left to finish this epoch\n",
            "batch loss : 0.001953775 -> about 187.379 second left to finish this epoch\n",
            "batch loss : 0.0017460652 -> about 173.177 second left to finish this epoch\n",
            "batch loss : 0.0023726588 -> about 158.991 second left to finish this epoch\n",
            "batch loss : 0.002480126 -> about 144.790 second left to finish this epoch\n",
            "batch loss : 0.0019739503 -> about 130.608 second left to finish this epoch\n",
            "batch loss : 0.0011753847 -> about 116.425 second left to finish this epoch\n",
            "batch loss : 0.001630585 -> about 102.247 second left to finish this epoch\n",
            "batch loss : 0.0012182327 -> about 88.061 second left to finish this epoch\n",
            "batch loss : 0.008346915 -> about 73.877 second left to finish this epoch\n",
            "batch loss : 0.0023960695 -> about 59.695 second left to finish this epoch\n",
            "batch loss : 0.0038274154 -> about 45.515 second left to finish this epoch\n",
            "batch loss : 0.0063633076 -> about 31.335 second left to finish this epoch\n",
            "batch loss : 0.004968614 -> about 17.158 second left to finish this epoch\n",
            "batch loss : 0.0006883657 -> about 2.978 second left to finish this epoch\n",
            "current epoch : 16 , current train loss : 0.0029295695208589123\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.002643307 -> about 687.351 second left to finish this epoch\n",
            "batch loss : 0.0017396755 -> about 668.621 second left to finish this epoch\n",
            "batch loss : 0.0023379629 -> about 654.207 second left to finish this epoch\n",
            "batch loss : 0.004949213 -> about 640.254 second left to finish this epoch\n",
            "batch loss : 0.0032362044 -> about 626.046 second left to finish this epoch\n",
            "batch loss : 0.0057873125 -> about 611.684 second left to finish this epoch\n",
            "batch loss : 0.0038332136 -> about 597.781 second left to finish this epoch\n",
            "batch loss : 0.002960175 -> about 583.779 second left to finish this epoch\n",
            "batch loss : 0.0012961797 -> about 569.564 second left to finish this epoch\n",
            "batch loss : 0.0017608082 -> about 556.032 second left to finish this epoch\n",
            "batch loss : 0.002206483 -> about 541.666 second left to finish this epoch\n",
            "batch loss : 0.0014688566 -> about 527.445 second left to finish this epoch\n",
            "batch loss : 0.0056863325 -> about 513.168 second left to finish this epoch\n",
            "batch loss : 0.000792676 -> about 499.040 second left to finish this epoch\n",
            "batch loss : 0.0008939369 -> about 484.911 second left to finish this epoch\n",
            "batch loss : 0.002563412 -> about 470.761 second left to finish this epoch\n",
            "batch loss : 0.0023570855 -> about 456.579 second left to finish this epoch\n",
            "batch loss : 0.004842451 -> about 442.356 second left to finish this epoch\n",
            "batch loss : 0.0043992847 -> about 428.137 second left to finish this epoch\n",
            "batch loss : 0.0015348044 -> about 413.981 second left to finish this epoch\n",
            "batch loss : 0.0015806186 -> about 399.880 second left to finish this epoch\n",
            "batch loss : 0.0020118107 -> about 385.764 second left to finish this epoch\n",
            "batch loss : 0.0044471184 -> about 371.539 second left to finish this epoch\n",
            "batch loss : 0.0015293158 -> about 357.333 second left to finish this epoch\n",
            "batch loss : 0.0044176504 -> about 343.107 second left to finish this epoch\n",
            "batch loss : 0.0013845095 -> about 328.926 second left to finish this epoch\n",
            "batch loss : 0.001117931 -> about 314.759 second left to finish this epoch\n",
            "batch loss : 0.0012928951 -> about 300.565 second left to finish this epoch\n",
            "batch loss : 0.0015952475 -> about 286.376 second left to finish this epoch\n",
            "batch loss : 0.0030213718 -> about 272.206 second left to finish this epoch\n",
            "batch loss : 0.0029839627 -> about 258.037 second left to finish this epoch\n",
            "batch loss : 0.004176939 -> about 243.953 second left to finish this epoch\n",
            "batch loss : 0.0048703114 -> about 229.767 second left to finish this epoch\n",
            "batch loss : 0.0013523366 -> about 215.588 second left to finish this epoch\n",
            "batch loss : 0.0013630046 -> about 201.407 second left to finish this epoch\n",
            "batch loss : 0.0013230246 -> about 187.233 second left to finish this epoch\n",
            "batch loss : 0.0016751745 -> about 173.066 second left to finish this epoch\n",
            "batch loss : 0.0009776396 -> about 158.897 second left to finish this epoch\n",
            "batch loss : 0.0012046627 -> about 144.716 second left to finish this epoch\n",
            "batch loss : 0.0026306103 -> about 130.536 second left to finish this epoch\n",
            "batch loss : 0.0013587443 -> about 116.381 second left to finish this epoch\n",
            "batch loss : 0.0015347231 -> about 102.209 second left to finish this epoch\n",
            "batch loss : 0.0010668222 -> about 88.036 second left to finish this epoch\n",
            "batch loss : 0.0006612239 -> about 73.861 second left to finish this epoch\n",
            "batch loss : 0.0018584698 -> about 59.680 second left to finish this epoch\n",
            "batch loss : 0.002420906 -> about 45.502 second left to finish this epoch\n",
            "batch loss : 0.009436864 -> about 31.326 second left to finish this epoch\n",
            "batch loss : 0.001292204 -> about 17.152 second left to finish this epoch\n",
            "batch loss : 0.0050632316 -> about 2.977 second left to finish this epoch\n",
            "current epoch : 17 , current train loss : 0.0025734414518905468\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0017061229 -> about 700.422 second left to finish this epoch\n",
            "batch loss : 0.0014964606 -> about 670.032 second left to finish this epoch\n",
            "batch loss : 0.0037742415 -> about 655.014 second left to finish this epoch\n",
            "batch loss : 0.0032287021 -> about 640.657 second left to finish this epoch\n",
            "batch loss : 0.0012214326 -> about 627.583 second left to finish this epoch\n",
            "batch loss : 0.0027456088 -> about 613.670 second left to finish this epoch\n",
            "batch loss : 0.0008936424 -> about 599.414 second left to finish this epoch\n",
            "batch loss : 0.0043348977 -> about 584.841 second left to finish this epoch\n",
            "batch loss : 0.0035467597 -> about 570.581 second left to finish this epoch\n",
            "batch loss : 0.0018666354 -> about 556.381 second left to finish this epoch\n",
            "batch loss : 0.0009459766 -> about 542.106 second left to finish this epoch\n",
            "batch loss : 0.0016877395 -> about 527.883 second left to finish this epoch\n",
            "batch loss : 0.0034436085 -> about 514.003 second left to finish this epoch\n",
            "batch loss : 0.0029769398 -> about 499.769 second left to finish this epoch\n",
            "batch loss : 0.001838325 -> about 485.674 second left to finish this epoch\n",
            "batch loss : 0.0007583084 -> about 471.347 second left to finish this epoch\n",
            "batch loss : 0.0042222096 -> about 457.090 second left to finish this epoch\n",
            "batch loss : 0.001086442 -> about 442.930 second left to finish this epoch\n",
            "batch loss : 0.00052162027 -> about 428.663 second left to finish this epoch\n",
            "batch loss : 0.002605385 -> about 414.454 second left to finish this epoch\n",
            "batch loss : 0.0014233377 -> about 400.183 second left to finish this epoch\n",
            "batch loss : 0.0007399479 -> about 386.006 second left to finish this epoch\n",
            "batch loss : 0.001074669 -> about 371.797 second left to finish this epoch\n",
            "batch loss : 0.00486088 -> about 357.632 second left to finish this epoch\n",
            "batch loss : 0.0005342122 -> about 343.422 second left to finish this epoch\n",
            "batch loss : 0.001927066 -> about 329.221 second left to finish this epoch\n",
            "batch loss : 0.0018193163 -> about 315.085 second left to finish this epoch\n",
            "batch loss : 0.0005938518 -> about 300.889 second left to finish this epoch\n",
            "batch loss : 0.00080252765 -> about 286.697 second left to finish this epoch\n",
            "batch loss : 0.0020592858 -> about 272.463 second left to finish this epoch\n",
            "batch loss : 0.0006078076 -> about 258.258 second left to finish this epoch\n",
            "batch loss : 0.00064023415 -> about 244.066 second left to finish this epoch\n",
            "batch loss : 0.0025949818 -> about 229.896 second left to finish this epoch\n",
            "batch loss : 0.00047882803 -> about 215.746 second left to finish this epoch\n",
            "batch loss : 0.0018533396 -> about 201.592 second left to finish this epoch\n",
            "batch loss : 0.0009359109 -> about 187.403 second left to finish this epoch\n",
            "batch loss : 0.0013214669 -> about 173.220 second left to finish this epoch\n",
            "batch loss : 0.0010213461 -> about 159.021 second left to finish this epoch\n",
            "batch loss : 0.0023037046 -> about 144.830 second left to finish this epoch\n",
            "batch loss : 0.0012529034 -> about 130.639 second left to finish this epoch\n",
            "batch loss : 0.0013555423 -> about 116.448 second left to finish this epoch\n",
            "batch loss : 0.00058888516 -> about 102.267 second left to finish this epoch\n",
            "batch loss : 0.0026021153 -> about 88.077 second left to finish this epoch\n",
            "batch loss : 0.0019260648 -> about 73.889 second left to finish this epoch\n",
            "batch loss : 0.00040717985 -> about 59.705 second left to finish this epoch\n",
            "batch loss : 0.0009178346 -> about 45.525 second left to finish this epoch\n",
            "batch loss : 0.0022012945 -> about 31.340 second left to finish this epoch\n",
            "batch loss : 0.0033196947 -> about 17.159 second left to finish this epoch\n",
            "batch loss : 0.000765115 -> about 2.978 second left to finish this epoch\n",
            "current epoch : 18 , current train loss : 0.0022592294475220543\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0007058027 -> about 699.795 second left to finish this epoch\n",
            "batch loss : 0.0018670945 -> about 671.287 second left to finish this epoch\n",
            "batch loss : 0.004475468 -> about 655.777 second left to finish this epoch\n",
            "batch loss : 0.0048542772 -> about 641.340 second left to finish this epoch\n",
            "batch loss : 0.0024475558 -> about 627.075 second left to finish this epoch\n",
            "batch loss : 0.00077078387 -> about 613.276 second left to finish this epoch\n",
            "batch loss : 0.0004511143 -> about 599.000 second left to finish this epoch\n",
            "batch loss : 0.005942168 -> about 584.747 second left to finish this epoch\n",
            "batch loss : 0.00256181 -> about 570.597 second left to finish this epoch\n",
            "batch loss : 0.0009806394 -> about 556.164 second left to finish this epoch\n",
            "batch loss : 0.0014225121 -> about 541.886 second left to finish this epoch\n",
            "batch loss : 0.0011106951 -> about 527.708 second left to finish this epoch\n",
            "batch loss : 0.002055663 -> about 513.340 second left to finish this epoch\n",
            "batch loss : 0.0006762901 -> about 499.140 second left to finish this epoch\n",
            "batch loss : 0.00077610795 -> about 484.938 second left to finish this epoch\n",
            "batch loss : 0.001179781 -> about 470.689 second left to finish this epoch\n",
            "batch loss : 0.0023251595 -> about 456.422 second left to finish this epoch\n",
            "batch loss : 0.001212721 -> about 442.226 second left to finish this epoch\n",
            "batch loss : 0.0022284992 -> about 427.964 second left to finish this epoch\n",
            "batch loss : 0.0017555051 -> about 413.867 second left to finish this epoch\n",
            "batch loss : 0.0007827905 -> about 399.662 second left to finish this epoch\n",
            "batch loss : 0.0006186677 -> about 385.476 second left to finish this epoch\n",
            "batch loss : 0.0005116608 -> about 371.470 second left to finish this epoch\n",
            "batch loss : 0.00063465495 -> about 357.262 second left to finish this epoch\n",
            "batch loss : 0.0016537043 -> about 343.090 second left to finish this epoch\n",
            "batch loss : 0.004228133 -> about 328.890 second left to finish this epoch\n",
            "batch loss : 0.0018827565 -> about 314.842 second left to finish this epoch\n",
            "batch loss : 0.00044091805 -> about 300.662 second left to finish this epoch\n",
            "batch loss : 0.0015992583 -> about 286.500 second left to finish this epoch\n",
            "batch loss : 0.001113309 -> about 272.305 second left to finish this epoch\n",
            "batch loss : 0.0010053042 -> about 258.117 second left to finish this epoch\n",
            "batch loss : 0.00042811423 -> about 243.953 second left to finish this epoch\n",
            "batch loss : 0.0038196463 -> about 229.766 second left to finish this epoch\n",
            "batch loss : 0.0013193204 -> about 215.568 second left to finish this epoch\n",
            "batch loss : 0.0013721762 -> about 201.371 second left to finish this epoch\n",
            "batch loss : 0.00090819894 -> about 187.200 second left to finish this epoch\n",
            "batch loss : 0.0015051147 -> about 173.032 second left to finish this epoch\n",
            "batch loss : 0.0014276502 -> about 158.856 second left to finish this epoch\n",
            "batch loss : 0.0009415152 -> about 144.681 second left to finish this epoch\n",
            "batch loss : 0.0025062105 -> about 130.504 second left to finish this epoch\n",
            "batch loss : 0.0024643748 -> about 116.324 second left to finish this epoch\n",
            "batch loss : 0.0006017627 -> about 102.156 second left to finish this epoch\n",
            "batch loss : 0.00056843995 -> about 87.985 second left to finish this epoch\n",
            "batch loss : 0.003880674 -> about 73.820 second left to finish this epoch\n",
            "batch loss : 0.00062677526 -> about 59.663 second left to finish this epoch\n",
            "batch loss : 0.0040086596 -> about 45.489 second left to finish this epoch\n",
            "batch loss : 0.0031871446 -> about 31.317 second left to finish this epoch\n",
            "batch loss : 0.0006611916 -> about 17.149 second left to finish this epoch\n",
            "batch loss : 0.002441537 -> about 2.976 second left to finish this epoch\n",
            "current epoch : 19 , current train loss : 0.002006924105396423\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.004533147 -> about 694.751 second left to finish this epoch\n",
            "batch loss : 0.0015811297 -> about 669.459 second left to finish this epoch\n",
            "batch loss : 0.0011295555 -> about 653.699 second left to finish this epoch\n",
            "batch loss : 0.00034109963 -> about 639.813 second left to finish this epoch\n",
            "batch loss : 0.0007705672 -> about 629.953 second left to finish this epoch\n",
            "batch loss : 0.0005796978 -> about 616.870 second left to finish this epoch\n",
            "batch loss : 0.002252144 -> about 602.644 second left to finish this epoch\n",
            "batch loss : 0.0005105523 -> about 587.630 second left to finish this epoch\n",
            "batch loss : 0.002916344 -> about 572.851 second left to finish this epoch\n",
            "batch loss : 0.0009127158 -> about 558.214 second left to finish this epoch\n",
            "batch loss : 0.001816448 -> about 543.551 second left to finish this epoch\n",
            "batch loss : 0.0027328345 -> about 529.336 second left to finish this epoch\n",
            "batch loss : 0.0029304603 -> about 514.913 second left to finish this epoch\n",
            "batch loss : 0.0042239656 -> about 500.429 second left to finish this epoch\n",
            "batch loss : 0.0028080223 -> about 486.034 second left to finish this epoch\n",
            "batch loss : 0.0037750548 -> about 471.675 second left to finish this epoch\n",
            "batch loss : 0.0016236779 -> about 457.461 second left to finish this epoch\n",
            "batch loss : 0.004335656 -> about 443.319 second left to finish this epoch\n",
            "batch loss : 0.0021770303 -> about 429.079 second left to finish this epoch\n",
            "batch loss : 0.0019819983 -> about 414.910 second left to finish this epoch\n",
            "batch loss : 0.0036983592 -> about 400.799 second left to finish this epoch\n",
            "batch loss : 0.002796076 -> about 386.534 second left to finish this epoch\n",
            "batch loss : 0.0005598942 -> about 372.309 second left to finish this epoch\n",
            "batch loss : 0.003168096 -> about 358.085 second left to finish this epoch\n",
            "batch loss : 0.0010207731 -> about 343.811 second left to finish this epoch\n",
            "batch loss : 0.0009568726 -> about 329.525 second left to finish this epoch\n",
            "batch loss : 0.006513595 -> about 315.288 second left to finish this epoch\n",
            "batch loss : 0.00086455164 -> about 301.067 second left to finish this epoch\n",
            "batch loss : 0.0015227059 -> about 286.880 second left to finish this epoch\n",
            "batch loss : 0.0005698637 -> about 272.661 second left to finish this epoch\n",
            "batch loss : 0.0013221863 -> about 258.451 second left to finish this epoch\n",
            "batch loss : 0.003508404 -> about 244.217 second left to finish this epoch\n",
            "batch loss : 0.0009454801 -> about 230.004 second left to finish this epoch\n",
            "batch loss : 0.00061141915 -> about 215.806 second left to finish this epoch\n",
            "batch loss : 0.0014472131 -> about 201.612 second left to finish this epoch\n",
            "batch loss : 0.0011653672 -> about 187.417 second left to finish this epoch\n",
            "batch loss : 0.0013226604 -> about 173.206 second left to finish this epoch\n",
            "batch loss : 0.0015124013 -> about 159.016 second left to finish this epoch\n",
            "batch loss : 0.0015852908 -> about 144.817 second left to finish this epoch\n",
            "batch loss : 0.0017502399 -> about 130.661 second left to finish this epoch\n",
            "batch loss : 0.002148301 -> about 116.496 second left to finish this epoch\n",
            "batch loss : 0.00045337158 -> about 102.307 second left to finish this epoch\n",
            "batch loss : 0.0016615357 -> about 88.121 second left to finish this epoch\n",
            "batch loss : 0.0007202135 -> about 73.926 second left to finish this epoch\n",
            "batch loss : 0.00051348883 -> about 59.733 second left to finish this epoch\n",
            "batch loss : 0.0005336822 -> about 45.544 second left to finish this epoch\n",
            "batch loss : 0.004174169 -> about 31.356 second left to finish this epoch\n",
            "batch loss : 0.0023025346 -> about 17.166 second left to finish this epoch\n",
            "batch loss : 0.0016920392 -> about 2.979 second left to finish this epoch\n",
            "current epoch : 20 , current train loss : 0.0017832663189296203\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0026377174 -> about 689.104 second left to finish this epoch\n",
            "batch loss : 0.00085384015 -> about 667.394 second left to finish this epoch\n",
            "batch loss : 0.0011812656 -> about 654.426 second left to finish this epoch\n",
            "batch loss : 0.00038757687 -> about 640.623 second left to finish this epoch\n",
            "batch loss : 0.0037075668 -> about 626.212 second left to finish this epoch\n",
            "batch loss : 0.0007312562 -> about 611.853 second left to finish this epoch\n",
            "batch loss : 0.004862097 -> about 598.037 second left to finish this epoch\n",
            "batch loss : 0.0008996965 -> about 583.818 second left to finish this epoch\n",
            "batch loss : 0.0004628754 -> about 569.465 second left to finish this epoch\n",
            "batch loss : 0.0017053784 -> about 555.342 second left to finish this epoch\n",
            "batch loss : 0.0010430916 -> about 540.971 second left to finish this epoch\n",
            "batch loss : 0.0019475607 -> about 526.915 second left to finish this epoch\n",
            "batch loss : 0.0010151657 -> about 512.889 second left to finish this epoch\n",
            "batch loss : 0.004304457 -> about 499.099 second left to finish this epoch\n",
            "batch loss : 0.00095326686 -> about 485.042 second left to finish this epoch\n",
            "batch loss : 0.00096331816 -> about 470.794 second left to finish this epoch\n",
            "batch loss : 0.0006119923 -> about 456.569 second left to finish this epoch\n",
            "batch loss : 0.0010216279 -> about 442.352 second left to finish this epoch\n",
            "batch loss : 0.0005890036 -> about 428.140 second left to finish this epoch\n",
            "batch loss : 0.004833361 -> about 413.946 second left to finish this epoch\n",
            "batch loss : 0.0018083074 -> about 399.741 second left to finish this epoch\n",
            "batch loss : 0.00088349567 -> about 385.554 second left to finish this epoch\n",
            "batch loss : 0.0053318935 -> about 371.324 second left to finish this epoch\n",
            "batch loss : 0.002135132 -> about 357.155 second left to finish this epoch\n",
            "batch loss : 0.0011681819 -> about 342.960 second left to finish this epoch\n",
            "batch loss : 0.0018652044 -> about 328.857 second left to finish this epoch\n",
            "batch loss : 0.002680212 -> about 314.672 second left to finish this epoch\n",
            "batch loss : 0.0016989463 -> about 300.481 second left to finish this epoch\n",
            "batch loss : 0.001664002 -> about 286.323 second left to finish this epoch\n",
            "batch loss : 0.00091517786 -> about 272.142 second left to finish this epoch\n",
            "batch loss : 0.0009427417 -> about 258.005 second left to finish this epoch\n",
            "batch loss : 0.0036905988 -> about 243.836 second left to finish this epoch\n",
            "batch loss : 0.0006466872 -> about 229.648 second left to finish this epoch\n",
            "batch loss : 0.00075768196 -> about 215.517 second left to finish this epoch\n",
            "batch loss : 0.0010856434 -> about 201.368 second left to finish this epoch\n",
            "batch loss : 0.0019652843 -> about 187.242 second left to finish this epoch\n",
            "batch loss : 0.00266387 -> about 173.057 second left to finish this epoch\n",
            "batch loss : 0.0014990491 -> about 158.870 second left to finish this epoch\n",
            "batch loss : 0.0013402421 -> about 144.680 second left to finish this epoch\n",
            "batch loss : 0.0023717354 -> about 130.513 second left to finish this epoch\n",
            "batch loss : 0.0017671336 -> about 116.371 second left to finish this epoch\n",
            "batch loss : 0.0023975228 -> about 102.226 second left to finish this epoch\n",
            "batch loss : 0.0024409904 -> about 88.108 second left to finish this epoch\n",
            "batch loss : 0.0012211576 -> about 73.956 second left to finish this epoch\n",
            "batch loss : 0.0016200645 -> about 59.794 second left to finish this epoch\n",
            "batch loss : 0.0024438268 -> about 45.606 second left to finish this epoch\n",
            "batch loss : 0.0004242115 -> about 31.408 second left to finish this epoch\n",
            "batch loss : 0.0010379524 -> about 17.200 second left to finish this epoch\n",
            "batch loss : 0.0010993644 -> about 2.986 second left to finish this epoch\n",
            "current epoch : 21 , current train loss : 0.0015946780488572977\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0009947519 -> about 699.730 second left to finish this epoch\n",
            "batch loss : 0.004817549 -> about 679.113 second left to finish this epoch\n",
            "batch loss : 0.0022283646 -> about 662.982 second left to finish this epoch\n",
            "batch loss : 0.0004318494 -> about 647.590 second left to finish this epoch\n",
            "batch loss : 0.0020232932 -> about 633.098 second left to finish this epoch\n",
            "batch loss : 0.0030961134 -> about 619.352 second left to finish this epoch\n",
            "batch loss : 0.00083484926 -> about 605.093 second left to finish this epoch\n",
            "batch loss : 0.00057882385 -> about 590.333 second left to finish this epoch\n",
            "batch loss : 0.0015668877 -> about 577.100 second left to finish this epoch\n",
            "batch loss : 0.003371761 -> about 562.478 second left to finish this epoch\n",
            "batch loss : 0.00045494473 -> about 548.181 second left to finish this epoch\n",
            "batch loss : 0.0007554028 -> about 533.353 second left to finish this epoch\n",
            "batch loss : 0.0017119966 -> about 518.553 second left to finish this epoch\n",
            "batch loss : 0.0008362481 -> about 503.746 second left to finish this epoch\n",
            "batch loss : 0.0012771895 -> about 489.220 second left to finish this epoch\n",
            "batch loss : 0.0019119873 -> about 474.704 second left to finish this epoch\n",
            "batch loss : 0.0013538497 -> about 460.324 second left to finish this epoch\n",
            "batch loss : 0.004925189 -> about 445.921 second left to finish this epoch\n",
            "batch loss : 0.0017551817 -> about 431.516 second left to finish this epoch\n",
            "batch loss : 0.0012177073 -> about 417.164 second left to finish this epoch\n",
            "batch loss : 0.0010251122 -> about 402.777 second left to finish this epoch\n",
            "batch loss : 0.00045831228 -> about 388.472 second left to finish this epoch\n",
            "batch loss : 0.002684133 -> about 374.103 second left to finish this epoch\n",
            "batch loss : 0.001348147 -> about 359.760 second left to finish this epoch\n",
            "batch loss : 0.0002894168 -> about 345.421 second left to finish this epoch\n",
            "batch loss : 0.0011262577 -> about 331.187 second left to finish this epoch\n",
            "batch loss : 0.00036014157 -> about 316.890 second left to finish this epoch\n",
            "batch loss : 0.00057151145 -> about 302.614 second left to finish this epoch\n",
            "batch loss : 0.001867977 -> about 288.325 second left to finish this epoch\n",
            "batch loss : 0.0008823952 -> about 273.998 second left to finish this epoch\n",
            "batch loss : 0.0006506484 -> about 259.793 second left to finish this epoch\n",
            "batch loss : 0.00057774456 -> about 245.514 second left to finish this epoch\n",
            "batch loss : 0.004171869 -> about 231.242 second left to finish this epoch\n",
            "batch loss : 0.00042125565 -> about 216.950 second left to finish this epoch\n",
            "batch loss : 0.0035990896 -> about 202.684 second left to finish this epoch\n",
            "batch loss : 0.0012090621 -> about 188.398 second left to finish this epoch\n",
            "batch loss : 0.0005722797 -> about 174.109 second left to finish this epoch\n",
            "batch loss : 0.0028597903 -> about 159.837 second left to finish this epoch\n",
            "batch loss : 0.0010097052 -> about 145.566 second left to finish this epoch\n",
            "batch loss : 0.0034644406 -> about 131.301 second left to finish this epoch\n",
            "batch loss : 0.00035327516 -> about 117.021 second left to finish this epoch\n",
            "batch loss : 0.0011336971 -> about 102.756 second left to finish this epoch\n",
            "batch loss : 0.0018942433 -> about 88.490 second left to finish this epoch\n",
            "batch loss : 0.00091053 -> about 74.232 second left to finish this epoch\n",
            "batch loss : 0.0032085287 -> about 59.980 second left to finish this epoch\n",
            "batch loss : 0.00076755043 -> about 45.723 second left to finish this epoch\n",
            "batch loss : 0.0017695302 -> about 31.473 second left to finish this epoch\n",
            "batch loss : 0.0007969322 -> about 17.231 second left to finish this epoch\n",
            "batch loss : 0.00094648055 -> about 2.990 second left to finish this epoch\n",
            "current epoch : 22 , current train loss : 0.0014153865147511452\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0017127051 -> about 690.421 second left to finish this epoch\n",
            "batch loss : 0.0016802023 -> about 671.648 second left to finish this epoch\n",
            "batch loss : 0.0010317157 -> about 657.740 second left to finish this epoch\n",
            "batch loss : 0.0011555334 -> about 644.353 second left to finish this epoch\n",
            "batch loss : 0.004157211 -> about 630.358 second left to finish this epoch\n",
            "batch loss : 0.0018290502 -> about 615.173 second left to finish this epoch\n",
            "batch loss : 0.00048181938 -> about 600.227 second left to finish this epoch\n",
            "batch loss : 0.0007521043 -> about 585.640 second left to finish this epoch\n",
            "batch loss : 0.0005849445 -> about 571.140 second left to finish this epoch\n",
            "batch loss : 0.00075284555 -> about 556.821 second left to finish this epoch\n",
            "batch loss : 0.002179079 -> about 542.372 second left to finish this epoch\n",
            "batch loss : 0.0005004073 -> about 528.205 second left to finish this epoch\n",
            "batch loss : 0.0007876688 -> about 513.834 second left to finish this epoch\n",
            "batch loss : 0.0013641923 -> about 499.503 second left to finish this epoch\n",
            "batch loss : 0.0011115711 -> about 485.265 second left to finish this epoch\n",
            "batch loss : 0.001035158 -> about 470.996 second left to finish this epoch\n",
            "batch loss : 0.0009309016 -> about 456.790 second left to finish this epoch\n",
            "batch loss : 0.0012416992 -> about 442.517 second left to finish this epoch\n",
            "batch loss : 0.0011098975 -> about 428.247 second left to finish this epoch\n",
            "batch loss : 0.0005803783 -> about 414.092 second left to finish this epoch\n",
            "batch loss : 0.000717223 -> about 399.862 second left to finish this epoch\n",
            "batch loss : 0.000412339 -> about 385.633 second left to finish this epoch\n",
            "batch loss : 0.001548114 -> about 371.404 second left to finish this epoch\n",
            "batch loss : 0.0007547033 -> about 357.220 second left to finish this epoch\n",
            "batch loss : 0.0025629378 -> about 343.057 second left to finish this epoch\n",
            "batch loss : 0.0029755172 -> about 328.962 second left to finish this epoch\n",
            "batch loss : 0.0009780194 -> about 314.867 second left to finish this epoch\n",
            "batch loss : 0.00035325278 -> about 300.650 second left to finish this epoch\n",
            "batch loss : 0.001515944 -> about 286.455 second left to finish this epoch\n",
            "batch loss : 0.0007784853 -> about 272.248 second left to finish this epoch\n",
            "batch loss : 0.007188511 -> about 258.069 second left to finish this epoch\n",
            "batch loss : 0.00037469954 -> about 243.880 second left to finish this epoch\n",
            "batch loss : 0.0005113097 -> about 229.703 second left to finish this epoch\n",
            "batch loss : 0.001028893 -> about 215.526 second left to finish this epoch\n",
            "batch loss : 0.00062328787 -> about 201.339 second left to finish this epoch\n",
            "batch loss : 0.0009202234 -> about 187.164 second left to finish this epoch\n",
            "batch loss : 0.0035441439 -> about 172.996 second left to finish this epoch\n",
            "batch loss : 0.0029051516 -> about 158.827 second left to finish this epoch\n",
            "batch loss : 0.0034056199 -> about 144.656 second left to finish this epoch\n",
            "batch loss : 0.00061157055 -> about 130.482 second left to finish this epoch\n",
            "batch loss : 0.00044068228 -> about 116.306 second left to finish this epoch\n",
            "batch loss : 0.0008914408 -> about 102.138 second left to finish this epoch\n",
            "batch loss : 0.00085819536 -> about 87.972 second left to finish this epoch\n",
            "batch loss : 0.00030721794 -> about 73.802 second left to finish this epoch\n",
            "batch loss : 0.0005907436 -> about 59.634 second left to finish this epoch\n",
            "batch loss : 0.0007656706 -> about 45.469 second left to finish this epoch\n",
            "batch loss : 0.0003988329 -> about 31.304 second left to finish this epoch\n",
            "batch loss : 0.00055343925 -> about 17.142 second left to finish this epoch\n",
            "batch loss : 0.0008729557 -> about 2.975 second left to finish this epoch\n",
            "current epoch : 23 , current train loss : 0.0012900602483275402\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.00038924202 -> about 692.089 second left to finish this epoch\n",
            "batch loss : 0.000411266 -> about 668.385 second left to finish this epoch\n",
            "batch loss : 0.0007030966 -> about 653.515 second left to finish this epoch\n",
            "batch loss : 0.00049848063 -> about 638.960 second left to finish this epoch\n",
            "batch loss : 0.0008648559 -> about 625.414 second left to finish this epoch\n",
            "batch loss : 0.0007246292 -> about 611.456 second left to finish this epoch\n",
            "batch loss : 0.0007347547 -> about 597.294 second left to finish this epoch\n",
            "batch loss : 0.00058698235 -> about 582.958 second left to finish this epoch\n",
            "batch loss : 0.0035285617 -> about 568.607 second left to finish this epoch\n",
            "batch loss : 0.005215438 -> about 554.630 second left to finish this epoch\n",
            "batch loss : 0.0011210861 -> about 540.583 second left to finish this epoch\n",
            "batch loss : 0.00045209506 -> about 526.438 second left to finish this epoch\n",
            "batch loss : 0.0005668956 -> about 512.216 second left to finish this epoch\n",
            "batch loss : 0.0005260396 -> about 498.024 second left to finish this epoch\n",
            "batch loss : 0.0006790693 -> about 483.825 second left to finish this epoch\n",
            "batch loss : 0.00053829537 -> about 469.704 second left to finish this epoch\n",
            "batch loss : 0.003179724 -> about 455.607 second left to finish this epoch\n",
            "batch loss : 0.0005947215 -> about 441.469 second left to finish this epoch\n",
            "batch loss : 0.0018682497 -> about 427.328 second left to finish this epoch\n",
            "batch loss : 0.0009032803 -> about 413.162 second left to finish this epoch\n",
            "batch loss : 0.0034148719 -> about 399.032 second left to finish this epoch\n",
            "batch loss : 0.0020508866 -> about 385.173 second left to finish this epoch\n",
            "batch loss : 0.0010078877 -> about 371.085 second left to finish this epoch\n",
            "batch loss : 0.0005039731 -> about 356.910 second left to finish this epoch\n",
            "batch loss : 0.0010147076 -> about 342.749 second left to finish this epoch\n",
            "batch loss : 0.00044902926 -> about 328.596 second left to finish this epoch\n",
            "batch loss : 0.0015300177 -> about 314.424 second left to finish this epoch\n",
            "batch loss : 0.00071876403 -> about 300.266 second left to finish this epoch\n",
            "batch loss : 0.0006343048 -> about 286.107 second left to finish this epoch\n",
            "batch loss : 0.0009694677 -> about 271.950 second left to finish this epoch\n",
            "batch loss : 0.0004481878 -> about 257.788 second left to finish this epoch\n",
            "batch loss : 0.00046607174 -> about 243.621 second left to finish this epoch\n",
            "batch loss : 0.0017702561 -> about 229.458 second left to finish this epoch\n",
            "batch loss : 0.00052167044 -> about 215.300 second left to finish this epoch\n",
            "batch loss : 0.00079873815 -> about 201.138 second left to finish this epoch\n",
            "batch loss : 0.0011563702 -> about 186.978 second left to finish this epoch\n",
            "batch loss : 0.00072467374 -> about 172.811 second left to finish this epoch\n",
            "batch loss : 0.00034951937 -> about 158.657 second left to finish this epoch\n",
            "batch loss : 0.0003562963 -> about 144.500 second left to finish this epoch\n",
            "batch loss : 0.0011495075 -> about 130.351 second left to finish this epoch\n",
            "batch loss : 0.00053851894 -> about 116.197 second left to finish this epoch\n",
            "batch loss : 0.00047868566 -> about 102.046 second left to finish this epoch\n",
            "batch loss : 0.0018335854 -> about 87.902 second left to finish this epoch\n",
            "batch loss : 0.0006116861 -> about 73.765 second left to finish this epoch\n",
            "batch loss : 0.00041658763 -> about 59.607 second left to finish this epoch\n",
            "batch loss : 0.0020562457 -> about 45.449 second left to finish this epoch\n",
            "batch loss : 0.0046965657 -> about 31.290 second left to finish this epoch\n",
            "batch loss : 0.0036504185 -> about 17.134 second left to finish this epoch\n",
            "batch loss : 0.00065377477 -> about 2.974 second left to finish this epoch\n",
            "current epoch : 24 , current train loss : 0.0011499667993786155\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0008302321 -> about 686.966 second left to finish this epoch\n",
            "batch loss : 0.0007606614 -> about 671.168 second left to finish this epoch\n",
            "batch loss : 0.002971845 -> about 656.171 second left to finish this epoch\n",
            "batch loss : 0.0004035352 -> about 641.262 second left to finish this epoch\n",
            "batch loss : 0.0021323976 -> about 626.851 second left to finish this epoch\n",
            "batch loss : 0.00053437264 -> about 612.605 second left to finish this epoch\n",
            "batch loss : 0.0002963125 -> about 598.212 second left to finish this epoch\n",
            "batch loss : 0.00050798035 -> about 584.116 second left to finish this epoch\n",
            "batch loss : 0.0008438391 -> about 569.998 second left to finish this epoch\n",
            "batch loss : 0.00046804373 -> about 555.814 second left to finish this epoch\n",
            "batch loss : 0.0005844827 -> about 541.558 second left to finish this epoch\n",
            "batch loss : 0.0028535936 -> about 527.368 second left to finish this epoch\n",
            "batch loss : 0.00082900026 -> about 513.203 second left to finish this epoch\n",
            "batch loss : 0.00042801176 -> about 499.045 second left to finish this epoch\n",
            "batch loss : 0.0028625529 -> about 484.916 second left to finish this epoch\n",
            "batch loss : 0.0018600922 -> about 470.762 second left to finish this epoch\n",
            "batch loss : 0.000935562 -> about 456.826 second left to finish this epoch\n",
            "batch loss : 0.0010751445 -> about 442.857 second left to finish this epoch\n",
            "batch loss : 0.0014779376 -> about 428.812 second left to finish this epoch\n",
            "batch loss : 0.0012843332 -> about 414.634 second left to finish this epoch\n",
            "batch loss : 0.0003439056 -> about 400.404 second left to finish this epoch\n",
            "batch loss : 0.0011227324 -> about 386.155 second left to finish this epoch\n",
            "batch loss : 0.000498225 -> about 371.884 second left to finish this epoch\n",
            "batch loss : 0.0012612022 -> about 357.670 second left to finish this epoch\n",
            "batch loss : 0.00091480353 -> about 343.467 second left to finish this epoch\n",
            "batch loss : 0.0004756087 -> about 329.291 second left to finish this epoch\n",
            "batch loss : 0.00042331382 -> about 315.078 second left to finish this epoch\n",
            "batch loss : 0.0013399892 -> about 300.869 second left to finish this epoch\n",
            "batch loss : 0.0004175825 -> about 286.637 second left to finish this epoch\n",
            "batch loss : 0.0009496611 -> about 272.439 second left to finish this epoch\n",
            "batch loss : 0.0022712485 -> about 258.247 second left to finish this epoch\n",
            "batch loss : 0.0018645072 -> about 244.056 second left to finish this epoch\n",
            "batch loss : 0.0003934137 -> about 229.849 second left to finish this epoch\n",
            "batch loss : 0.0003523955 -> about 215.655 second left to finish this epoch\n",
            "batch loss : 0.00091526716 -> about 201.467 second left to finish this epoch\n",
            "batch loss : 0.00095083984 -> about 187.285 second left to finish this epoch\n",
            "batch loss : 0.000603126 -> about 173.105 second left to finish this epoch\n",
            "batch loss : 0.0033365374 -> about 158.924 second left to finish this epoch\n",
            "batch loss : 0.000681132 -> about 144.756 second left to finish this epoch\n",
            "batch loss : 0.001331704 -> about 130.583 second left to finish this epoch\n",
            "batch loss : 0.0012530251 -> about 116.397 second left to finish this epoch\n",
            "batch loss : 0.0006468246 -> about 102.218 second left to finish this epoch\n",
            "batch loss : 0.00054939353 -> about 88.043 second left to finish this epoch\n",
            "batch loss : 0.0005496319 -> about 73.863 second left to finish this epoch\n",
            "batch loss : 0.0006793532 -> about 59.682 second left to finish this epoch\n",
            "batch loss : 0.0006776261 -> about 45.503 second left to finish this epoch\n",
            "batch loss : 0.00046451145 -> about 31.325 second left to finish this epoch\n",
            "batch loss : 0.0008925793 -> about 17.151 second left to finish this epoch\n",
            "batch loss : 0.0023493906 -> about 2.977 second left to finish this epoch\n",
            "current epoch : 25 , current train loss : 0.0010422730877377266\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.00047273576 -> about 696.355 second left to finish this epoch\n",
            "batch loss : 0.0006863325 -> about 669.946 second left to finish this epoch\n",
            "batch loss : 0.0008381669 -> about 654.734 second left to finish this epoch\n",
            "batch loss : 0.0005042804 -> about 640.157 second left to finish this epoch\n",
            "batch loss : 0.00078147673 -> about 625.953 second left to finish this epoch\n",
            "batch loss : 0.0015420488 -> about 611.827 second left to finish this epoch\n",
            "batch loss : 0.0009224664 -> about 597.518 second left to finish this epoch\n",
            "batch loss : 0.0011836819 -> about 583.179 second left to finish this epoch\n",
            "batch loss : 0.0005992231 -> about 568.984 second left to finish this epoch\n",
            "batch loss : 0.0042821406 -> about 554.837 second left to finish this epoch\n",
            "batch loss : 0.001085133 -> about 541.008 second left to finish this epoch\n",
            "batch loss : 0.0003306641 -> about 526.782 second left to finish this epoch\n",
            "batch loss : 0.00096287206 -> about 512.971 second left to finish this epoch\n",
            "batch loss : 0.00045768765 -> about 498.734 second left to finish this epoch\n",
            "batch loss : 0.0016557832 -> about 484.523 second left to finish this epoch\n",
            "batch loss : 0.0013001538 -> about 470.325 second left to finish this epoch\n",
            "batch loss : 0.0006330252 -> about 456.128 second left to finish this epoch\n",
            "batch loss : 0.0006121001 -> about 441.950 second left to finish this epoch\n",
            "batch loss : 0.00038029987 -> about 427.767 second left to finish this epoch\n",
            "batch loss : 0.0006326807 -> about 413.552 second left to finish this epoch\n",
            "batch loss : 0.0014311132 -> about 399.337 second left to finish this epoch\n",
            "batch loss : 0.00057514425 -> about 385.165 second left to finish this epoch\n",
            "batch loss : 0.00086990313 -> about 370.986 second left to finish this epoch\n",
            "batch loss : 0.0033333937 -> about 356.855 second left to finish this epoch\n",
            "batch loss : 0.00073980866 -> about 342.668 second left to finish this epoch\n",
            "batch loss : 0.0005738709 -> about 328.522 second left to finish this epoch\n",
            "batch loss : 0.0017504228 -> about 314.350 second left to finish this epoch\n",
            "batch loss : 0.0019793145 -> about 300.218 second left to finish this epoch\n",
            "batch loss : 0.0005708061 -> about 286.092 second left to finish this epoch\n",
            "batch loss : 0.0049803746 -> about 271.898 second left to finish this epoch\n",
            "batch loss : 0.00050047674 -> about 257.744 second left to finish this epoch\n",
            "batch loss : 0.00143975 -> about 243.572 second left to finish this epoch\n",
            "batch loss : 0.0005259542 -> about 229.434 second left to finish this epoch\n",
            "batch loss : 0.00057095895 -> about 215.283 second left to finish this epoch\n",
            "batch loss : 0.000696335 -> about 201.198 second left to finish this epoch\n",
            "batch loss : 0.00094864593 -> about 187.035 second left to finish this epoch\n",
            "batch loss : 0.00053502736 -> about 172.876 second left to finish this epoch\n",
            "batch loss : 0.00046491495 -> about 158.709 second left to finish this epoch\n",
            "batch loss : 0.0006301693 -> about 144.545 second left to finish this epoch\n",
            "batch loss : 0.0006735842 -> about 130.393 second left to finish this epoch\n",
            "batch loss : 0.0021058451 -> about 116.227 second left to finish this epoch\n",
            "batch loss : 0.00025543937 -> about 102.070 second left to finish this epoch\n",
            "batch loss : 0.0006061918 -> about 87.911 second left to finish this epoch\n",
            "batch loss : 0.0009092076 -> about 73.751 second left to finish this epoch\n",
            "batch loss : 0.001117684 -> about 59.597 second left to finish this epoch\n",
            "batch loss : 0.00031895662 -> about 45.443 second left to finish this epoch\n",
            "batch loss : 0.0006696298 -> about 31.285 second left to finish this epoch\n",
            "batch loss : 0.0008476635 -> about 17.129 second left to finish this epoch\n",
            "batch loss : 0.0011142106 -> about 2.973 second left to finish this epoch\n",
            "current epoch : 26 , current train loss : 0.00094912205325266\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.00047229696 -> about 689.207 second left to finish this epoch\n",
            "batch loss : 0.00041188428 -> about 668.452 second left to finish this epoch\n",
            "batch loss : 0.00040310487 -> about 654.669 second left to finish this epoch\n",
            "batch loss : 0.00023472888 -> about 640.434 second left to finish this epoch\n",
            "batch loss : 0.0024204343 -> about 626.220 second left to finish this epoch\n",
            "batch loss : 0.003181411 -> about 612.408 second left to finish this epoch\n",
            "batch loss : 0.0003280256 -> about 598.071 second left to finish this epoch\n",
            "batch loss : 0.0008133848 -> about 584.159 second left to finish this epoch\n",
            "batch loss : 0.00042173365 -> about 570.520 second left to finish this epoch\n",
            "batch loss : 0.00039402593 -> about 556.131 second left to finish this epoch\n",
            "batch loss : 0.0006267241 -> about 541.899 second left to finish this epoch\n",
            "batch loss : 0.00068679557 -> about 527.704 second left to finish this epoch\n",
            "batch loss : 0.00043044085 -> about 513.529 second left to finish this epoch\n",
            "batch loss : 0.00082569587 -> about 499.476 second left to finish this epoch\n",
            "batch loss : 0.0009624978 -> about 485.379 second left to finish this epoch\n",
            "batch loss : 0.0016342803 -> about 471.199 second left to finish this epoch\n",
            "batch loss : 0.0013005927 -> about 456.990 second left to finish this epoch\n",
            "batch loss : 0.0020901167 -> about 442.798 second left to finish this epoch\n",
            "batch loss : 0.0009221586 -> about 428.593 second left to finish this epoch\n",
            "batch loss : 0.00060471264 -> about 414.518 second left to finish this epoch\n",
            "batch loss : 0.0005293382 -> about 400.355 second left to finish this epoch\n",
            "batch loss : 0.00050483143 -> about 386.187 second left to finish this epoch\n",
            "batch loss : 0.00071099354 -> about 372.010 second left to finish this epoch\n",
            "batch loss : 0.00033378092 -> about 357.799 second left to finish this epoch\n",
            "batch loss : 0.00057737203 -> about 343.628 second left to finish this epoch\n",
            "batch loss : 0.00024987914 -> about 329.462 second left to finish this epoch\n",
            "batch loss : 0.00041414902 -> about 315.274 second left to finish this epoch\n",
            "batch loss : 0.0014701838 -> about 301.088 second left to finish this epoch\n",
            "batch loss : 0.00025645248 -> about 286.892 second left to finish this epoch\n",
            "batch loss : 0.000663623 -> about 272.724 second left to finish this epoch\n",
            "batch loss : 0.00037998962 -> about 258.584 second left to finish this epoch\n",
            "batch loss : 0.00043164386 -> about 244.394 second left to finish this epoch\n",
            "batch loss : 0.00029540405 -> about 230.186 second left to finish this epoch\n",
            "batch loss : 0.0016135151 -> about 215.968 second left to finish this epoch\n",
            "batch loss : 0.0005436763 -> about 201.761 second left to finish this epoch\n",
            "batch loss : 0.00086314406 -> about 187.569 second left to finish this epoch\n",
            "batch loss : 0.00039739732 -> about 173.388 second left to finish this epoch\n",
            "batch loss : 0.00037713224 -> about 159.189 second left to finish this epoch\n",
            "batch loss : 0.0003677265 -> about 144.993 second left to finish this epoch\n",
            "batch loss : 0.00041007632 -> about 130.792 second left to finish this epoch\n",
            "batch loss : 0.00051051524 -> about 116.590 second left to finish this epoch\n",
            "batch loss : 0.00070299645 -> about 102.390 second left to finish this epoch\n",
            "batch loss : 0.0008157011 -> about 88.192 second left to finish this epoch\n",
            "batch loss : 0.00027306605 -> about 73.991 second left to finish this epoch\n",
            "batch loss : 0.0011286653 -> about 59.793 second left to finish this epoch\n",
            "batch loss : 0.00046063843 -> about 45.590 second left to finish this epoch\n",
            "batch loss : 0.0004515701 -> about 31.389 second left to finish this epoch\n",
            "batch loss : 0.00089683325 -> about 17.187 second left to finish this epoch\n",
            "batch loss : 0.00039415358 -> about 2.983 second left to finish this epoch\n",
            "current epoch : 27 , current train loss : 0.0008738114355433152\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.00043720176 -> about 697.324 second left to finish this epoch\n",
            "batch loss : 0.00063706125 -> about 671.801 second left to finish this epoch\n",
            "batch loss : 0.0034792277 -> about 657.071 second left to finish this epoch\n",
            "batch loss : 0.00018887174 -> about 645.151 second left to finish this epoch\n",
            "batch loss : 0.00048352763 -> about 630.094 second left to finish this epoch\n",
            "batch loss : 0.00025227017 -> about 615.475 second left to finish this epoch\n",
            "batch loss : 0.0010808918 -> about 600.912 second left to finish this epoch\n",
            "batch loss : 0.00045676535 -> about 586.309 second left to finish this epoch\n",
            "batch loss : 0.00051698484 -> about 571.877 second left to finish this epoch\n",
            "batch loss : 0.00034772843 -> about 557.516 second left to finish this epoch\n",
            "batch loss : 0.00048211613 -> about 543.216 second left to finish this epoch\n",
            "batch loss : 0.0006150076 -> about 528.966 second left to finish this epoch\n",
            "batch loss : 0.00058161677 -> about 514.642 second left to finish this epoch\n",
            "batch loss : 0.0002572816 -> about 500.427 second left to finish this epoch\n",
            "batch loss : 0.0022899495 -> about 486.141 second left to finish this epoch\n",
            "batch loss : 0.00022508665 -> about 471.886 second left to finish this epoch\n",
            "batch loss : 0.00058975146 -> about 457.679 second left to finish this epoch\n",
            "batch loss : 0.00069044705 -> about 443.409 second left to finish this epoch\n",
            "batch loss : 0.00031636245 -> about 429.153 second left to finish this epoch\n",
            "batch loss : 0.00022320019 -> about 414.877 second left to finish this epoch\n",
            "batch loss : 0.00056784187 -> about 400.647 second left to finish this epoch\n",
            "batch loss : 0.0008540313 -> about 386.475 second left to finish this epoch\n",
            "batch loss : 0.0010836858 -> about 372.261 second left to finish this epoch\n",
            "batch loss : 0.00060560764 -> about 358.045 second left to finish this epoch\n",
            "batch loss : 0.00069345086 -> about 343.858 second left to finish this epoch\n",
            "batch loss : 0.00035812586 -> about 329.768 second left to finish this epoch\n",
            "batch loss : 0.0035679676 -> about 315.565 second left to finish this epoch\n",
            "batch loss : 0.00043915765 -> about 301.369 second left to finish this epoch\n",
            "batch loss : 0.0005221638 -> about 287.145 second left to finish this epoch\n",
            "batch loss : 0.0035035408 -> about 272.918 second left to finish this epoch\n",
            "batch loss : 0.00033216423 -> about 258.690 second left to finish this epoch\n",
            "batch loss : 0.00075775397 -> about 244.493 second left to finish this epoch\n",
            "batch loss : 0.00029678788 -> about 230.281 second left to finish this epoch\n",
            "batch loss : 0.006408949 -> about 216.072 second left to finish this epoch\n",
            "batch loss : 0.00091833767 -> about 201.854 second left to finish this epoch\n",
            "batch loss : 0.000996051 -> about 187.635 second left to finish this epoch\n",
            "batch loss : 0.0015366161 -> about 173.423 second left to finish this epoch\n",
            "batch loss : 0.00043993053 -> about 159.211 second left to finish this epoch\n",
            "batch loss : 0.00032586014 -> about 145.001 second left to finish this epoch\n",
            "batch loss : 0.00075631426 -> about 130.798 second left to finish this epoch\n",
            "batch loss : 0.00045381754 -> about 116.597 second left to finish this epoch\n",
            "batch loss : 0.00039664822 -> about 102.388 second left to finish this epoch\n",
            "batch loss : 0.0004481237 -> about 88.186 second left to finish this epoch\n",
            "batch loss : 0.00031475647 -> about 73.983 second left to finish this epoch\n",
            "batch loss : 0.00027085326 -> about 59.785 second left to finish this epoch\n",
            "batch loss : 0.00073015876 -> about 45.583 second left to finish this epoch\n",
            "batch loss : 0.0010118389 -> about 31.382 second left to finish this epoch\n",
            "batch loss : 0.0005232892 -> about 17.186 second left to finish this epoch\n",
            "batch loss : 0.0002749798 -> about 2.983 second left to finish this epoch\n",
            "current epoch : 28 , current train loss : 0.0007863532555340228\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.00030001847 -> about 689.292 second left to finish this epoch\n",
            "batch loss : 0.0006976871 -> about 670.585 second left to finish this epoch\n",
            "batch loss : 0.00038455895 -> about 656.498 second left to finish this epoch\n",
            "batch loss : 0.00257055 -> about 641.876 second left to finish this epoch\n",
            "batch loss : 0.00030337667 -> about 627.010 second left to finish this epoch\n",
            "batch loss : 0.00056547066 -> about 612.848 second left to finish this epoch\n",
            "batch loss : 0.00074234523 -> about 598.616 second left to finish this epoch\n",
            "batch loss : 0.0005788431 -> about 584.573 second left to finish this epoch\n",
            "batch loss : 0.0010811861 -> about 570.464 second left to finish this epoch\n",
            "batch loss : 0.00028370414 -> about 556.266 second left to finish this epoch\n",
            "batch loss : 0.00018263803 -> about 542.024 second left to finish this epoch\n",
            "batch loss : 0.0005823057 -> about 527.813 second left to finish this epoch\n",
            "batch loss : 0.0005011909 -> about 513.624 second left to finish this epoch\n",
            "batch loss : 0.00036026194 -> about 499.539 second left to finish this epoch\n",
            "batch loss : 0.0014726992 -> about 485.362 second left to finish this epoch\n",
            "batch loss : 0.0005094255 -> about 471.228 second left to finish this epoch\n",
            "batch loss : 0.00033999668 -> about 457.106 second left to finish this epoch\n",
            "batch loss : 0.00043545727 -> about 442.996 second left to finish this epoch\n",
            "batch loss : 0.0011123836 -> about 428.890 second left to finish this epoch\n",
            "batch loss : 0.00030975183 -> about 414.738 second left to finish this epoch\n",
            "batch loss : 0.00065229624 -> about 400.675 second left to finish this epoch\n",
            "batch loss : 0.0006195528 -> about 386.564 second left to finish this epoch\n",
            "batch loss : 0.00047291844 -> about 372.356 second left to finish this epoch\n",
            "batch loss : 0.00083993457 -> about 358.144 second left to finish this epoch\n",
            "batch loss : 0.00081435515 -> about 343.942 second left to finish this epoch\n",
            "batch loss : 0.0010064064 -> about 329.759 second left to finish this epoch\n",
            "batch loss : 0.0012744475 -> about 315.556 second left to finish this epoch\n",
            "batch loss : 0.00088877964 -> about 301.385 second left to finish this epoch\n",
            "batch loss : 0.00039664243 -> about 287.177 second left to finish this epoch\n",
            "batch loss : 0.0010711533 -> about 272.966 second left to finish this epoch\n",
            "batch loss : 0.0005506835 -> about 258.784 second left to finish this epoch\n",
            "batch loss : 0.0006448711 -> about 244.578 second left to finish this epoch\n",
            "batch loss : 0.00072138594 -> about 230.361 second left to finish this epoch\n",
            "batch loss : 0.00038902904 -> about 216.151 second left to finish this epoch\n",
            "batch loss : 0.0019915118 -> about 201.958 second left to finish this epoch\n",
            "batch loss : 0.0007645728 -> about 187.751 second left to finish this epoch\n",
            "batch loss : 0.000639261 -> about 173.553 second left to finish this epoch\n",
            "batch loss : 0.0009469851 -> about 159.350 second left to finish this epoch\n",
            "batch loss : 0.00052787294 -> about 145.132 second left to finish this epoch\n",
            "batch loss : 0.0003722793 -> about 130.907 second left to finish this epoch\n",
            "batch loss : 0.0004998115 -> about 116.693 second left to finish this epoch\n",
            "batch loss : 0.0015641372 -> about 102.474 second left to finish this epoch\n",
            "batch loss : 0.00028382803 -> about 88.278 second left to finish this epoch\n",
            "batch loss : 0.0003586163 -> about 74.064 second left to finish this epoch\n",
            "batch loss : 0.003005145 -> about 59.843 second left to finish this epoch\n",
            "batch loss : 0.0016012904 -> about 45.627 second left to finish this epoch\n",
            "batch loss : 0.00035485317 -> about 31.410 second left to finish this epoch\n",
            "batch loss : 0.00046991766 -> about 17.196 second left to finish this epoch\n",
            "batch loss : 0.00048522506 -> about 2.984 second left to finish this epoch\n",
            "current epoch : 29 , current train loss : 0.0007495386818656843\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0003121617 -> about 698.144 second left to finish this epoch\n",
            "batch loss : 0.00047767366 -> about 669.279 second left to finish this epoch\n",
            "batch loss : 0.00015877423 -> about 654.839 second left to finish this epoch\n",
            "batch loss : 0.00031647453 -> about 640.705 second left to finish this epoch\n",
            "batch loss : 0.00024062792 -> about 626.179 second left to finish this epoch\n",
            "batch loss : 0.00051459845 -> about 611.998 second left to finish this epoch\n",
            "batch loss : 0.0003174004 -> about 597.825 second left to finish this epoch\n",
            "batch loss : 0.0005285642 -> about 583.614 second left to finish this epoch\n",
            "batch loss : 0.00037021792 -> about 569.497 second left to finish this epoch\n",
            "batch loss : 0.00045252402 -> about 555.410 second left to finish this epoch\n",
            "batch loss : 0.0007071085 -> about 541.433 second left to finish this epoch\n",
            "batch loss : 0.00043987337 -> about 527.329 second left to finish this epoch\n",
            "batch loss : 0.0003603617 -> about 513.183 second left to finish this epoch\n",
            "batch loss : 0.00041201335 -> about 498.965 second left to finish this epoch\n",
            "batch loss : 0.00020341654 -> about 484.728 second left to finish this epoch\n",
            "batch loss : 0.0013593149 -> about 470.572 second left to finish this epoch\n",
            "batch loss : 0.002359301 -> about 456.778 second left to finish this epoch\n",
            "batch loss : 0.00090834283 -> about 442.524 second left to finish this epoch\n",
            "batch loss : 0.00038418215 -> about 428.315 second left to finish this epoch\n",
            "batch loss : 0.00035842363 -> about 414.114 second left to finish this epoch\n",
            "batch loss : 0.00023469607 -> about 399.914 second left to finish this epoch\n",
            "batch loss : 0.0043377457 -> about 385.747 second left to finish this epoch\n",
            "batch loss : 0.00024654082 -> about 371.582 second left to finish this epoch\n",
            "batch loss : 0.0005311648 -> about 357.375 second left to finish this epoch\n",
            "batch loss : 0.00030327713 -> about 343.168 second left to finish this epoch\n",
            "batch loss : 0.00055998145 -> about 328.968 second left to finish this epoch\n",
            "batch loss : 0.00030703205 -> about 314.784 second left to finish this epoch\n",
            "batch loss : 0.00025102997 -> about 300.621 second left to finish this epoch\n",
            "batch loss : 0.00068095606 -> about 286.444 second left to finish this epoch\n",
            "batch loss : 0.0004761545 -> about 272.254 second left to finish this epoch\n",
            "batch loss : 0.0005013655 -> about 258.064 second left to finish this epoch\n",
            "batch loss : 0.0003174515 -> about 243.877 second left to finish this epoch\n",
            "batch loss : 0.00070605084 -> about 229.739 second left to finish this epoch\n",
            "batch loss : 0.00041054736 -> about 215.583 second left to finish this epoch\n",
            "batch loss : 0.0007994783 -> about 201.410 second left to finish this epoch\n",
            "batch loss : 0.00038772984 -> about 187.226 second left to finish this epoch\n",
            "batch loss : 0.0002925468 -> about 173.052 second left to finish this epoch\n",
            "batch loss : 0.0007511392 -> about 158.889 second left to finish this epoch\n",
            "batch loss : 0.0005451695 -> about 144.759 second left to finish this epoch\n",
            "batch loss : 0.0011834599 -> about 130.589 second left to finish this epoch\n",
            "batch loss : 0.00040515955 -> about 116.416 second left to finish this epoch\n",
            "batch loss : 0.001049869 -> about 102.240 second left to finish this epoch\n",
            "batch loss : 0.00041801453 -> about 88.061 second left to finish this epoch\n",
            "batch loss : 0.00025729174 -> about 73.885 second left to finish this epoch\n",
            "batch loss : 0.0008404836 -> about 59.706 second left to finish this epoch\n",
            "batch loss : 0.00029223628 -> about 45.525 second left to finish this epoch\n",
            "batch loss : 0.00024773937 -> about 31.342 second left to finish this epoch\n",
            "batch loss : 0.0014031045 -> about 17.160 second left to finish this epoch\n",
            "batch loss : 0.0002610778 -> about 2.978 second left to finish this epoch\n",
            "current epoch : 30 , current train loss : 0.0006702826678514032\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.00032011748 -> about 692.669 second left to finish this epoch\n",
            "batch loss : 0.00095067854 -> about 670.734 second left to finish this epoch\n",
            "batch loss : 0.0005604676 -> about 656.110 second left to finish this epoch\n",
            "batch loss : 0.0007931177 -> about 641.114 second left to finish this epoch\n",
            "batch loss : 0.0002454264 -> about 626.550 second left to finish this epoch\n",
            "batch loss : 0.0003917471 -> about 612.794 second left to finish this epoch\n",
            "batch loss : 0.0002573196 -> about 598.546 second left to finish this epoch\n",
            "batch loss : 0.00022505363 -> about 584.364 second left to finish this epoch\n",
            "batch loss : 0.00029514424 -> about 570.143 second left to finish this epoch\n",
            "batch loss : 0.00047081918 -> about 555.904 second left to finish this epoch\n",
            "batch loss : 0.000522273 -> about 541.561 second left to finish this epoch\n",
            "batch loss : 0.000427977 -> about 527.565 second left to finish this epoch\n",
            "batch loss : 0.00025381224 -> about 513.437 second left to finish this epoch\n",
            "batch loss : 0.00030195166 -> about 499.276 second left to finish this epoch\n",
            "batch loss : 0.00024020378 -> about 484.956 second left to finish this epoch\n",
            "batch loss : 0.00042109238 -> about 470.709 second left to finish this epoch\n",
            "batch loss : 0.0011944382 -> about 456.442 second left to finish this epoch\n",
            "batch loss : 0.00021937517 -> about 442.210 second left to finish this epoch\n",
            "batch loss : 0.00069730135 -> about 428.021 second left to finish this epoch\n",
            "batch loss : 0.00032156747 -> about 413.864 second left to finish this epoch\n",
            "batch loss : 0.0002569611 -> about 399.677 second left to finish this epoch\n",
            "batch loss : 0.0005568295 -> about 385.526 second left to finish this epoch\n",
            "batch loss : 0.002377777 -> about 371.353 second left to finish this epoch\n",
            "batch loss : 0.0013593986 -> about 357.191 second left to finish this epoch\n",
            "batch loss : 0.0004054207 -> about 343.062 second left to finish this epoch\n",
            "batch loss : 0.0010302151 -> about 328.889 second left to finish this epoch\n",
            "batch loss : 0.0003650185 -> about 314.743 second left to finish this epoch\n",
            "batch loss : 0.000655126 -> about 300.622 second left to finish this epoch\n",
            "batch loss : 0.0015978946 -> about 286.426 second left to finish this epoch\n",
            "batch loss : 0.00078350655 -> about 272.264 second left to finish this epoch\n",
            "batch loss : 0.0003979105 -> about 258.096 second left to finish this epoch\n",
            "batch loss : 0.0002542144 -> about 243.906 second left to finish this epoch\n",
            "batch loss : 0.0003325001 -> about 229.711 second left to finish this epoch\n",
            "batch loss : 0.0005943949 -> about 215.577 second left to finish this epoch\n",
            "batch loss : 0.0002918084 -> about 201.411 second left to finish this epoch\n",
            "batch loss : 0.0003572134 -> about 187.236 second left to finish this epoch\n",
            "batch loss : 0.00023584247 -> about 173.066 second left to finish this epoch\n",
            "batch loss : 0.0005559753 -> about 158.886 second left to finish this epoch\n",
            "batch loss : 0.0001721554 -> about 144.701 second left to finish this epoch\n",
            "batch loss : 0.00020883375 -> about 130.528 second left to finish this epoch\n",
            "batch loss : 0.00025387778 -> about 116.352 second left to finish this epoch\n",
            "batch loss : 0.0004885938 -> about 102.181 second left to finish this epoch\n",
            "batch loss : 0.001365497 -> about 88.006 second left to finish this epoch\n",
            "batch loss : 0.0008958411 -> about 73.834 second left to finish this epoch\n",
            "batch loss : 0.0003274188 -> about 59.662 second left to finish this epoch\n",
            "batch loss : 0.000390111 -> about 45.489 second left to finish this epoch\n",
            "batch loss : 0.00040008908 -> about 31.319 second left to finish this epoch\n",
            "batch loss : 0.00035822048 -> about 17.148 second left to finish this epoch\n",
            "batch loss : 0.0033012244 -> about 2.976 second left to finish this epoch\n",
            "current epoch : 31 , current train loss : 0.0006175088244127629\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0003279099 -> about 686.376 second left to finish this epoch\n",
            "batch loss : 0.0005657996 -> about 669.724 second left to finish this epoch\n",
            "batch loss : 0.00032665595 -> about 654.928 second left to finish this epoch\n",
            "batch loss : 0.0006082782 -> about 640.969 second left to finish this epoch\n",
            "batch loss : 0.00051584415 -> about 626.978 second left to finish this epoch\n",
            "batch loss : 0.00022797116 -> about 612.878 second left to finish this epoch\n",
            "batch loss : 0.0005105241 -> about 598.561 second left to finish this epoch\n",
            "batch loss : 0.00049758033 -> about 585.247 second left to finish this epoch\n",
            "batch loss : 0.00038417248 -> about 570.824 second left to finish this epoch\n",
            "batch loss : 0.0012488991 -> about 556.608 second left to finish this epoch\n",
            "batch loss : 0.00060087023 -> about 542.299 second left to finish this epoch\n",
            "batch loss : 0.00041035525 -> about 527.956 second left to finish this epoch\n",
            "batch loss : 0.0004226953 -> about 513.717 second left to finish this epoch\n",
            "batch loss : 0.00062015106 -> about 499.391 second left to finish this epoch\n",
            "batch loss : 0.00063057087 -> about 485.153 second left to finish this epoch\n",
            "batch loss : 0.0003079093 -> about 470.939 second left to finish this epoch\n",
            "batch loss : 0.00031703047 -> about 456.756 second left to finish this epoch\n",
            "batch loss : 0.0008194431 -> about 442.490 second left to finish this epoch\n",
            "batch loss : 0.0010879657 -> about 428.297 second left to finish this epoch\n",
            "batch loss : 0.0023971645 -> about 414.093 second left to finish this epoch\n",
            "batch loss : 0.00019561306 -> about 399.955 second left to finish this epoch\n",
            "batch loss : 0.0005469123 -> about 385.830 second left to finish this epoch\n",
            "batch loss : 0.00021139196 -> about 371.721 second left to finish this epoch\n",
            "batch loss : 0.00079224014 -> about 357.556 second left to finish this epoch\n",
            "batch loss : 0.0005260798 -> about 343.405 second left to finish this epoch\n",
            "batch loss : 0.00019992318 -> about 329.189 second left to finish this epoch\n",
            "batch loss : 0.0006598702 -> about 314.987 second left to finish this epoch\n",
            "batch loss : 0.0003328035 -> about 300.838 second left to finish this epoch\n",
            "batch loss : 0.00035071917 -> about 286.640 second left to finish this epoch\n",
            "batch loss : 0.0005190308 -> about 272.578 second left to finish this epoch\n",
            "batch loss : 0.00019060387 -> about 258.365 second left to finish this epoch\n",
            "batch loss : 0.00020346063 -> about 244.183 second left to finish this epoch\n",
            "batch loss : 0.00031169094 -> about 229.984 second left to finish this epoch\n",
            "batch loss : 0.00051346165 -> about 215.804 second left to finish this epoch\n",
            "batch loss : 0.00032353654 -> about 201.601 second left to finish this epoch\n",
            "batch loss : 0.0007693985 -> about 187.395 second left to finish this epoch\n",
            "batch loss : 0.0002593056 -> about 173.198 second left to finish this epoch\n",
            "batch loss : 0.00032312822 -> about 159.001 second left to finish this epoch\n",
            "batch loss : 0.00019551776 -> about 144.822 second left to finish this epoch\n",
            "batch loss : 0.0002517891 -> about 130.637 second left to finish this epoch\n",
            "batch loss : 0.00033097758 -> about 116.441 second left to finish this epoch\n",
            "batch loss : 0.000496851 -> about 102.239 second left to finish this epoch\n",
            "batch loss : 0.00031042905 -> about 88.047 second left to finish this epoch\n",
            "batch loss : 0.00044997706 -> about 73.863 second left to finish this epoch\n",
            "batch loss : 0.0025424077 -> about 59.678 second left to finish this epoch\n",
            "batch loss : 0.00030928585 -> about 45.496 second left to finish this epoch\n",
            "batch loss : 0.0002871244 -> about 31.318 second left to finish this epoch\n",
            "batch loss : 0.00024920743 -> about 17.144 second left to finish this epoch\n",
            "batch loss : 0.0003048888 -> about 2.975 second left to finish this epoch\n",
            "current epoch : 32 , current train loss : 0.0005776411065524576\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.00025577127 -> about 692.263 second left to finish this epoch\n",
            "batch loss : 0.00076678355 -> about 667.946 second left to finish this epoch\n",
            "batch loss : 0.0010804103 -> about 654.655 second left to finish this epoch\n",
            "batch loss : 0.00022149456 -> about 640.100 second left to finish this epoch\n",
            "batch loss : 0.00025502237 -> about 625.001 second left to finish this epoch\n",
            "batch loss : 0.0005215329 -> about 610.086 second left to finish this epoch\n",
            "batch loss : 0.0004879827 -> about 595.671 second left to finish this epoch\n",
            "batch loss : 0.0007511084 -> about 581.539 second left to finish this epoch\n",
            "batch loss : 0.00020532278 -> about 567.135 second left to finish this epoch\n",
            "batch loss : 0.0031256282 -> about 552.927 second left to finish this epoch\n",
            "batch loss : 0.00040251433 -> about 539.546 second left to finish this epoch\n",
            "batch loss : 0.00046956522 -> about 525.993 second left to finish this epoch\n",
            "batch loss : 0.0030419945 -> about 512.892 second left to finish this epoch\n",
            "batch loss : 0.00039184204 -> about 498.786 second left to finish this epoch\n",
            "batch loss : 0.00024220695 -> about 485.040 second left to finish this epoch\n",
            "batch loss : 0.0001823286 -> about 471.396 second left to finish this epoch\n",
            "batch loss : 0.00065654184 -> about 457.470 second left to finish this epoch\n",
            "batch loss : 0.00039963942 -> about 443.145 second left to finish this epoch\n",
            "batch loss : 0.00032705473 -> about 428.835 second left to finish this epoch\n",
            "batch loss : 0.0003391974 -> about 414.553 second left to finish this epoch\n",
            "batch loss : 0.0004959128 -> about 400.260 second left to finish this epoch\n",
            "batch loss : 0.00036952342 -> about 385.962 second left to finish this epoch\n",
            "batch loss : 0.0009232106 -> about 371.871 second left to finish this epoch\n",
            "batch loss : 0.0002671137 -> about 357.639 second left to finish this epoch\n",
            "batch loss : 0.000646618 -> about 343.598 second left to finish this epoch\n",
            "batch loss : 0.0002489885 -> about 329.383 second left to finish this epoch\n",
            "batch loss : 0.00029752654 -> about 315.083 second left to finish this epoch\n",
            "batch loss : 0.00024260988 -> about 300.816 second left to finish this epoch\n",
            "batch loss : 0.00041564798 -> about 286.576 second left to finish this epoch\n",
            "batch loss : 0.00041681458 -> about 272.343 second left to finish this epoch\n",
            "batch loss : 0.00043654442 -> about 258.110 second left to finish this epoch\n",
            "batch loss : 0.00029383833 -> about 243.879 second left to finish this epoch\n",
            "batch loss : 0.00035521443 -> about 229.666 second left to finish this epoch\n",
            "batch loss : 0.0025180313 -> about 215.457 second left to finish this epoch\n",
            "batch loss : 0.0010320662 -> about 201.247 second left to finish this epoch\n",
            "batch loss : 0.00059499877 -> about 187.060 second left to finish this epoch\n",
            "batch loss : 0.00024666643 -> about 172.878 second left to finish this epoch\n",
            "batch loss : 0.00072126114 -> about 158.698 second left to finish this epoch\n",
            "batch loss : 0.00034801598 -> about 144.536 second left to finish this epoch\n",
            "batch loss : 0.000301825 -> about 130.357 second left to finish this epoch\n",
            "batch loss : 0.00023342624 -> about 116.188 second left to finish this epoch\n",
            "batch loss : 0.00027157948 -> about 102.025 second left to finish this epoch\n",
            "batch loss : 0.00036339718 -> about 87.868 second left to finish this epoch\n",
            "batch loss : 0.0001715936 -> about 73.709 second left to finish this epoch\n",
            "batch loss : 0.00051780406 -> about 59.553 second left to finish this epoch\n",
            "batch loss : 0.00057961955 -> about 45.401 second left to finish this epoch\n",
            "batch loss : 0.00040964727 -> about 31.258 second left to finish this epoch\n",
            "batch loss : 0.00087291875 -> about 17.115 second left to finish this epoch\n",
            "batch loss : 0.00053746346 -> about 2.970 second left to finish this epoch\n",
            "current epoch : 33 , current train loss : 0.0005601058317335515\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0006086431 -> about 683.011 second left to finish this epoch\n",
            "batch loss : 0.00024189454 -> about 665.296 second left to finish this epoch\n",
            "batch loss : 0.00021819894 -> about 651.051 second left to finish this epoch\n",
            "batch loss : 0.0002622587 -> about 636.790 second left to finish this epoch\n",
            "batch loss : 0.002001061 -> about 622.726 second left to finish this epoch\n",
            "batch loss : 0.0002615673 -> about 608.347 second left to finish this epoch\n",
            "batch loss : 0.0002932088 -> about 594.131 second left to finish this epoch\n",
            "batch loss : 0.0002333482 -> about 579.990 second left to finish this epoch\n",
            "batch loss : 0.00026419555 -> about 566.053 second left to finish this epoch\n",
            "batch loss : 0.00023855697 -> about 551.856 second left to finish this epoch\n",
            "batch loss : 0.000386408 -> about 537.939 second left to finish this epoch\n",
            "batch loss : 0.0005166244 -> about 523.990 second left to finish this epoch\n",
            "batch loss : 0.0033711228 -> about 509.837 second left to finish this epoch\n",
            "batch loss : 0.0003296992 -> about 495.716 second left to finish this epoch\n",
            "batch loss : 0.00046867435 -> about 481.627 second left to finish this epoch\n",
            "batch loss : 0.00036874594 -> about 467.522 second left to finish this epoch\n",
            "batch loss : 0.00037319158 -> about 453.444 second left to finish this epoch\n",
            "batch loss : 0.0005021538 -> about 439.337 second left to finish this epoch\n",
            "batch loss : 0.00022365013 -> about 425.229 second left to finish this epoch\n",
            "batch loss : 0.00080198894 -> about 411.150 second left to finish this epoch\n",
            "batch loss : 0.0003770374 -> about 397.241 second left to finish this epoch\n",
            "batch loss : 0.0001833085 -> about 383.166 second left to finish this epoch\n",
            "batch loss : 0.00025113206 -> about 369.063 second left to finish this epoch\n",
            "batch loss : 0.00024507748 -> about 354.928 second left to finish this epoch\n",
            "batch loss : 0.0007067964 -> about 340.815 second left to finish this epoch\n",
            "batch loss : 0.0004942121 -> about 326.699 second left to finish this epoch\n",
            "batch loss : 0.000246791 -> about 312.621 second left to finish this epoch\n",
            "batch loss : 0.0005231219 -> about 298.548 second left to finish this epoch\n",
            "batch loss : 0.00025884743 -> about 284.464 second left to finish this epoch\n",
            "batch loss : 0.00038375886 -> about 270.389 second left to finish this epoch\n",
            "batch loss : 0.0027050688 -> about 256.314 second left to finish this epoch\n",
            "batch loss : 0.00023273613 -> about 242.241 second left to finish this epoch\n",
            "batch loss : 0.00043844018 -> about 228.156 second left to finish this epoch\n",
            "batch loss : 0.00024060241 -> about 214.100 second left to finish this epoch\n",
            "batch loss : 0.0002898255 -> about 200.016 second left to finish this epoch\n",
            "batch loss : 0.00044229184 -> about 185.932 second left to finish this epoch\n",
            "batch loss : 0.00060442905 -> about 171.849 second left to finish this epoch\n",
            "batch loss : 0.00026138336 -> about 157.765 second left to finish this epoch\n",
            "batch loss : 0.00027980347 -> about 143.693 second left to finish this epoch\n",
            "batch loss : 0.00043915692 -> about 129.625 second left to finish this epoch\n",
            "batch loss : 0.0005005336 -> about 115.549 second left to finish this epoch\n",
            "batch loss : 0.00054566434 -> about 101.468 second left to finish this epoch\n",
            "batch loss : 0.0015280186 -> about 87.412 second left to finish this epoch\n",
            "batch loss : 0.00030461376 -> about 73.338 second left to finish this epoch\n",
            "batch loss : 0.0009313106 -> about 59.260 second left to finish this epoch\n",
            "batch loss : 0.00017287623 -> about 45.183 second left to finish this epoch\n",
            "batch loss : 0.00041534446 -> about 31.107 second left to finish this epoch\n",
            "batch loss : 0.00030105776 -> about 17.031 second left to finish this epoch\n",
            "batch loss : 0.00021703991 -> about 2.956 second left to finish this epoch\n",
            "current epoch : 34 , current train loss : 0.0004952497694928248\n",
            "Model saved in file : /content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt\n",
            "batch loss : 0.0003256839 -> about 679.305 second left to finish this epoch\n",
            "batch loss : 0.00055057224 -> about 663.761 second left to finish this epoch\n",
            "batch loss : 0.0002174438 -> about 650.188 second left to finish this epoch\n",
            "batch loss : 0.00030646386 -> about 635.504 second left to finish this epoch\n",
            "batch loss : 0.00026880403 -> about 621.510 second left to finish this epoch\n",
            "batch loss : 0.00053726044 -> about 607.287 second left to finish this epoch\n",
            "batch loss : 0.0005568498 -> about 593.537 second left to finish this epoch\n",
            "batch loss : 0.00021021531 -> about 579.733 second left to finish this epoch\n",
            "batch loss : 0.0004528983 -> about 565.715 second left to finish this epoch\n",
            "batch loss : 0.00039960322 -> about 551.576 second left to finish this epoch\n",
            "batch loss : 0.00027414144 -> about 537.488 second left to finish this epoch\n",
            "batch loss : 0.00035322143 -> about 523.372 second left to finish this epoch\n",
            "batch loss : 0.0004237609 -> about 509.277 second left to finish this epoch\n",
            "batch loss : 0.00035387225 -> about 495.285 second left to finish this epoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b66e41e1857e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mtrain_img_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_img_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mtrain_pos_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pos_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       model_save_path = '/content/drive/MyDrive/Speech2Pickup/dataset1_model/HGN_senEM_model_wo/model.ckpt')\n\u001b[0m",
            "\u001b[0;32m/content/code_HGN_senEM_train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(img_resize, heatmap_resize, num_hg_Depth, dim_hg_feat, n_mels, time_steps, encoder_model_path, encoder_args, input_shapes, seed, training_state, batch_size, max_epoch, num_train, save_stride, learning_rate, dropout_rate, restore_flag, restore_path, restore_epoch, total_images, total_heatmaps, train_speech_inputs, train_img_idx, train_pos_outputs, model_save_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                    ph_heatmap: batch_heatmaps, ph_dropout: dropout_rate}\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mcurr_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurr_train_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7-oZq5P9WQ5"
      },
      "source": [
        "# Reset tensor graph\n",
        "# Finish wandb process\n",
        "\n",
        "tf.reset_default_graph()\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSRSZz3bT67K"
      },
      "source": [
        "# Check variable list (debugging)\n",
        "\n",
        "variables_names = [v.name for v in tf.trainable_variables()]\n",
        "variables_names"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}