# -*- coding: utf-8 -*-
"""speech2vecEM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrUtjNUv5cITGyEGx4vlsVJzH5PSClXF
"""

import pickle
file = open("/content/drive/MyDrive/machine_listening_2020/Project/pickle/train_data_.pkl","rb")

data_train = pickle.load(file)

data_train = data_train.values()

data_train = list(data_train)

from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from torch.utils import data
import torch.nn.functional as F
from tqdm.notebook  import tqdm

max_leng = 10

encoder_input_dim = 13
decoder_hidden_dim = 50
encoder_hidden_dim = 50
decoder_output_dim = 13
BATCH_SIZE = 512

def padding(mfcc_feature, max_leng = max_leng):
  length = mfcc_feature.shape[0]
  if(length > max_leng):
    ret = mfcc_feature[:max_leng,:]
  elif(length < max_leng):
    diff = max_leng- length
    zeros = np.zeros((diff,13))
    ret = np.concatenate((mfcc_feature,zeros),axis = 0)
  else:
    ret = mfcc_feature
  return ret

windowed_x = []
windowed_y1 = []
windowed_y2 = []
for sect in tqdm(data_train):
  for word_i in range(1,len(sect)-1):
    
    x_temp = padding(sect[word_i])
    y1_temp = padding(sect[word_i - 1])
    y2_temp = padding(sect[word_i + 1])

    if x_temp.shape == (max_leng,13) and y1_temp.shape == (max_leng,13) and y2_temp.shape == (max_leng,13):
      windowed_x.append(x_temp)
      windowed_y1.append(y1_temp)
      windowed_y2.append(y2_temp)
    else:
      print("shape error!",x_temp.shape,y1_temp.shape,y2_temp.shape)

windowed_x = windowed_x
windowed_y1 = windowed_y1
windowed_y2 = windowed_y2

del data_train

class Encoder(nn.Module):
    def __init__(self, input_dim , enc_hid_dim):
        super().__init__()
        
        self.rnn = nn.GRU(input_dim, enc_hid_dim, bidirectional = True,batch_first = True)
        
                
    def forward(self, input):
        
        #input = [src len, batch size]
      
        
        outputs, hidden = self.rnn(input)
                
        #outputs = [batch size,src len, hid dim * num directions]
        #hidden = [n layers * num directions, batch size, hid dim]
        
        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]
        #outputs are always from the last layer
        
        #hidden [-2, :, : ] is the last of the forwards RNN 
        #hidden [-1, :, : ] is the last of the backwards RNN
        
        #initial decoder hidden is final hidden state of the forwards and backwards 
        
        return outputs, hidden

class Attention(nn.Module):
    def __init__(self, enc_hid_dim, dec_hid_dim):
        super().__init__()
        
        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)
        self.v = nn.Linear(dec_hid_dim, 1, bias = False)
        
    def forward(self, hidden, encoder_outputs):
        
        #hidden = [batch size, dec hid dim]
        
        batch_size = encoder_outputs.shape[0]
        src_len = encoder_outputs.shape[1]
        
        #repeat decoder hidden state src_len times
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
      
        #hidden = [batch size, src len, dec hid dim]
        #encoder_outputs = [batch size, src len, enc hid dim * 2]
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) 
        
        #energy = [batch size, src len, dec hid dim]

        attention = self.v(energy).squeeze(2)
        
        #attention= [batch size, src len]
        
        return F.softmax(attention, dim=1)

class Decoder(nn.Module):
    def __init__(self, output_dim, enc_hid_dim, dec_hid_dim, attention):
        super().__init__()

        self.attention = attention
         
        self.rnn = nn.GRU((enc_hid_dim * 2), dec_hid_dim, batch_first = True)
        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, output_dim)
        
        
    def forward(self, hidden, encoder_outputs):
             
        #hidden = [batch size, dec hid dim]
        #encoder_outputs = [batch size,src len,  enc hid dim * 2]
        

        a = self.attention(hidden, encoder_outputs)
                
        #a = [batch size, src len]
        
        a = a.unsqueeze(1)
        
        #a = [batch size, 1, src len]
       
        #encoder_outputs = [batch size, src len, enc hid dim * 2]
        
        weighted = torch.bmm(a, encoder_outputs)
        
        #weighted = [batch size, 1, enc hid dim * 2]
        
        rnn_input = weighted
        # print(rnn_input)
        
        #rnn_input = [batch size, 1, enc hid dim * 2]
            
        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))
        
        #output = [seq len, batch size, dec hid dim * n directions]
        #hidden = [n layers * n directions, batch size, dec hid dim]
        
        #seq len, n layers and n directions will always be 1 in this decoder, therefore:
        #output = [1, batch size, dec hid dim]
        #hidden = [1, batch size, dec hid dim]
        #this also means that output == hidden
        # print(output.shape)
        # print(hidden.shape)
        # assert (output == hidden).all()
        # output = output.squeeze(0)
        hidden = hidden.squeeze(0)
        weighted = weighted.permute(1, 0, 2)
        weighted = weighted.squeeze(0)
        final_output = self.fc_out(torch.cat((hidden, weighted), dim = 1))
        #prediction = [batch size, output dim]
        
        return final_output, hidden

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
    def forward(self, src, trg1, trg2, teacher_forcing_ratio = 0.5):
        
        #src = [batch size,src len, dim]
        #trg = [batch size,trg len, dim]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time
        
        batch_size = src.shape[0]
        trg_len = max_leng
        decoder_hidden_size = decoder_output_dim
        
        #tensor to store decoder outputs
        outputs1 = torch.zeros(batch_size, trg_len, decoder_hidden_size).to(self.device)
        outputs2 = torch.zeros(batch_size, trg_len, decoder_hidden_size).to(self.device)
        
        #encoder_outputs is all hidden states of the input sequence, back and forwards
        #hidden is the final forward and backward hidden states, passed through a linear layer
        encoder_outputs, hidden = self.encoder(src)
        hidden = 0.5*hidden[0,:,:] + 0.5*hidden[1,:,:]
                
        #first input to the decoder is the <sos> tokens
        input = trg1[0,:]
        
        for t in range(1, trg_len):
            

            final_output, hidden = self.decoder(hidden, encoder_outputs)
            
            #place predictions in a tensor holding predictions for each token
            outputs1[:,t,:] = final_output
            

        encoder_outputs, hidden = self.encoder(src)
        hidden = 0.5*hidden[0,:,:] + 0.5*hidden[1,:,:]
        for t in range(1, trg_len):
            
            #insert input token embedding, previous hidden state and all encoder hidden states
            #receive output tensor (predictions) and new hidden state
            final_output, hidden = self.decoder(hidden, encoder_outputs)
            
            #place predictions in a tensor holding predictions for each token
            outputs2[:,t,:] = final_output
            
        return outputs1, outputs2

encoder_input_dim = 13
decoder_hidden_dim = 50
encoder_hidden_dim = 50
decoder_output_dim = 13
device = torch.device("cuda")

# dummy = torch.rand(8,max_leng,13).to(device)
# dummy_y1 = torch.rand(8,max_leng,13).to(device)
# dummy_y2 = torch.rand(8,max_leng,13).to(device)
x_dataset_demo = torch.tensor(windowed_x,dtype=torch.float32)
y_dataset_demo1 = torch.tensor(windowed_y1,dtype=torch.float32)
y_dataset_demo2 = torch.tensor(windowed_y2,dtype=torch.float32)

train_dataset = data.TensorDataset(x_dataset_demo, y_dataset_demo1,y_dataset_demo2)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)

attn = Attention(decoder_hidden_dim, decoder_hidden_dim)
enc = Encoder(encoder_input_dim, encoder_hidden_dim)
dec = Decoder(decoder_output_dim,encoder_hidden_dim, decoder_hidden_dim, attn)
model = Seq2Seq(enc,dec,device).to(device)

# model.apply(init_weights)

optimizer = optim.SGD(model.parameters(), momentum = 0.9, lr = 0.001)
scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1)
mse_loss = nn.MSELoss()

!nvidia-smi

import math
model.train()
EPOCHS = 100
clip = 1
losses = []
best_loss = math.inf
for epo in range(EPOCHS):
  epoch_loss = 0

  for i, batch in tqdm(enumerate(train_loader)):
      
      src = batch[0].to(device)
      trg1 = batch[1].to(device)
      trg2 = batch[1].to(device)
      
      optimizer.zero_grad()
      
      output1,output2 = model(src, trg1, trg2)
      
      #trg = [trg len, batch size]
      #output = [trg len, batch size, output dim]
      
      # output_dim = output.shape[-1]
      
      # output = output[1:].view(-1, output_dim)
      # trg = trg[1:].view(-1)
      
      #trg = [(trg len - 1) * batch size]
      #output = [(trg len - 1) * batch size, output dim]

      # print(model.encoder.rnn.weight_hh_l0_reverse.data)
      loss = mse_loss(output1,trg1) + mse_loss(output2,trg2)

      
      loss.backward()
      
      torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
      
      optimizer.step()
      scheduler.step()
      # # print(model.encoder.rnn.grad)
      # for name, parms in model.named_parameters():	
      #   print('-->name:', name, '-->grad_requirs:',parms.requires_grad, \
      #   ' -->grad_value:',parms.grad)

      epoch_loss += loss.item()
  epoch_loss /= len(train_loader)
  if epoch_loss < best_loss:
    best_loss = epoch_loss
    torch.save(model.state_dict(), "speech2vec_model_.pt")
  losses.append(epoch_loss)
  print("Epoch:{}, loss:{} ".format(epo + 1,epoch_loss))

import matplotlib.pyplot as plt
plt.plot(losses)

import pickle
with open("/content/drive/MyDrive/machine_listening_2020/Project/pickle/word_2_mfcc_.pkl",'rb') as file:
  word_2_mfcc = pickle.load(file)

print(word_2_mfcc.keys())
words = [w.lower() for w in word_2_mfcc.keys()]
mfccs = [padding(word_2_mfcc[w]) for w in word_2_mfcc.keys()]
x_dataset_ext = torch.tensor(mfccs,dtype=torch.float32)
ext_dataset = data.TensorDataset(x_dataset_ext)
print(x_dataset_ext.shape)
ext_loader = torch.utils.data.DataLoader(ext_dataset, batch_size=128, shuffle=False)

attn_new = Attention(decoder_hidden_dim, decoder_hidden_dim)
enc_new = Encoder(encoder_input_dim, encoder_hidden_dim)
dec_new = Decoder(decoder_output_dim,encoder_hidden_dim, decoder_hidden_dim, attn_new)
model_new = Seq2Seq(enc_new,dec_new,device).to(device)
model_new.load_state_dict(torch.load('/content/speech2vec_model_.pt'))
model_new.eval()

epoch_loss = 0
final_results = []
with torch.no_grad():
  epoch_loss = 0

  for i, batch in tqdm(enumerate(ext_loader)):
      
      src = batch[0].to(device)
      
      _,output = enc_new(src)

      output = torch.cat((output[0,:,:],output[1,:,:]),1)
      if i == 0:
        final_results = output.cpu().numpy()
      else:
        final_results = np.concatenate((final_results,output.cpu().numpy()))
      print(output.shape)
      print(final_results.shape)

with open("speech_word2vec.txt", "w") as f:
    for offset,word in enumerate(words):
        f.write(word)
        f.write(" ")
        for i in range(0, 100):
            f.write(str(final_results[offset,i]))
            f.write(" ")
        f.write("\n")

